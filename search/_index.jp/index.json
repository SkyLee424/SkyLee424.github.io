[{"categories":["ForBeginers"],"content":"什么是后端？ 简而言之，后端是支撑 App、网站等前端展示内容的“幕后英雄”。它负责处理数据、存储数据、业务逻辑等，确保前端能够顺利地展示出用户需要的信息。\n举个例子：你在网上购物时，点击购买按钮后，前端会将请求发送给后端，后端负责验证订单信息、处理支付、更新库存、生成订单等操作，然后将结果返回给前端展示给你。前端就是用户与应用交互的界面，而后端是应用处理和存储数据的“大脑”。\n要成为一名后端开发人员，需要掌握哪些技术栈？ 技术栈：是指一套在开发过程中使用的工具、框架和技术。后端开发技术栈包括很多部分，从编程语言到操作系统、数据库、消息队列等，涉及的知识面非常广泛。\n这里以我的简历作为切入点，来帮助你们更清晰的了解需要掌握的技术栈：\n编程语言 主流的后端开发语言有：\nGolang Java C++ \u0026hellip; 这三个语言做个对比的话：\n语言 特点 学习难度（门槛） 发展前途 主观推荐程度 Golang 一门算是比较新的语言，不仅活跃在传统后端，还广泛运用在云原生、分布式系统建设上 不高，但是缺乏一个系统的学习路线，需要自己琢磨 腾讯、字节目前很多用的都是 Go 和 C++，潜力无限 中等 Java 传统开发语言，生态好 不高，有一套完整的学习体系，免费的资源也比较多 虽然市场稳定，但相比新兴语言发展空间较小 中等 C++ 强大的底层语言，广泛应用于高性能计算与系统开发 高，资源较少，学习曲线陡峭 C++ 起一个「基石」的作用，在系统、底层编程中至关重要，尤其在操作系统领域 低 其实语言这块不是特别重要，大部分公司都支持「转语言」，更看重的还是你的其他能力\n数据结构与算法 数据结构与算法算是基础中的基础了，笔试、面试环节都会问到，除此之外，在实际开发中也离不开 DS 的支持\n常见的数据结构：\n链表、数组、栈、队列 树（如二叉树、AVL树、红黑树） 图（如图的遍历、最短路径算法） 哈希表、堆、字典 \u0026hellip; 常见算法：\n排序算法（如快排、归并排序） 查找算法（如二分查找、哈希查找） 动态规划、分治法、回溯法 \u0026hellip; 操作系统 计算机网络 数据库 数据库是后端开发中至关重要的部分，它帮助我们存储和管理应用的数据。后端开发常用的数据库有：\n关系型数据库（如 MySQL、PostgreSQL） 非关系型数据库（如 MongoDB、Redis） 消息队列 消息队列（如 Kafka、RabbitMQ）是后端开发中非常重要的工具，它用于在分布式系统中传递消息，解耦不同服务之间的依赖，提高系统的可扩展性和可靠性。常见的应用场景包括：\n异步任务处理：例如，用户上传文件后，后端将文件处理任务放入消息队列中，后台服务异步处理文件。 流量削峰：在高峰期间，将请求转发到消息队列中，避免系统过载。 分布式、云原生 分布式系统：当应用的规模增长时，我们需要将系统拆分成多个模块，每个模块分布在不同的机器上运行。分布式系统的核心问题包括服务注册与发现、负载均衡、数据一致性等。 云原生：云原生是一种通过云计算环境构建应用程序的方法，主要依靠微服务架构、容器（如 Docker）和编排工具（如 Kubernetes）。它强调弹性、扩展性、快速迭代和高效的资源利用。 就业情况 泼个冷水：实际上对于「双非」的我们，目前后端的就业情况是 不太好 的\n就拿我们这一届来说，后端开发找到实习的，仅有个位数（我了解到的情况，不一定准确哈）\n本就学历不占优的我们，要想最后拿到满意的 offer，肯定是要付出更多的努力的，并且 学习方式、路线也得正确\n我要不要学后端开发？ 了解了后端开发的技术栈、就业情况，你可能在思考：我要不要学后端开发？\n如果你： 喜欢逻辑思维和解决问题\n后端开发的核心工作之一就是处理业务逻辑、设计数据流和优化系统性能。如果你喜欢通过逻辑推理解决问题、挑战复杂的系统架构、并在代码中寻找最佳解法，后端开发非常适合你。\n享受与数据和性能打交道\n后端开发涉及大量与数据库、数据结构、性能优化等相关的工作。如果你喜欢深入了解数据如何存储和处理，关注系统如何高效工作，后端开发能够让你在这方面不断探索和成长。\n对技术深度感兴趣，愿意学到更多底层知识\n后端开发通常需要理解操作系统、网络协议、分布式系统等较为底层的技术。如果你有兴趣从更深的层次理解技术，后端开发能够提供丰富的学习资源和挑战，帮助你掌握这些基础技术。\n有一定的团队合作精神\n后端开发通常是与前端、运维、测试等多个角色紧密合作的。如果你喜欢与团队成员协作，解决跨部门的技术问题，后端开发的工作方式会非常适合你。\n你可能不适合学后端开发，如果： 你更喜欢快速可视化的结果\n后端开发往往需要更多的逻辑思维和数据处理，最终的结果不会像前端那样直接展示给用户看。如果你喜欢快速看到代码的结果并与用户交互，前端开发 可能更符合你的兴趣。\n你对系统设计和底层架构不感兴趣\n后端开发涉及到很多复杂的系统设计，特别是在处理高并发、分布式系统等问题时。如果你对这些挑战不感兴趣，可能会觉得后端开发比较 枯燥，甚至会感到力不从心。\n你喜欢创造用户体验而非后台技术\n后端开发的主要任务是构建稳定的服务器和API，而不涉及到直接与用户交互的界面设计。如果你更注重设计、用户体验（UI/UX），前端开发 可能是一个更合适的选择。\n我应该如何学习后端开发？ 如果你决定学习后端开发，这里可以给出我的一个学习路线作为参考：\n关于 - Sky_Lee 的个人博客 推荐 这里推荐一些你可能会用到的工具或网站：\nLeetCode（力扣） ：一个 面向工作 的刷题网站 NowCoder（牛客） ：找工作可以在上面讨论，还能看看其他人的 面经 GeekHour ：分享了一些快速入门教程，帮助你更快速的理解一个领域 极客时间 ：很多优秀的技术文章，帮助你更深入的理解一个领域 李文周的个人博客 ：初学 Golang 可以了解一下 极客兔兔 ：同上 \u0026hellip; 另外打个广告：\n我的博客 ：记录了我平时写的一些内容 BlueBell 论坛 ：我自己写的一个论坛，你们也可以上去发点文章啥的哈哈哈 ","permalink":"https://blogs.skylee.top/posts/forbeginers/note/","tags":["ForBeginers"],"title":"For Backend Beginers"},{"categories":["Distributed-System"],"content":"定义 分布式锁是一种用于在分布式系统环境中控制并发访问共享资源的机制。在分布式系统中的多个进程或节点可能会竞争访问同一个资源时，分布式锁可以确保每次只有一个进程能够访问该资源，从而避免出现数据不一致或冲突。\n实现方式 基于数据库 利用数据库进行锁管理是最简单的一种方式。一种常见的方法是利用数据库表行锁，以某一特定表记录作为锁标志。例如：\n插入唯一键记录，两个不同进程同时插入记录时，只有一个能成功。 更新记录中的标志位来表示锁的状态。 但这种方式存在性能瓶颈，不适合并发量高的场景\n基于 Redis SET 命令很常见，那 NX 选项是干嘛的呢？\nNX 选项要求设置 Key 时：\n如果 Key 不存在，那么设置 Key 为 Value，并返回 OK 如果 Key 存在，那么不修改 Key，并返回 nil 基于这个特性，可以利用 NX 选项设置分布式锁：\n获取锁 一个进程想要获取锁，只需要执行 SETNX 即可：\nSET Lock-0 value123 NX # 或者直接使用 SETNX SETNX Lock-0 value123 如果返回 OK，说明成功获取锁，可以访问接下来的临界资源\n但要考虑一种情况：如果进程挂了，就永远无法释放这个锁了，因此，需要给锁设置一个合理的超时时间：\nSET Lock-0 value123 NX EX 10 # 设置 10s 的过期时间 释放锁 一个进程想要释放已有的锁，只需要执行 DELETE 即可：\nDELETE Lock-0 但是这种方式存在一个潜在的问题，来看看这个场景：\n进程 1 获取了 Lock0，设置了超时时间为 10s 进程 1 执行业务逻辑，但整体时间超过了 10s 由于超时，锁自动释放 进程 2 此时可以成功获取锁 Lock0，并执行业务逻辑 此时，进程 1 业务逻辑执行完毕，于是 释放锁 Lock0 可以发现问题所在：进程 1 错误地释放了不属于自己的锁，该锁由进程 2 持有\n那么如何解决这个问题呢？很简单，就是在设置锁时，Value 设置成一个随机值：\nSET Lock-0 \u0026lt;random_value\u0026gt; NX EX 10 # 设置 10s 的过期时间 然后在释放锁时，检查一下当前这个锁是否还属于自己：\nvalue := redis.Get(\u0026#34;Lock-0\u0026#34;) // 如果 value 与自己设置的 value 相同，执行释放操作 if value == random_value { redis.Delete(\u0026#34;Lock-0\u0026#34;) } 但是这种方式还存在一个问题：Get 与 Delete 整体不是一个原子操作\n如果进程 1 判断 Lock-0 还属于自己，进入 if 分支并准备释放的同时，锁恰好过期并被另外一个进程获取了，那么还是会出现错误释放的问题\n怎么解决？肯定要将这两个操作封装成一个原子操作\n如果释放锁的逻辑在我们的业务服务器上执行，那还是回到了分布式的问题，又要依赖分布式锁才能实现原子操作，形成闭环\n解决方案：使用 Lua，将释放锁的逻辑封装成一个 Lua 脚本，然后在 Redis Server 上串行执行\n集群模式下的 Redis 如果 Redis 是单节点部署，那上面描述的操作完全没有问题\n但是，Redis 还提供了：主从（哨兵）模式和集群模式\n如果 Redis 是多节点部署，那么还是会产生问题：\n假设进程 1 成功执行 SETNX 命令 在 Redis 主节点同步数据给从节点时，主节点宕机了，没有成功将 SETNX 这一条命令同步给从节点 哨兵（或者集群模式下的其他主节点）发现了主节点宕机，于是在从节点中选出新的主节点 新的从节点上是不包含 SETNX 命令产生的结果的，也就是说，我们的服务进程会认为锁已经被释放（或者压根就不存在），这样造成了重复获取锁\n解决方案呢？\n基于 Zookeeper 前面提到了集群模式下的 Redis，分布式锁会有一定问题\n原因就出现在：Redis 本身不是强一致的，在同步数据给从节点前，就告诉客户端命令执行完成了\n那么如何解决？\n配置 min-replicas-to-write 和 min-replicas-max-lag\nRedis 提供了两个配置参数来确保写操作在一定数量的从节点确认后才返回客户端：\nmin-replicas-to-write：指定写操作至少需要多少个从节点确认。 min-replicas-max-lag：指定从节点的最大延迟（以秒为单位），超过这个延迟的从节点将不被视为有效确认。 例如：\nmin-replicas-to-write 2 min-replicas-max-lag 10 这表示至少需要有 2 个从节点在 10 秒内确认写操作，Redis 才会返回客户端。如果条件不满足，写操作将被拒绝，并返回错误信息。\n使用 Zookeeper 而不是 Redis 作为分布式锁解决方案\n使用 ZK 来实现分布式锁，因为 ZAB 协议是强一致的（类似 Raft），不会出现上面描述的问题\n使用 ZooKeeper 实现非扩展锁 func Lock() { for { // 尝试创建锁文件，该文件与客户端会话绑定 if Create(\u0026#34;lock\u0026#34;, data, O_Ephemeral) == true { // 如果自己是第一个创建的，会返回 true，说明拿到了锁 return } // 设置 watch，如果此时存在 lock，那么等待 ZK 通知 lock 文件被删除 if Exist(\u0026#34;lock\u0026#34;, true) == true { wait() } // 否则说明在 Create 到 Exist 调用期间，锁被释放，重新尝试 } } func Unlock(version int) { Delete(\u0026#34;lock\u0026#34;, version) } 注意 Exist 的调用，除了设置 watch 以外，还相当于双重判断了 lock 到底存不存在\n但这种实现方式存在「惊群效应」，因此被叫做「非扩展锁」\n为什么会有「惊群效应」？因为当一个客户端释放锁时，ZK 会通知所有 watch 了这个 lock file 的客户端\n也就是说，剩余的客户端被 几乎同时唤醒，重试获取锁\n使用 ZooKeeper 实现扩展锁 为了解决上面提到的「惊群效应」，这里提供另一种实现：\n// 以 Ephemeral、Sequential 模式创建锁文件 // 这里假设文件名为 lock-6 Create(\u0026#34;lock\u0026#34;, data, O_Ephemeral | O_Sequential) for { // 获取以 lock 开头的所有文件 List(\u0026#34;lock*\u0026#34;) // 如果没有比自己创建的文件名更小的（即 lock-1 ... lock-5） // 说明自己是第一个创建的，获取成功 if no lower-file { break } // 如果存在比自己小的下一个文件（即 lock-5） // 等待，直到 ZK 通知 if Exist(next-lower-file, true) { wait() } // continue } 上面的代码，理想情况下，最多循环两次即可获取锁，因为 watch 的对象仅仅是一个，因此避免了「惊群效应」\n这里讲一下 List：\nList 得到了文件的列表，我们就知道了比自己序列号更小的下一个锁文件。Zookeeper 可以确保，一旦一个序列号，比如说 27，被使用了，那么之后创建的 Sequential 文件不会使用更小的序列号。所以，我们可以确定第一次 LIST 之后，不会有序列号低于 27 的锁文件被创建，那为什么在重试的时候要再次 LIST 文件？为什么不直接跳过？\n答案是，持有更低序列号 Sequential 文件的客户端，可能在我们没有注意的时候就释放了锁，也可能已经挂了。比如说，我们是排在第 27 的客户端，但是排在第 26 的客户端在它获得锁之前就挂了。因为它挂了，Zookeeper 会自动的删除它的锁文件（因为创建锁文件时，同时也指定了 ephemeral=TRUE）。所以这时，我们要等待的是序列号 25 的锁文件释放。所以，尽管不可能再创建序列号更小的锁文件，但是排在前面的锁文件可能会有变化，所以我们需要在循环的最开始再次调用 LIST，以防在等待锁的队列里排在我们前面的客户端挂了。\n只要不存在比自己序号更低的锁文件，就成功获取到了锁\n这种实现方式感觉实现了一种「排序」等待机制，客户端获取锁的顺序，与第一次创建锁文件的顺序是一致的\n","permalink":"https://blogs.skylee.top/posts/distributed-system/distributed-lock/note/","tags":["分布式","Redis","Zookeeper"],"title":"分布式锁"},{"categories":["Golang"],"content":"Go 内存逃逸分析 什么是内存逃逸 在 C、C++ 中，如果需要将对象分配到堆区，我们使用 malloc 或者 new 手动分配内存，并在对象使用完毕后，手动调用 free 或者 delete 释放内存\n但在 Go 中，没有这个烦恼：\nGo 编译器会分析对象到底应该被分配在栈区，还是堆区 Go 的 GC 会自动回收不再使用的对象，释放内存 那么，编译器是如何判断一个对象分配的位置呢？内存逃逸分析就是用来做这个事情的\n内存逃逸会造成什么影响 内存逃逸，主要还是会产生性能方面的问题\n分配内存\n一个对象，如果分配在栈区，那么分配内存时，只需要 brk 系统调用，移动栈顶指针即可；\n如果分配在堆区，那么需要 mmap 系统调用，去 OS 申请一块内存，效率不如 brk\n此外，brk 会额外申请一部分内存，减少系统调用次数，进而减少变态开销\n释放内存\n一个对象，如果分配在栈区，那么在函数调用完成后，会自动的将该对象占用的栈空间释放（这里需要一定的汇编基础来理解）\n这意味着，我们不需要自己去释放这个对象占用的空间，这会减轻 GC 的压力（STW 时间更短）\n常见内存逃逸场景 指针 package main type Foo struct { A int B int } func Bar() *Foo { f := new(Foo) return f } func main() { Bar() } 在编译时，加上 -gcflags=-m 启用逃逸分析，输出如下：\nSky_Lee@SkyLeeMBP test % go build -gcflags=-m main.go # command-line-arguments ./main.go:8:6: can inline Bar ./main.go:13:6: can inline main ./main.go:14:5: inlining call to Bar ./main.go:9:10: new(Foo) escapes to heap ./main.go:14:5: new(Foo) does not escape 可以看到，在 return f 这里发生了内存逃逸：f 逃逸到了堆\n为什么不能将 f 分配到栈？\n如果 f 分配在栈区，在 Bar 调用结束后，f 的生命周期就结束了，会自动释放 f 占用的空间，那么外部的函数还要使用这个返回的 f，会产生未定义的行为\n空接口(any) package main import \u0026#34;fmt\u0026#34; type Foo struct { A int B int } func Bar() *Foo { f := new(Foo) // line 9 return f } func main() { f := Bar() // line 17 fmt.Println(f) } 编译时输出：\nSky_Lee@SkyLeeMBP test % go build -gcflags=-m main.go # command-line-arguments ./main.go:10:6: can inline Bar ./main.go:17:10: inlining call to Bar ./main.go:18:13: inlining call to fmt.Println ./main.go:11:10: new(Foo) escapes to heap ./main.go:17:10: new(Foo) escapes to heap ./main.go:18:13: ... argument does not escape 可以发现，在 17 行也发生了逃逸，这是因为 fmt.Println() 的参数是 any 类型，编译时无法判断其实际类型，无法确定其占用空间，只能分配在堆区\n如果传递的参数是 f.A，那么 17 行不会发生逃逸，但 f.A 仍会逃逸\n栈空间不足 一个进程的栈空间大小是有限的（使用 ulimit -s 查看），通常为 8192k\n当一个对象的大小超过栈空间的大小，那只能分配在堆区\n函数闭包 package main import \u0026#34;fmt\u0026#34; type Bar = func () func Foo() Bar { n := 0 return func() { n++ fmt.Printf(\u0026#34;n: %d\\n\u0026#34;, n) } } func main() { } 编译输出：\nSky_Lee@SkyLeeMBP test % go build -gcflags=-m main.go # command-line-arguments ./main.go:7:6: can inline Foo ./main.go:11:13: inlining call to fmt.Printf ./main.go:15:6: can inline main ./main.go:8:2: moved to heap: n ./main.go:9:9: func literal escapes to heap ./main.go:11:13: ... argument does not escape ./main.go:11:25: n escapes to heap 对象 n 发生了逃逸，因为 n 的生命周期已经与返回的 Bar 一致了，只有 Bar 被销毁，n 才会随着销毁，因此，必须将 n 分配到堆区\n传值还是传指针 package main type Foo struct { A int } func Bar0(f *Foo) { } func Bar1(f Foo) { } func Test0() { f0 := Foo{} f1 := Foo{} Bar0(\u0026amp;f0) Bar1(f1) } func Test1() { f0 := Foo{} f1 := Foo{} go Bar0(\u0026amp;f0) // line: 23 go Bar1(f1) } func main() { } 输出：\nSky_Lee@SkyLeeMBP test % go build -gcflags=-m main.go # command-line-arguments ./main.go:7:6: can inline Bar0 ./main.go:10:6: can inline Bar1 ./main.go:13:6: can inline Test0 ./main.go:16:6: inlining call to Bar0 ./main.go:17:6: inlining call to Bar1 ./main.go:27:6: can inline main ./main.go:7:11: f does not escape ./main.go:21:2: moved to heap: f0 这里不再分析为什么 Test1 的 f0 需要分配在堆区\n从性能角度考虑：传值还是传指针\n传值：存在一次拷贝带来的性能开销 传指针：仅拷贝指针本身，拷贝带来的性能影响忽略不计，但是可能逃逸到堆区 那么，在设计函数时，到底传值还是传指针呢？\n如果对象本身不大，拷贝带来的性能影响较小，传值，减小 GC 负担 如果对象占用空间较大，无法忽略拷贝的性能开销，传指针 参考资料 Go 逃逸分析 ","permalink":"https://blogs.skylee.top/posts/go/memoryescape/note/","tags":["Golang"],"title":"Go: 内存逃逸分析"},{"categories":["Distributed-System"],"content":"分布式事务主要由两部分组成。第一个是并发控制（Concurrency Control）第二个是原子提交（Atomic Commit）。\n为什么要有分布式事务 之所以提及分布式事务，是因为对于拥有大量数据的人来说，他们通常会将数据进行分割或者分片到许多不同的服务器上。假设你运行了一个银行，你一半用户的账户在一个服务器，另一半用户的账户在另一个服务器，这样的话可以同时满足负载分担和存储空间的要求。对于其他的场景也有类似的分片，比如说对网站上文章的投票，或许有上亿篇文章，那么可以在一个服务器上对一半的文章进行投票，在另一个服务器对另一半进行投票。\n如果执行事务时，涉及到的数据存放在不同的服务器上面，那么这个事务就上升为「分布式事务」了\n先后原子性（Before-or-After Atomicity） 我们用一个「转账」案例来描述 先后原子性 的定义\n定义「转账」的过程如下：\n# TRANSFER(A, B, 10): A 向 B 转 10 元 procedure TRANSFER (reference debit_account, reference credit_account, amount) debit_account ← debit_account - amount credit_account ← credit_account + amount 假设 debit_account 和 credit_account 被分片，即存放在不同的服务器上\n此时，有两个客户端分别执行（A 初始值为 300，B 初始值为 100）：\nTRANSFER (A, B, $10) 和\nTRANSFER (B, C, $25) 那么，最终结果可能有以下几种：\n上图展示了可能发生的几种时间序列，容易发现，case1 和 case2 是合法的，剩下的几个 case 都是不合法的\n我们的目标是确保前两个时间序列之一实际发生。实现这一目标的一种方法是使步骤 1-1 和 1-2 具有原子性，步骤 2-1 和 2-2 同样具有原子性。在原始程序中\n也就是说：当执行 TRANSFER 操作时，我们希望它是一个 事务，因为我们不希望将 debit_account、credit_account 的中间值暴露给其它任何人看到\n先后原子性 是一种更为普遍的约束，即 同时 操作 相同 数据的多个动作 不应相互干扰\nConcurrent actions have the before-or-after property if their effect from the point of view of their invokers is the same as if the actions occurred either completely before or completely after one another.\n由于我们对先后原子性的定义是每个先后动作的行为就像它在其他先后动作之前或之后完全运行一样，因此先后原子性直接导致了这种正确性概念。\n换句话说，先后原子性的效果是 将动作串行化，因此先后原子性保证了协调的正确性。\n另一种表达这一思想的方式是说，当并发动作具有先后属性时，它们是可串行化的：存在某种串行顺序，如果遵循该顺序，将导致相同的最终状态。 因此在图 9.2 中，case1 和 case2 的顺序可以从串行顺序得到，但 case3 到 case6 的动作不能。\n并发控制 数据库的事务处理系统会使用锁，在事务使用任何数据之前，它需要获得数据的锁。\n如果一些其他的事务已经在使用这里的数据，锁会被它们持有，当前事务必须等待这些事务结束，之后当前事务才能获取到锁。\n下面讨论一种很常见的锁：两阶段锁（Two-Phase Locking）\n两阶段锁分为两个阶段：\n获取锁 事务结束前一直持有锁 当一个事务开始后，在访问任何数据前，都要获取该数据对应的锁（可能是行锁，也可能是粒度更大的锁）\n在事务执行中，必须持有锁，直到事务提交或终止\n例如对于之前提到的转账案例：\n// thread-1 Begin() // 阶段一 GetLock(A) Read(A) GetLock(B) Read(B) Write(B) Write(A) Commit() // 阶段二 Unlock(A) Unlock(B) // thread-1 Begin() GetLock(B) Read(B) GetLock(C) Read(C) Write(B) Write(C) Commit() Unlock(C) Unlock(B) 两阶段锁的本质实际上就是「迫使」事务 串行 执行，以满足「先后原子性」\n为什么要提交或终止事务以后，才释放锁？\n假设读取或写入一条数据 A 以后，接下来不会读取或写入 A，为什么不能立即释放 A 的锁？\n这样并发能力不是更强吗？\n如果在提交或终止事务前，释放锁，这种方式类似 MySQL 的 Read Uncommitted 事务隔离级别，会读取到其它事务没有提交的数据，不满足「先后原子性」\n两阶段锁很容易死锁，怎么解决？\n例如：\n// thread-1 Begin() GetLock(A) Read(A) GetLock(B) Read(B) Commit() Unlock(A, B) // thread-2 Begin() GetLock(B) Read(B) GetLock(A) Read(A) Commit() Unlock(A, B) 如果 thread1 持有锁 A，thread 2 持有锁 B：\n当 thread1 想要获取锁 B 时，发现已经被占用，等待 当 thread2 想要获取锁 A 时，发现已经被占用，等待 陷入死锁，怎么解决？\n一般来说，数据库都会「识别」到死锁的情况，通常的解决方案是：让其中一个事务回滚（后面重试即可），这样另一个事务就能获取到锁，打破局面\n原子提交 单个机器上的事务提交很简单，但是扩展到多个机器，也就是分布式事务的提交，就比较麻烦了\n因为在分布式的环境下，一些机器可能出现故障，或者网络出现问题等等\n如何保证即使出现错误，也能保证我们的 事务提交操作也是原子的，是分布式事务的一大挑战\n常用的解决方案是：两阶段提交（2PC）\n角色 在 2PC 中，有两种角色：\n参与者（worker） 协调者（coordinator） 参与者是实际的事务执行者，负责执行协调者发来的事务请求\n阶段 两阶段提交，分为：\n事务准备阶段 事务提交阶段 执行过程 假设多事务由协调者 Alice 请求 Worker Bob、Charles 和 Dawn 分别执行组件事务 X、Y 和 Z。\n首先，客户端开启一个新的事务，对 X、Y、Z 执行读取或者写入操作\n这会要求协调者 Alice 向 Bob 发送：\nFrom:Alice To: Bob Re: my transaction 271 Please do X as part of my transaction 类似的消息也发送给 Charles 和 Dawn，也提及事务 271，并请求他们分别执行 Y 和 Z。\n像普通的 RPC 一样，如果 Alice 在合理的时间内没有收到一个或多个 Worker 的响应，她会向未响应的 Worker 重新发送消息，尽可能多次地 重新发送 以引起响应。\nWorker 在收到这样的请求时，会 检查是否有重复请求，然后，它进行请求动作的预提交部分，向 Alice 报告这一部分已经顺利完成：\nFrom:Bob To: Alice Re: your transaction 271 My part X is ready to commit. 假设客户端要执行的事务就这些，此时客户端 想要提交事务，于是客户端会告诉协调者 Alice，请帮我提交事务 271\nAlice 收到客户端的请求后，开始事务提交过程，也就是 2PC 的第一阶段\n协调者会给所有参与者发送一条 Prepare 消息：\nTwo-phase-commit message #1: From:Alice To: Bob Re: my transaction 271 PREPARE to commit X. 参与者收到 Prepare 消息后，如果自己能够提交事务，那么会返回一个 YES 消息给协调者：\nTwo-phase-commit message #2: From:Bob To:Alice Re: your transaction 271 I am PREPARED to commit my part. Have you decided to commit yet? Regards. 当协调者收到了所有参与者的响应后，进入 2PC 的第二阶段\n协调者会给所有参与者发送一条 Commit 消息，要求各参与者提交自己部分的事务：\nTwo-phase-commit message #3 From:Alice To:Bob Re: my transaction 271 My transaction committed. Thanks for your help. 参与者收到 Commit 消息后，会提交自己部分的事务，并 释放所有锁\n为了遵循「两阶段锁」原则，参与者在开启子事务时，操作任何数据前都要获取锁\n直到协调者发来一条 Commit 或者 Abort 消息，然后提交或者终止子事务后，才会释放所有锁\n大部分 2PC 的实现要求参与者在 Commit 或 About 一个事务后，给协调者发送一条 ACK，进而让协调者 安全地删除部分数据（这部分数据是啥后面会讲）\n故障恢复 来看看几个故障的场景：\nWorker 崩溃 情况 1: Worker 在发送 YES 消息前崩溃\n这种情况，Coordinator 会尝试 重发 Prepare 消息给 Worker，直到 Worker 响应，才能进入 2PC 的第二阶段\n当 Worker 重启以后，收到了 Prepare 消息，但是自己却没有办法发送 YES，因为重启后，内存的数据被清除了\n此时 Worker 被授予 单方面 Abort 事务的权限，可以直接返回一个 No 给 Coordinator\n当然，一个 Worker 的重启可能会花费较长的时间，为了避免无限等待，Coordinator 也可以单方面 Abort 事务\n情况 2: Worker 在发送 YES 消息后崩溃\n这种情况，Coordinator 可以收到全部的 YES 消息，并进入 2PC 的第二阶段\nCoordinator 给所有 Worker 发送 Commit 消息，但有一个 Worker 由于宕机正在重启，无法及时响应\nCoordinator 必须 重发 Commit 消息给 Worker，直到 Worker 响应\n当 Worker 重启以后，收到了 Commit 消息，此时 Worker 还能单方面 Abort 事务吗？\n显然不能，因为其它 Worker 已经提交了子事务，宕机的 Worker 也必须提交自己的子事务\n因此，Worker 必须 在发送 YES 消息前，将「事务的状态记录」持久化 到磁盘\n情况 3: Worker 在发送 ACK 后崩溃\nWorker 有可能在处理完 Commit 之后就崩溃了。但是这样的话，子事务已经提交。这样的话，故障重启就不需要做任何事情，因为事务已经完成了。\n因为没有收到 ACK，事务协调者会再次发送 Commit 消息。当 Worker 重启之后，收到了 Commit 消息时，它可能已经将 Log 中的修改写入到自己的持久化存储中、释放了锁、并删除了有关事务的 Log。\n所以我们需要关心，如果 Worker 收到了同一个 Commit 消息两次，该怎么办？\n这里 Worker 可以记住事务的信息，但是这 会消耗内存，所以实际上 Worker 会完全忘记已经在磁盘上持久化存储的事务的信息。对于一个它不知道事务的 Commit 消息，Worker 会 无条件 ACK 这条消息\n总结下来：\nWorker 必须 在发送 YES 消息前，将「事务的状态记录」持久化 到磁盘 一个 Worker 的重启可能会花费较长的时间，为了避免无限等待，Coordinator 也可以单方面 Abort 事务 为了及时释放内存和磁盘空间，当 Worker 提交了子事务后，需要删除磁盘上持久化存储的事务的信息 对于一个它不知道事务的 Commit 消息，Worker 会 无条件 ACK 这条消息 Coordinator 崩溃 情况 1: Coordinator 在发送 Prepare 时崩溃\n这个情况又可以细分为：\n发送 Prepare 前 发送了若干个 Prepare 所有 Prepare 均发送完毕 无论哪种情况，客户端在超时没有收到 Coordinator 响应，会尝试重发「事务提交」请求\n当 Coordinator 重启后，收到了客户端的「事务提交」请求后，再发送 Prepare 就行\n但这要求 Worker 具有辨别重复请求的能力\n情况 2: Coordinator 在发送 Commit 后崩溃\n这种情况会导致某个 Worker 一直收不到 Coordinator 的 Commit 消息\n如果 Worker 收到了 Prepare 消息，并回复了 Yes，在等待了 10 秒钟或者 10 分钟之后还没有收到 Commit 消息，它能单方面的决定 Abort 事务吗？\n不能\n因为其它 Worker 有可能已经提交了自己的子事务，所以，Worker 在这里 必须无限等待，直到收到 Coordinator 的 Commit 消息\n这种场景也被叫做 「Block」，是 2PC 最大的弊端，决定了 2PC 本身的可用性\n这里的 Block 行为是两阶段提交里非常重要的一个特性，并且它不是一个好的属性。因为它意味着，在特定的故障中，你会很容易的陷入到一个需要等待很长时间的场景中，在等待过程中，你会一直持有锁，并阻塞其他的事务。所以，人们总是尝试在两阶段提交中，将这个区间尽可能快的完成，这样可能造成 Block 的时间窗口也会尽可能的小。所以人们尽量会确保协议中这部分尽可能轻量化，甚至对于一些变种的协议，对于一些特定的场景都不用等待。\n也就是说，要想打破这个 block 的局面，只能等待 Coordinator 重启后重发 Commit 消息\n因此，Coordinator 在开启 2PC 前，必须 持久化 事务相关元数据，发送 Commit 前，需要在磁盘中标记 \u0026ldquo;Commit 未完成\u0026rdquo; 状态（具体怎么实现我也不清楚，但应该有一个类似的步骤）\n那什么时候，Coordinator 可以安全删除这部分持久化的元数据呢？\n当 Coordiantor 收到所有 Worker 的 ACK 后，说明所有子事务均已提交，此时可以删除这部分元数据\n为什么要 2PC？ 为什么要两阶段提交？只使用一个阶段不行吗？\n更具体的说：Coordinator 收到客户端的 Commit 请求后，不需要 Prepare 阶段，而是直接要求 Worker「提交」子事务不可以吗？\n先来看看网上主流的说法：\n2PC 保证了一致性和原子性：\n一致性：2PC 的第一阶段（准备阶段）确保所有参与者都同意提交事务之前，协调者不会进行实际的提交操作。这确保了如果任何参与者不能提交事务（由于检查失败、资源不足等原因），整个事务将被回滚，从而保证了全局一致性。\n原子性：通过两个阶段，2PC 能够确保事务的提交要么在所有参与者上成功执行，要么在所有参与者上回滚，没有中间状态。单独使用一个阶段无法实现这种全局原子性。\n我个人认为，使用 2PC 而不是 1PC 的原因，主要是：降低 Block 的概率\nWorker 可能由于各种原因（例如内存、磁盘空间不足，网络问题）导致 无法提交事务\n假设采用 一阶段提交，即 Coordinator 直接给所有 Worker 发送 Commit 请求\n如果某个 Worker 超时未响应，为了保证一致性和原子性，Coordinator 需要 阻塞 等待，直到该 Worker 响应（因为其它 Worker 已经提交子事务） 如果某个 Worker 由于某种原因返回 Abort，Coordinator 需要要求其它 Worker 回滚子事务，但是此前 Worker 已经提交（应用）了子事务，想要 回滚很困难 因此，需要使用两阶段提交，可以在 Prepare 阶段：\n排除某个 Worker 的网络问题（如果一直没有响应，Coordinator 有权回滚整个事务） 排除某个 Worker 要求 Abort 的情况 这样，阻塞的可能性大大降低，性能自然得到提升\n总结 使用 两阶段锁 + 两阶段提交，可以满足事务的「先后原子性」\n两阶段提交实现了原子提交。它在大量的将数据分割在多个服务器上的分片数据库或者存储系统中都有使用。\n两阶段提交可以支持读写多条记录，一些更特殊的存储系统不允许你在多条记录上支持事务。对于这些不支持事务中包含多条数据的系统，你就不需要两阶段提交。但是如果你需要在事务中支持多条数据，并且你将数据分片在多台服务器之上，那么你必须支持两阶段提交。\n为什么 2PC 这么慢?\n大量的网络请求：对于 N 个 Worker，一次事务提交，最理想的情况，也需要 4N 个 RPC call 磁盘写入：2PC 要求 Coordinator 和 Worker 需要 持久化 相关数据以实现错误恢复 存在 Block 的情况：如果发生 Block，只能等待 Coordinator 恢复 与 Raft 对比\n下面的分析来自 Robert 教授的总结部分 两阶段提交的架构中，本质上是有一个 Leader（事务协调者），将消息发送给 Follower（事务参与者），Leader 只能在收到了足够多 Follower 的回复之后才能继续执行。这与 Raft 非常像，但是，这里协议的属性与 Raft 又非常的不一样。这两个协议解决的是完全不同的问题。\n因为 Raft 的每个 Server 做的都是相同的事，存放的也是几乎相同的数据；而 2PC，每个 Worker 存放的数据是完全不同的\n所以，Raft 通过复制可以不用每一个参与者都在线，而两阶段提交每个参与者都做了不同的工作，并且每个参与者的工作都必须完成，所以两阶段提交对于可用性没有任何帮助。Raft 完全就是可用性，而两阶段提交完全不是高可用的，系统中的任何一个部分出错了，系统都有可能等待直到这个部分修复。\n怎么结合 Raft 实现 2PC 的高可用\n一图胜千言：\n每个集群通过 Raft 实现冗余复制，保证可用性\n参考资料 Lecture 12 - Distributed Transaction 9.1.5 Before-or-After Atomicity: Coordinating Concurrent Threads 9.6.3 Multiple-Site Atomicity: Distributed Two-Phase Commit ","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/distributed-transaction/note/","tags":["分布式","MIT-6.824"],"title":"分布式事务: Distributed Transaction"},{"categories":["Distributed-System"],"content":"CR(Chain Replication) CR 是一种分布式存储协议，具有与 Raft 不一样的特性\n基本原理 从名字可以看出，CR 的网络拓扑就像一条「链」一样：\n每个节点以「链」的方式排列，类似双向链表\n写请求处理 客户端的写请求会发给 Head 节点处理\n当 Head 节点收到一个写请求，会将这个写请求发送给后继节点，以此类推，最终到达 Tail 节点\nTail 节点收到了这个写请求后，会发送 ACK 给前驱节点，以此类推，最终到达 Head 节点\nHead 节点收到 ACK 后，说明所有节点都完成了复制，于是 Commit 这条请求，并响应客户端\n如果发生丢包，或者某个节点挂掉，导致 Head 没有收到 ACK，客户端请求超时怎么办？\n一般来说，客户端请求超时会 重发 请求，这就要求 Head 节点具有 辨别重复请求 的能力，因为我们不希望重复执行请求\n读请求处理 客户端的读请求会发给 Tail 节点处理\nCR 这种方式，相当于将读写请求分离了，可以降低 Head 的压力，而 Raft 协议读写请求都在 Leader 进行，压力较大\n一致性模型 CR 是 强一致性 保障的，读写请求均满足「线性一致」\n对于读请求，只会由 Tail 节点处理，这意味着读取的数据肯定都是 Commit 的数据\n对于写请求，只有 所有节点 都完成了复制，Head 节点才会收到 ACK，才会响应客户端\n错误恢复 与 Raft 很多错误场景不同，CR 的错误恢复比较简单，只需要考虑三种场景：\nHead 挂了 Tail 挂了 中间节点挂了 下面的话来自 Robert 教授 ：\n如果 HEAD 出现故障，作为最接近的服务器，下一个节点可以接手成为新的 HEAD，并不需要做任何其他的操作。对于还在处理中的请求，可以分为两种情况：\n对于任何已经发送到了第二个节点的写请求，不会因为 HEAD 故障而停止转发，它会持续转发直到 commit。 如果写请求发送到 HEAD，在 HEAD 转发这个写请求之前 HEAD 就故障了，那么这个写请求必然没有 commit，也必然没有人知道这个写请求，我们也必然没有向发送这个写请求的客户端确认这个请求，因为写请求必然没能送到 TAIL。所以，对于只送到了 HEAD，并且在 HEAD 将其转发前 HEAD 就故障了的写请求，我们不必做任何事情。或许客户端会重发这个写请求，但是这并不是我们需要担心的问题。 如果 TAIL 出现故障，处理流程也非常相似，TAIL 的前一个节点可以接手成为新的 TAIL。所有 TAIL 知道的信息，TAIL 的前一个节点必然都知道，因为 TAIL 的所有信息都是其前一个节点告知的。\n中间节点出现故障会稍微复杂一点，但是基本上来说，需要做的就是：将故障节点从链中移除。\n或许有一些写请求被故障节点接收了，但是还没有被故障节点之后的节点接收，所以，当我们将其从链中移除时，故障节点的前一个节点或许需要重发最近的一些写请求给它的新后继节点。\n假设第二个节点不能与 HEAD 进行通信，第二个节点能不能直接接管成为新的 HEAD，并通知客户端将请求发给自己，而不是之前的 HEAD？\n这个问题描述的其实就是「网络分区」问题\n如果采取这种方式，那么一条链上可能会存在 两个 Head，发生了「脑裂」\nHEAD 还在正常运行，同时 HEAD 认为第二个节点挂了。然而第二个节点实际上还活着，它认为 HEAD 挂了。所以现在他们都会认为，另一个服务器挂了，我应该接管服务并处理写请求。因为从 HEAD 看来，其他服务器都失联了，HEAD 会认为自己现在是唯一的副本，那么它接下来既会是 HEAD，又会是 TAIL。第二个节点会有类似的判断，会认为自己是新的 HEAD。所以现在有了脑裂的两组数据，最终，这两组数据会变得完全不一样。\n这肯定不是我们希望发生的，也就是说：原始的 CR 不具有应对「网络分区」和「脑裂」的能力\n这意味着，CR 在实际应用中，不能单独使用\n配置中心(Configuration Manager) 为了应对 CR 的「网络分区」和「脑裂」问题，一个可行的思路是：我们不能让 CR 链上的节点「自认为」某个节点挂了，而是需要一种「共识」\n这种「共识」，需要依靠「外部的权威」（External Authority），来决定一个节点，到底是活的还是死的\n我们将这种「外部的权威」称作「配置中心」(Configuration Manager)\n配置中心本身必须是 容错 的，可以是 Raft，也可以是 Paxos\n在 CR 的场景下，使用的是 ZooKeeper\nZooKeeper 在之前的文章也有提到，它基于 ZAB 协议，与 Raft 是类似的一种方案\nConfiguration Manager 通告给所有参与者整个链的信息，所以所有的客户端都知道 HEAD 在哪，TAIL 在哪，所有的服务器也知道自己在链中的前一个节点和后一个节点是什么。现在，单个服务器对于其他服务器状态的判断，完全不重要。假如第二个节点真的挂了，在收到新的配置之前，HEAD 需要不停的尝试重发请求。节点自己不允许决定谁是活着的，谁挂了。\n这种架构极其常见，这是正确使用 Chain Replication 和 CRAQ 的方式。在这种架构下，像 Chain Replication 一样的系统 不用担心网络分区和脑裂，进而可以使用类似于 Chain Replication 的方案来构建非常高速且有效的复制系统。\n缺点 写入速率受限\nCR 同步写请求是以链的方式同步的，串行化，速度较慢\n并且 CR 写入速率还会受到「慢」的节点的限制\n因为 CR 需要等待 所有 节点复制完毕，才会 commit 一条写请求\n而 Raft 这种使用「过半票决」的协议，就可以在一定程度上避免这个问题\n无法独立使用\n前面提到 CR 必须依靠「配置中心」，本身不具备应对网络分区和脑裂的能力\nCRAQ(Chain Replication with Apportioned Queries) CRAQ 是对 CR 的改进\n基本原理 CRAQ 在 CR 的基础上增加了分摊查询（Apportioned Queries），即读请求不一定每次都在 Tail 节点进行，大幅提高读请求的处理能力\n写入基本原理如下：\n在 CR 的基础上，每个节点保存 多版本 的对象（有点类似 MySQL 的 MVCC），每个对象有一个版本号，以及标记（clean or dirty）\n当一个节点收到写请求时，除了写入以外，还要处理标记：\n如果本节点为 Tail 节点，那么标记为 clean 否则，标记为 dirty 当一个节点收到了 Tail 节点的 ACK 后，可以将 dirty 标记修改为 clean 标记\n读取基本原理如下：\n客户端可以在任意节点执行读请求\n当一个节点收到了读请求，需要判断读取对象的标记：\n如果是 clean，那么直接返回客户端结果，否则： 询问 Tail 节点，该对象的版本号 根据 Tail 节点返回的版本号，决定返回哪个对象给客户端 优势 读请求分发 从上面的分析可以看出，CRAQ 最主要的优势就是读请求分发，大幅提高读请求的吞吐\n如果是 clean，直接返回 如果不是 clean，只会询问 Tail 版本号，而不是完整数据，降低了 Tail 的压力 广播降低写延迟 CR 节点间通信是「链式」的，串行化\n而 CRAQ 相较于 CR，实现了 广播通信，并行化：\n复制不需要串行沿链传播，而是广播到整个链 只有元数据才需要沿链传播 若节点没收到广播消息，它会从前驱节点拉数据（收到提交消息后，传播提交消息前） 尾节点也广播 ACK 消息给前面的节点，若前面的没收到，则会在下一次读时询问尾节点，从而让数据 clean 这种方式提高了写请求同步的效率，进一步提高写请求的吞吐，降低写延迟\n一致性模型 容易发现，CRAQ 与 CR 一致，读写均满足线性一致性\n这也是 CRAQ 最吸引人的地方：既能满足线性一致性，又能提供读请求的高吞吐\n参考资料 Lecture 09 - More Replication, CRAQ Object Storage on CRAQ: High-throughput chain replication for read-mostly workloads 论文阅读-Object Storage on CRAQ: High-throughput chain replication for read-mostly workload ","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/craq/note/","tags":["分布式","MIT-6.824"],"title":"CRAQ: High-throughput chain replication for read-mostly workloads"},{"categories":["Distributed-System"],"content":"APIs ZK 的使用场景 ZK 主要用于 单个数据中心 内：\n管理集群的配置信息 选举 Leader Test-And-Set 服务的实现 \u0026hellip; Znode Zookeeper 的 API 某种程度上来说像是一个文件系统。它有一个层级化的目录结构，有一个根目录（root），之后每个应用程序有自己的子目录。比如说应用程序 1 将自己的文件保存在 APP1 目录下，应用程序 2 将自己的文件保存在 APP2 目录下，这些目录又可以包含文件和其他的目录。\n文件和目录都被称为 Znodes。Zookeeper 中包含了 3 种类型的 Znode：\nRegular Znode：普通 Znode，创建后永久存在，不会自动删除 Ephemeral Znode：这种 Znode 与创建它的客户端会话绑定，如果客户端挂掉（超时没有心跳），ZK 会将这个 Znode 删掉 Sequential Znode：顺序 Znode，命名格式为 ，当若干个客户端并发的创建名叫 filename 的 Sequential Znode 时，ZK 保证生成的 num 是递增的，不会重复 APIs ZK 的 API 很少，但很强大，通过组合这些 API 可以实现很多功能，这里直接用 Robert 教授的话 来展示：\nCREATE(PATH，DATA，FLAG)。入参分别是文件的全路径名 PATH，数据 DATA，和表明 znode 类型的 FLAG。这里有意思的是，CREATE 的语义是排他的。也就是说，如果我向 Zookeeper 请求创建一个文件，如果我得到了 yes 的返回，那么说明这个文件之前不存在，我是第一个创建这个文件的客户端；如果我得到了 no 或者一个错误的返回，那么说明这个文件之前已经存在了。所以，客户端知道文件的创建是排他的。在后面有关锁的例子中，我们会看到，如果有多个客户端同时创建同一个文件，实际成功创建文件（获得了锁）的那个客户端是可以通过 CREATE 的返回知道的。\nDELETE(PATH，VERSION)。入参分别是文件的全路径名 PATH，和版本号 VERSION。有一件事情我之前没有提到，每一个 znode 都有一个表示当前版本号的 version，当 znode 有更新时，version 也会随之增加。对于 delete 和一些其他的 update 操作，你可以增加一个 version 参数，表明当且仅当 znode 的当前版本号与传入的 version 相同，才执行操作。当存在多个客户端同时要做相同的操作时，这里的参数 version 会非常有帮助（并发操作不会被覆盖）。所以，对于 delete，你可以传入一个 version 表明，只有当 znode 版本匹配时才删除。\nEXIST(PATH，WATCH)。入参分别是文件的全路径名 PATH，和一个有趣的额外参数 WATCH。通过指定 watch，你可以监听对应文件的变化。不论文件是否存在，你都可以设置 watch 为 true，这样 Zookeeper 可以确保如果文件有任何变更，例如创建，删除，修改，都会通知到客户端。此外，判断文件是否存在和 watch 文件的变化，在 Zookeeper 内是原子操作。所以，当调用 exist 并传入 watch 为 true 时，不可能在 Zookeeper 实际判断文件是否存在，和建立 watch 通道之间，插入任何的创建文件的操作，这对于正确性来说非常重要。\nGETDATA(PATH，WATCH)。入参分别是文件的全路径名 PATH，和 WATCH 标志位。这里的 watch 监听的是文件的内容的变化。\nSETDATA(PATH，DATA，VERSION)。入参分别是文件的全路径名 PATH，数据 DATA，和版本号 VERSION。如果你传入了 version，那么 Zookeeper 当且仅当文件的版本号与传入的 version 一致时，才会更新文件。\nLIST(PATH)。入参是目录的路径名，返回的是路径下的所有文件。\n使用 ZooKeeper 实现计数器 因为 ZK 的读取不满足线性一致，要想正确实现计数器递增操作，需要保证 count 的 读取与写入 是一个原子操作，以保证多个客户端并发读写的正确性：\nfor { // 获取原始 count 和对应的版本号 count, version := GetData(\u0026#34;count-file\u0026#34;, true) // 设置 count 为 count + 1，当且仅当此时 count-file 的 version 与此前一致才更新 if SetData(\u0026#34;count-file\u0026#34;, count+1, version) == true { break } } 时间复杂度为 $O(n^2)$，因为一次循环，所有客户端中只有一个能够成功，其它全部都会失败，假设有 1000 个客户端，在最坏的条件下，一个客户端执行一次递增操作，需要 1000 次循环\n因此仅适合低负载场景\n使用 ZooKeeper 实现非扩展锁 这里讨论的有点像「分布式锁」的实现\nfunc Lock() { for { // 尝试创建锁文件，该文件与客户端会话绑定 if Create(\u0026#34;lock\u0026#34;, data, O_Ephemeral) == true { // 如果自己是第一个创建的，会返回 true，说明拿到了锁 return } // 设置 watch，如果此时存在 lock，那么等待 ZK 通知 lock 文件被删除 if Exist(\u0026#34;lock\u0026#34;, true) == true { wait() } // 否则说明在 Create 到 Exist 调用期间，锁被释放，重新尝试 } } func Unlock(version int) { Delete(\u0026#34;lock\u0026#34;, version) } 注意 Exist 的调用，除了设置 watch 以外，还相当于双重判断了 lock 到底存不存在\n但这种实现方式存在「惊群效应」，因此被叫做「非扩展锁」\n为什么会有「惊群效应」？因为当一个客户端释放锁时，ZK 会通知所有 watch 了这个 lock file 的客户端\n也就是说，剩余的客户端被 几乎同时唤醒，重试获取锁\n使用 ZooKeeper 实现扩展锁 为了解决上面提到的「惊群效应」，这里提供另一种实现：\n// 以 Ephemeral、Sequential 模式创建锁文件 // 这里假设文件名为 lock-6 Create(\u0026#34;lock\u0026#34;, data, O_Ephemeral | O_Sequential) for { // 获取以 lock 开头的所有文件 List(\u0026#34;lock*\u0026#34;) // 如果没有比自己创建的文件名更小的（即 lock-1 ... lock-5） // 说明自己是第一个创建的，获取成功 if no lower-file { break } // 如果存在比自己小的下一个文件（即 lock-5） // 等待，直到 ZK 通知 if Exist(next-lower-file, true) { wait() } // continue } 上面的代码，理想情况下，最多循环两次即可获取锁，因为 watch 的对象仅仅是一个，因此避免了「惊群效应」\n这里讲一下 List：\nList 得到了文件的列表，我们就知道了比自己序列号更小的下一个锁文件。Zookeeper 可以确保，一旦一个序列号，比如说 27，被使用了，那么之后创建的 Sequential 文件不会使用更小的序列号。所以，我们可以确定第一次 LIST 之后，不会有序列号低于 27 的锁文件被创建，那为什么在重试的时候要再次 LIST 文件？为什么不直接跳过？\n答案是，持有更低序列号 Sequential 文件的客户端，可能在我们没有注意的时候就释放了锁，也可能已经挂了。比如说，我们是排在第 27 的客户端，但是排在第 26 的客户端在它获得锁之前就挂了。因为它挂了，Zookeeper 会自动的删除它的锁文件（因为创建锁文件时，同时也指定了 ephemeral=TRUE）。所以这时，我们要等待的是序列号 25 的锁文件释放。所以，尽管不可能再创建序列号更小的锁文件，但是排在前面的锁文件可能会有变化，所以我们需要在循环的最开始再次调用 LIST，以防在等待锁的队列里排在我们前面的客户端挂了。\n只要不存在比自己序号更低的锁文件，就成功获取到了锁\n这种实现方式感觉实现了一种「排序」等待机制，客户端获取锁的顺序，与第一次创建锁文件的顺序是一致的\n参考资料 Lecture 09 - More Replication, CRAQ ","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/zookeeper/apis/note/","tags":["分布式","MIT-6.824","ZooKeeper"],"title":"ZooKeeper APIs"},{"categories":["Distributed-System"],"content":"线性一致 首先要了解一个概念：线性一致\n一个系统的执行历史是一系列的客户端请求，或许这是来自多个客户端的多个请求。如果 执行历史 整体可以按照 一个顺序 排列，且排列顺序 与客户端请求的实际时间相符合，那么它是线性一致的。\n有关「线性一致」的几个例子，可以看看 Robert 教授的讲座 总的来说：\n线性一致 是对于存储系统中 强一致 的一种标准定义 线性一致的定义是 有关历史记录 的定义，而 不是系统的定义。所以我们不能说一个系统设计是线性一致的，我们只能说请求的历史记录是线性一致的。 强一致系统表现的也与系统中只有一份数据的拷贝一样，这与线性一致的定义非常接近。所以，可以合理的认为强一致与线性一致是一样的。 ZooKeeper 相比 Raft 来说，Raft 实际上就是一个库。你可以在一些更大的多副本系统中使用 Raft 库。但是 Raft 不是一个你可以直接交互的独立的服务，你必须要设计你自己的应用程序来与 Raft 库交互。\n所以这里有一个有趣的问题：是否有一些有用的，独立的，通用的系统可以帮助人们构建分布式系统？是否有这样的服务可以包装成一个任何人都可以使用的独立服务，并且极大的减轻构建分布式应用的痛苦？\n所以，第一个问题是，对于一个通用的服务，API 应该是怎样？我不太确定类似于 Zookeeper 这类软件的名字是什么，它们可以被认为是一个通用的 协调服务（General-Purpose Coordination Service）。\n通过上面 Robert 教授说的这段话，可以知道：ZooKeeper 是一个分布式协调服务，用于帮助建立分布式系统\n作为一个多副本系统，Zookeeper 本身是一个 容错的 ，通用的协调服务，它与其他系统一样，通过 多副本 来完成容错。\n即然涉及到多副本，我们自然能想到：n 个副本能不能带来（近似） n 倍的性能提升？\n对于 Raft 来说，n 个副本不仅不会带来 n 倍的性能提升，反而会导致性能下降\n因为副本数越多，Leader 的压力就会越大，而 Raft 的 读写操作均在 Leader 进行，因此整体的吞吐肯定是下降的\n那 ZooKeeper 呢？\n这里直接给出结论：ZK 的 读 性能会随着副本数的增加而增加\n这是一个好消息，因为 绝大多数的操作都是读操作，仅有少部分操作是写操作\n或许你已经大概猜到为什么 ZK 的读性能会随着副本数的增加而增加，而写性能不会\n如果我们将 读请求分发 到不同的节点（即 Leader 节点和 Follower 节点），就可以获得 n 倍的读性能提升\n读请求分发到不同节点，是否存在不一致的问题？也就是说，还遵循「线性一致」吗？\nZK 的一致性保证 ZK 的论文在 2.3 小节提到：\nZooKeeper 有两个基本的顺序保证：\n线性写：所有更新 ZooKeeper 状态的操作是串行的，先来先服务 FIFO 客户端顺序：来自单个客户端的所有请求按客户端发送的顺序依次执行。 也就是说，ZK 的一致性保障可以总结为：\n保证 所有客户端 写请求满足「线性一致」 保证 单个客户端 所有请求满足「线性一致」 从这里我们可以回答上面的问题：ZooKeeper 不保证所有客户端的读请求的「线性一致」，也就是说：客户端可能读取到 过期 的数据，这也是我们常说的「最终一致性」保障\n事实上，对于任何分布式系统来说，写请求的「线性一致」是基本要求，如果写请求无法保证「线性一致」，那这个系统本身就没有什么意义，我们完全无法预料最终的结果是什么\n相反，读请求可以根据「使用需求」来做 trade-off：\n如果要强一致性保障，那么需要舍弃性能 如果需要读性能，那么需要舍弃一致性保障（只能最终一致） 如何保证写请求的线性一致 ZooKeeper 写请求线性一致保障实现原理，与 Raft 是类似的：\n所有的写请求都由 Leader 处理并提交，这样可以确保写请求的全序。 ZAB 协议 确保写请求只有在被过半追随者确认后才会被提交，从而保证了写请求的持久性和一致性。 事务日志和同步机制确保了在服务器崩溃或网络分区后，系统可以恢复并保持一致的状态。 ZAB 协议，与 Raft 协议类似，用于节点间数据同步，核心思想都是「过半票决」\n如何保证单个客户端所有请求的线性一致 Zookeeper 通过 ZXID 来保障单个客户端请求的线性一致\n上面的图描述了当前 ZK 集群的 Logs 状态\n假设 Client 在 Leader 执行了一次读请求：\nLeader 返回执行当前请求时，对应的 ZXID = 4 Client 记录下 ZXID，并取最大值保存到本地 为了保证请求的线性一致，客户端后续的请求，对应的 ZK 节点必须满足： LastZXID \u0026gt;= ClientZXID\n当 ZK 节点接收到一个请求后，首先需要判断客户端携带的 ZXID\n如果客户端携带的 ZXID 比本地日志的最大 ZXID 还要大，那么就无法为这个客户端提供服务，例如上图中的 Follower-2\n对于这种情况，客户端有两种选择：等待；或者选择其它 ZK 节点\n同步操作（sync） 通过前面的介绍，我们知道客户端可能读取到「过期」的数据（Follower 没有同步到最新位置，就为 Client 提供读服务）\n如果客户端想要读取 最新的 数据怎么办？\nZooKeeper 提供了一个 Sync API 实现这一点\nSync API 实际上就是一个 写请求，只不过这个写请求并不实际写入任何数据\n我们知道 ZK 保证所有客户端写请求的线性一致性，因此调用 Sync API 后，必须要 半数以上 的节点确认了这条 Sync 后，Sync API 才会返回\n那也仅仅是半数节点啊，如果客户端请求的是处于「另一个分区」的 ZK Node 呢？\n别急，Sync 返回的同时不是还会带上 ZXID 吗，客户端下次执行读请求时，带上这个 ZXID，只有请求的 ZK Node 同步到这个位置以后才能提供服务\n通过上面的分析，得出结论：调用 Sync 后，可以保证客户端看到 Sync 对应的状态，可以合理地认为是最新的\n就绪文件（Ready File） ZooKeeper 最广泛的应用场景，就是 管理集群的元数据 了\nZK 将这些数据以「文件」的形式存储，也被叫做「ZNode」\n然而，这些数据不会一成不变，肯定会随着集群的运行，动态发生变化\n那么如何更新这些元数据呢？\n我们当然不希望客户端获取到部分更新的元数据（通常来说这些数据没有任何意义），因此必须满足 原子更新\nZooKeeper 使用就绪文件（Ready File）来标记一个文件（ZNode）是否可用（存在）\n所谓 Ready File，就是以 Ready 为名字的 file。如果 Ready file 存在，那么允许读这个配置。如果 Ready file 不存在，那么说明配置正在更新过程中，我们不应该读取配置。\n如果一个客户端要更新元数据，它的执行流是这样的：\ndelete(\u0026#34;Ready-File-0\u0026#34;) // 删除 Ready File write(f1) // 写元数据 write(f2) // 写元数据 create(\u0026#34;Ready-File-0\u0026#34;) // 创建 Ready File 如果一个客户端要读取元数据，它的执行流是这样的：\nexists(\u0026#34;Ready-File-0\u0026#34;, watch = true) // 检查 Ready File 是否存在 read(f1) // 如果存在，读取元数据 read(f2) 我们考虑下面这种情况：\n容易发现，Client2 此时读取的元数据是「错误」的：旧的 f1 + 新的 f2，没有任何意义\n那么 ZK 是如何解决这种情况，即实现「原子」修改呢？\nwatch 机制\n调用 exists API，不仅仅会判断 Ready File 是否存在，还会注册一个 watch 事件，即监听 Ready File 的状态\n当一个客户端想要修改 Ready File 时，ZK 会 先通知 之前 watch 这个 Ready File 的客户端，告诉它们这个 Ready File 发生了变化，然后再真正执行 Ready File 的修改\n当客户端收到 ZK 的通知（Ready File 发生变化），客户端会立刻终止此次读取行为，然后再重试读：\n","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/zookeeper/basic/note/","tags":["分布式","MIT-6.824","ZooKeeper"],"title":"ZooKeeper: Wait-free coordination for Internet-scale systems"},{"categories":["Distributed-System","Raft"],"content":"Lab4B 整体比较简单，要求实现 KVServer 重启宕机后，仍能 快速恢复 之前的状态，因此，Service 需要创建快照，并发送给 Raft Server\n快照内容 快照应该包含以下内容：\n状态机的全部 K-V 键值对 ClientID 与 Epoch 的映射关系 lastReply 缓存 applyIndex type SnapshotData struct { StateMachine map[string]string ClientEpoch map[int64]int64 LastReply map[int64]int64 } applyIndex 隐式包含在 rf.Snapshot 的参数中\n如何创建快照 初步想法是：\n每个 KVServer 会有一个后台 goroutine：refreshSnapshot refreshSnapshot 会定期检查 Raft 实例的状态 如果 persister.RaftStateSize() 比 maxraftstate 大，那么创建快照 如何应用快照 Raft Server 重启时，会将持久化的 Snapshot 通过 applyCh 发给 KVServer\n因此，需要修改 KVServer 的 applier：\n如果一个 command 的类型时 SnapshotValid，应用快照 应用快照，就是无条件使用快照重置 KVServer，这包括了：状态机、ClientEpoch、LastReply\n正确性验证 4B 部分测试 100 次的结果如下：\n踩的坑 KVServer 应该有辨别 command 乱序的能力 之前一直有一个误区：认为依托于 Raft 的 Service，如果检测到乱序，直接 panic\n事实上这是不正确的，因为 Raft 的 applier 实现，注定了会有乱序 command\n为什么？\n直接看 Raft applier 的代码：\nfor !rf.killed() { time.Sleep(getApplyTimeout()) rf.mu.Lock() CurrentTerm, isLeader := rf.GetState() if isLeader { updateCommitIndex(CurrentTerm) } commitIndex := rf.commitIndex if rf.applyIndex + 1 \u0026gt; commitIndex { // no msgs to apply rf.mu.Unlock() continue } msgs := rf.getLogs(rf.Logs[rf.getIndex(rf.applyIndex + 1):rf.getIndex(commitIndex + 1)]) rf.mu.Unlock() // apply msgs without lock for _, msg := range msgs { applyMsg := ApplyMsg { CommandValid: true, CommandIndex: msg.Index, Command: msg.Command, } rf.applyCh \u0026lt;- applyMsg DPrintf(\u0026#34;{%v}%v: applied command which index is %v(term:%v)\\n\u0026#34;, CurrentTerm, rf.me, msg.Index, msg.Term) } rf.mu.Lock() // Why take the maximum value of rf.applyIndex and commitIndex? // Because we did not lock when applying, // and rf.applyIndex may increase due to InstallSnapshot RPC. // We do not want the updated rf.applyIndex to become smaller, // which may cause duplicate apply rf.applyIndex = max(rf.applyIndex, commitIndex) rf.mu.Unlock() } 原因就出现在：向 applyCh 提交 msg 时，没有持锁\n如果 Leader 发来一个 InstallSnapshot RPC，并且 Snapshot 是描述的「日志前缀」\nKVServer 收到这个快照以后，使用快照重置状态机，更新 applyIndex，注意：更新后的 applyIndex 一定会变小\n在收到快照之前，Raft 的 applier 已经准备批量向 KVServer 提交日志（这部分日志条目的确定，是依靠于之前的 applyIndex 和 commitIndex），此时日志的 Index 一定比更新后的 applyIndex 大\n于是，给上层应用的感觉就是：Raft 提交的 Command 乱序了\n此时 KVServer 正确的做法应该是：拒绝 apply 这条日志，跳过即可\n为什么不 panic？\n因为此时虽然看起来乱序了，但是 Raft 后续肯定会 apply 我们希望的 Command，到那时，KVServer 就可以 apply\n","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/lab4/lab4b/note/","tags":["分布式","MIT-6.824","Raft"],"title":"MIT-6.824: Lab 4B: Key/value service with snapshots"},{"categories":["Distributed-System","Raft"],"content":"Lab4A 核心是要实现「线性化」语义\nClient Client 必须在 RPC 超时，或者 Server 返回自己不是 Leader 的情况下，自动切换到下一个 Server：\n向「Leader」发送 RPC 检查响应结果 如果 Err = ErrWrongLeader，修改 ck.LeaderID = ck.LeaderID + 1，重新从步骤 1 开始 如果 Err = ErrNotFinish，睡眠「最大操作」时间，切换到下一个 Server，重新从步骤 1 开始 如果 Err = OK，更新 Epoch，返回结果 如果 RPC 超时，那么切换到下一个 Server，重新从步骤 1 开始 上面的处理方式比较简单，更「生产可用」的处理方式（例如 Redis、Kafka 的分布式实现）：\n集群中每一个节点都有整个集群的元数据 Client 缓存集群元数据 Client 在请求时，依据缓存的元数据来请求，而不是每次轮询 在本次 Lab 中，「元数据」比较简单，指的是 Leader 的「网络位置」\nServer 为了保证「线性一致性」，Server 需要有辨别某一个请求是否是 重复请求 的能力\n如何辨别？这一点事实上已经在 Lab2 中实现过：即维护 Client 与 CommandEpoch 的映射关系\nClient 每次请求时，需要带上自己的 ID，以及本次请求的 Epoch\nEpoch 是一个单调递增的值，会随着 Client 请求逐渐递增\n当 Client 发现某次请求失败，会使用 相同的 Epoch 重新尝试执行该 Command\nServer 可以利用 Client 与 CommandEpoch 的映射关系 来辨别某个请求是否重复：如果某个请求的 Epoch 与之前缓存的 Epoch 相等，或者更小，说明这个请求是过期的请求\nCommandHandler RPC 要实现线性化语义，单个 Server 必须 线性处理客户端的请求，更进一步的，Server 对 Start 方法的调用是「线性化」的\n具体逻辑如下：\n如果此次请求不是 get 请求，则 校验此次请求是不是一个重复的请求，如果是，直接返回缓存的执行结果 向 Raft 集群发送 Start，如果 Start 返回不是 Leader，直接返回结果给客户端 删除上一次回复的缓存 等待 Raft 集群完成该 Command 的共识 返回结果给客户端 Start 发送的命令定义如下：\ntype Op struct { // Your definitions here. // Field names must start with capital letters, // otherwise RPC will break. Type TypeRequest Key string Value string ClientID int64 ClientEpoch int64 } applier Server 使用一个 后台 goroutie，即 applier，将 Raft 提交的日志条目应用到状态机，以及更新 Client 的 ID 与 Epoch 的映射关系 和 响应结果的缓存\n为了确保任意时刻，同一个网络分区内的 Server，保存的 Client 的 ID 与 Epoch 的映射关系是一致的，Server 间需要同步 Client 的 ID 与 Epoch 的映射关系，以及 响应结果的缓存，这点可以通过 Raft 的同步机制来实现\n具体来说，每一轮循环：\n从 applyCh 获取一条日志 更新 applyIndex 判断这个 Command 是不是一个重复的 Command： 如果是，直接忽略，从缓存的 lastReply 获取本次返回结果 否则： 将日志应用到状态机 以状态机的返回结果作为本次返回结果 缓存此次响应结果的缓存（如果不是 Get 请求） 更新 Client 的 ID 与 Epoch 的映射关系 判断自己是不是 Leader，如果是 Leader，发送 reply 到 channel 正确性验证 由于 Lab4A 完整测试一次需要耗时仅 5min，并且测试会占用比较多的 CPU 资源，这里没有进行大量的 Lab4A 测试\n完整测试 10 次，并且 TestPersistPartitionUnreliableLinearizable4A（Test: unreliable net, restarts, partitions, random keys, many clients (4A) \u0026hellip;）测试 100 次，均通过：\n踩的坑 客户端请求超时处理 在 TestOnePartition4A 测试中，遇到了一个错误：\n2024/05/26 09:21:31 client-1274618360016164535-1: received response from server-1 test_test.go:100: Get(1): expected: 15 received: 16 --- FAIL: TestOnePartition4A (2.66s) FAIL FAIL\t6.5840/kvraft\t4.009s FAIL 经过画图分析，最后 get(1) 的返回值应该为 16，难道是测试用例出问题了吗？\n并不是，如果最后一个 get(1) 的返回值为 15 的话，说明客户端应该具有超时处理的能力，也就是说：如果一个 RPC 时间太长，客户端应该尝试将请求发送到另一个 Server\n一开始我认为 labrpc 的实现已经包含了超时处理，然而并没有\n因此，我们需要手动做 超时控制\nno-op 日志 有一个情况：Leader 刚发完 AE 请求，网络就被分区了，即使 Follower 成功 append 到本地，Leader 也无法更新 commitIndex\n然后，集群选举出新的 Leader，由于 Term 改变，新的 Leader 无法更新 commitIndex，除非客户端发来一个新的请求，Leader 才能间接提交之前的日志\n因此，Leader 刚选举出来，除了发送心跳外，还应该发送一条 no-op 日志，以此间接提交之前的日志\n但是，在添加 no-op 日志后，Lab3B 测试全挂了。。遂放弃\n没有 no-op 日志，可能存在请求无限等待的情况，只有一个新的请求来了，才能打破这个局面\nClient 的 ID 与 Epoch 的映射关系，需要在 applier 更新，而不是在 RPC Handler 更新 这一点主要是在 Follower 这端考虑的\n因为一个客户端的请求，最终只会在 Leader 上执行，如果在 RPC Handler 更新映射关系的话，Follower 就无法同步 Leader 本地的「映射关系」，进而无法识别一个 command 是否为重复的 command，这样存在重复 apply 相同 command 的问题\n因此，「映射关系」应该在 applier 更新，这样 Follower 就可以同步 Leader 内存中存储的「映射关系」\n这里也侧面反映了，applier 在向状态机应用 command 时，也需要检查 command 是否重复（也是在 Follower 这段考虑的）\n注意：「映射关系」应该仅在 applier 更新，否则 Leader 的 applier 可能永远无法应用某个 command\napplier 也需要判断一条 Command 是否重复 这一点也是在 Follower 这端考虑的\n因为一个客户端的请求，最终只会在 Leader 上执行，如果仅仅在 RPC Handler 检查是否重复，Follower 还是会应用重复的日志\nLab3（Raft）本身有 bug Raft 某些难以复现的 bug 可能在 Lab3 的测试用例中展现不出来，但是在 Lab4A 就原形毕露了\n至少我发现了此前实现的 Raft 有以下几个 bug：\nApply（提交）乱序 RequestAppendEntries RPC 乱序，未能正确处理，导致日志被覆盖 为啥在 Lab3 测试不出来这些问题？因为 Lab4 的测试用例考虑到了 网络分区 的情况\n在 网络分区 + 不稳定的网络 这两个条件下，一些隐藏的问题就可能会暴露出来\n","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/lab4/lab4a/note/","tags":["分布式","MIT-6.824","Raft"],"title":"MIT-6.824: Lab 4A: Key/value service without snapshots"},{"categories":["Distributed-System","Raft"],"content":"Server State type Raft struct { snapshot []byte } Snapshot 依赖 Raft 的 Service 层会定期调用 Snapshot 方法，以要求 Raft 实例创建快照：\nfunc (rf *Raft) Snapshot(index int, snapshot []byte) { } 具体实现：\n检查 index 是否合法，index 必须大于等于 rf.logs[0].Index 截断 rf.logs 持久化当前状态和快照 截断 rf.logs 时，截断的是 [0:index-rf.logs[0].Index-1]，也就是说，rf.logs[index] 并没有截断，而是继续保留在内存中，这样做的好处是：\n哨兵日志 相当于持久化了 lastIncludedIndex 和 lastIncludedTerm InstallSnapshot RPC 在 Lab3B 中，提到了 startReplica 函数\nLeader 向 Follower 同步数据时，需要先确定 nextIndex\n如果 Leader 发送 rf.Logs 的所有内容，Follower 还是拒绝了 Leader 的 AE 请求，说明 Follower 同步进度太慢，以至于 Leader 的 rf.Logs 中，没有 Follower 所需的日志（这部分日志由于快照的创建而被截断）\n上面的图片展示了这种情况：\nFollower 的同步速度很慢，即使 Leader 的 nextIndex=7，也无法被 Follower 接受 此时，Leader 需要发送自己的「快照」给 Follower，让 Follower 使用快照来覆盖自己本地的 Logs：\n当然还有一种特殊的情况，即 Follower 收到了描述自己日志前缀的快照：\n这通常是由于重传和错误产生的，对于这种「错误」的情况，Follower 需要：\n截断 被快照覆盖部分的日志 使用该 Snapshot 作为自己的 Snapshot 基本分析完毕，来看看具体实现\n具体实现 参数与返回值如下：\ntype InstallSnapshotArgs struct { Term int // Leader\u0026#39;s term LastIncludedIndex int // The index corresponding to the last log entry contained in the snapshot LastIncludedTerm int // The term corresponding to the last log entry contained in the snapshot Data []byte // Snapshot data } type InstallSnapshotReply struct { Term int // Follower\u0026#39;s term, for leader to update itself } Follower 收到 Leader 发来的 InstallSnapshot RPC，应该：\n如果 Leader 的 Term 比自己小，忽略 如果收到的快照的 LastIncludedIndex 比本地快照的 LastIncludedIndex 小，说明这个快照是一个过期的快照，忽略 保存快照文件（使用 Leader 发来的快照文件覆盖本地快照文件） 讨论两种情况： 如果 rf.logs（内存）中存在一个 Entry 与 LastIncludedIndex、LastIncludedTerm 对应（即此次快照描述的是 rf.Logs 的前缀），那么截断 Entry 前的日志 否则，丢弃整个 rf.Logs 使用快照内容重置状态机（这里指将快照内容 同步 发送到 applyCh） 更新 applyIndex 和 commitIndex 2024.5.29 更新：\n注意：这里一定是同步发送，而不是异步，原因 下文 会讲\n正确性验证 这里提供一个测试代码的脚本文件，可以指定测试的用例、并发量、总测试次数，并将日志输出重定向到相应文件中：\n#!/bin/bash # 默认执行次数 runs=5 # 默认测试用例名称 test_name=\u0026#34;3A\u0026#34; # 默认最大并发数 concurrency=10 # 解析选项 while [[ $# -gt 0 ]]; do key=\u0026#34;$1\u0026#34; case $key in -n) runs=\u0026#34;$2\u0026#34; shift # 过去参数值 shift # 过去参数值 ;; -name=*) test_name=\u0026#34;${key#*=}\u0026#34; shift # 过去参数 ;; -c) concurrency=\u0026#34;$2\u0026#34; shift # 过去参数值 shift # 过去参数值 ;; *) echo \u0026#34;Unknown option: $key\u0026#34; exit 1 ;; esac done # 根据测试用例名称创建Logs和Status目录 log_dir=\u0026#34;logs-$test_name\u0026#34; status_dir=\u0026#34;status-$test_name\u0026#34; rm -rf $log_dir/ $status_dir/ # 创建日志和状态目录 mkdir -p $log_dir mkdir -p $status_dir # 控制并发数 sem() { local max_concurrent=$1 shift local cmd=\u0026#34;$@\u0026#34; ( eval \u0026#34;$cmd\u0026#34; ) \u0026amp; while [[ $(jobs -r -p | wc -l) -ge $max_concurrent ]]; do sleep 0.1 done } # 定义终止处理函数 terminate() { echo \u0026#34;Termination signal received. Stopping all tests...\u0026#34; pkill -P $$ # 终止所有子进程 wait # 等待所有子进程结束 summarize_results exit 1 } # 定义总结测试结果的函数 summarize_results() { success=1 for (( i=1; i\u0026lt;=$runs; i++ )); do if [ -f \u0026#34;./$status_dir/log${i}.status\u0026#34; ]; then status=$(cat \u0026#34;./$status_dir/log${i}.status\u0026#34;) if [ \u0026#34;$status\u0026#34; -ne 0 ]; then echo \u0026#34;Test $i failed. See ./$log_dir/log${i}.log for details.\u0026#34; success=0 fi else success=0 fi done if [ $success -eq 1 ]; then echo \u0026#34;All tests passed successfully.\u0026#34; else echo \u0026#34;Some tests failed.\u0026#34; fi } # 捕获 SIGINT 信号并调用终止处理函数 trap terminate SIGINT # 循环执行命令 for (( i=1; i\u0026lt;=$runs; i++ )); do echo \u0026#34;Running test $i with test case $test_name\u0026#34; sem $concurrency \u0026#34;go test -run \u0026#39;$test_name\u0026#39; -v -count=1 \u0026gt; \u0026#39;./$log_dir/log${i}.log\u0026#39; 2\u0026gt;\u0026amp;1; echo \\$? \u0026gt; \u0026#39;./$status_dir/log${i}.status\u0026#39;\u0026#34; done # 等待所有后台作业完成 wait # 总结测试结果 summarize_results 使用上面的 shell，参数为 -c 25 -n 100 -name=3D，可以通过测试：\n踩的坑 (Lab3C)向 service 提交 command 时，不应该持锁 当 Raft 实例向应用提交 command 时，不应该持锁，否则有可能因为客户端没有及时取走数据，导致 Raft 整体阻塞（因为目前的实现，整体使用一把大锁）\n提交 command，要么不持锁，要么异步提交\n但是异步提交存在问题：如何保证提交的顺序性？\n要想保证提交的顺序性，即使使用多线程，还是要利用同步机制保证 command 的顺序性\n因此，还不如直接用单线程，只不过在 apply 时不要持锁\n异步发送 InstallSnapshotRPC 这个原因不再赘述，与 Lab3C 异步发送 AE 的原因是一样的\n并发问题 引入了 snapshot 后：\ncommitIndex applyIndex Logs 这三个变量，可能会由于 Leader 发来了一个 InstallSnapshotRPC 而改变，这会影响到：\nstartApply startReplica AE RPC Make 这四个函数\n对于 startApply 来说，问题主要出现在循环中：\n// ... applyIndex, commitIndex := rf.applyIndex, rf.commitIndex rf.mu.Unlock() for index := applyIndex + 1; index \u0026lt;= commitIndex; index++ { rf.mu.Lock() log := rf.getEntry(index) // can be a problem? rf.mu.Unlock() applyMsg := ApplyMsg { CommandValid: true, CommandIndex: index, Command: log.Command, } rf.applyCh \u0026lt;- applyMsg } rf.applyIndex = commitIndex // can be a problem? 可能出现问题的代码，已经在注释中给出\n原因是：循环执行过程中，Logs 可能发生改变（InstallSnapshotRPC 截断了 Logs），我们要提交的日志条目可能已经被截断了，要特别注意这种情况\n解决办法很简单：每次尝试 getEntry 前，检查 commitIndex、applyIndex 有没有改变，如果改变，说明 Leader 发来了 InstallSnapshotRPC\n由于我们已经在 InstallSnapshotRPC 中使用快照提交了快照包含的日志，因此，这里不能重复提交（事实上也提交不了。。）\n同样的，更新 applyIndex 前，也要判断 applyIndex 有没有发生改变，如果改变了，就不要更新\nstartReplica、AE RPC 实现时，在获取一个日志条目前，一定要先检查这个日志条目是否还在 Logs 中，避免 panic\n对于 Make 函数来说，由于有了 snapshot，当 Raft 实例宕机重启后，读取的 Logs 可能并不是完整的，有一部分包含在快照中，这个时候，一定 要初始化 applyIndex 和 commitIndex 为哨兵日志的 Index\n2024.5.29 更新：\n重写了 applier 核心逻辑，修复了可能存在的 apply out-of-order 问题，下文有提到\napplyIndex 只能严格递增 在 AE RPC 中，有一种边界情况：更新后的 commitIndex 比 applyIndex 小\n这种情况不被允许，因为对于同一个 server，我们不能重复提交相同 index 的日志（exactly once）\n那为什么会出现 leaderCommitIndex \u0026lt; rf.applyIndex 的情况？\n假设 leader1 成功将 index = 3 以及之前的日志同步到 follower\n于是，leader1 更新自己的 commitIndex 为 3，并 apply，此时 leader1 的 applyIndex 也更新为 3\n然而，在 leader1 给其它 follower 发送心跳前，宕机了，这意味着其它 follower 的 commitIndex 可能小于 3，因为它们并不知道 leader1 的 commitIndex 已经更新\n然后，集群选出第二个 leader2\n然后，leader1 重新上线，收到了 leader2 的心跳，其中，commitIndex = 2\n如果此时更新 commitIndex 为 2，并且如果你的实现：applyIndex 的更新依赖于 commitIndex，那就会出现问题\n解决方案：\n不允许 applyIndex 减小（除了 InstallSnapshot RPC），applyIndex 应该严格递增 极低概率出现 apply out of order 来看一段日志：\n# 省略部分内容 # 2024/05/28 09:20:31 {5}0: Load Raft State Successfully, CurrentTerm: 5 VotedFor: -1 Logs: [{4 119 83 \u0026lt;nil\u0026gt;} {4 120 83 3739460424151144431} {4 121 83 3168617896739618178} {4 122 83 9162378228834488708} {4 123 83 6831704875973104458} {5 124 124 549014041314024410} {5 125 125 2306247720335571870} {5 126 126 3489300829279971277}] # 省略部分内容 # 2024/05/28 09:20:31 {5}0: Received snapshot from leader, lastIncludedIndex: 139, lastIncludedTerm: 5 2024/05/28 09:20:31 {5}0: Received AE from 2 2024/05/28 09:20:31 {5}0: Appended Entries to local log(current len:142) 2024/05/28 09:20:31 {5}0: Received AE from 2 2024/05/28 09:20:31 {5}0: Received unexpected AE from 2: args.PrevLogIndex not int local log 2024/05/28 09:20:31 {5}0: applied command which index is 140(term:5) 2024/05/28 09:20:31 apply error: server 0 apply out of order, expected index 120, got 140 exit status 1 FAIL\t6.5840/raft\t121.630s 日志第一行表明 server-0 宕机重启完毕了，并且有快照，快照的 lastIncludedIndex 为 119\n然后，server-0 收到了来自 leader 的 InstallSnapshotRPC，要求应用一个新的快照，新的快照的 lastIncludedIndex 为 139\nserver-0 会做以下几件事：\n使用新的快照替换旧的快照 将 applyIndex 由 119 修改为 139 将新的快照发给上层 Service 然后，后台的 applier 开始 apply command，第一条 command 的 index 肯定是 140（因为 applyIndex 已经修改为 139）\n一切看起来如此合理，为啥测试程序期望的 command index 为 120，而不是 140 呢？\n只有一个原因：applier 在 apply command 时，「将新的快照发给上层 Service」这个操作 还没有开始执行\n为什么会出现这个情况？因为 InstallSnapshot RPC 中，将新的快照发给上层 Service 这个操作是 异步的\n正确的实现应该是：更新 applyIndex 和 将新的快照发给上层 Service 这两个操作应该是一个 原子操作\n怎么实现原子操作？最简单的方式就是 同步 发送新的快照到上层 Service：\n// RequestInstallSnapshot RPC handler. func (rf *Raft) RequestInstallSnapshot(args *RequestInstallSnapshotArgs, reply *RequestInstallSnapshotReply) { rf.mu.Lock() defer rf.mu.Unlock() // ... // apply snapshot synchronously rf.applyCh \u0026lt;- applyMsg rf.persist() } 当然这可能导致客户端响应的延迟，我们也可以这样写：\n// RequestInstallSnapshot RPC handler. func (rf *Raft) RequestInstallSnapshot(args *RequestInstallSnapshotArgs, reply *RequestInstallSnapshotReply) { rf.mu.Lock() // defer rf.mu.Unlock() 不要释放锁 // ... go func() { rf.applyCh \u0026lt;- applyMsg rf.mu.Unlock() // 在这里释放 }() rf.persist() } 同样的，Make 函数也有问题，修改如下：\nfunc Make(peers []*labrpc.ClientEnd, me int, persister *Persister, applyCh chan ApplyMsg) *Raft { // ... // Reset state machine using snapshot if len(rf.SnapshotData) \u0026gt; 0 { applyMsg := ApplyMsg { SnapshotValid: true, Snapshot: rf.SnapshotData, SnapshotIndex: rf.Logs[0].Index, SnapshotTerm: rf.Logs[0].Term, } rf.mu.Lock() go func() { rf.applyCh \u0026lt;- applyMsg // To avoid possible apply out-of-order problems, // lock until the snapshot is successfully applied to applych rf.mu.Unlock() }() } // ... return rf } 写在最后 Lab3D 是整个 Lab3 的最后一个小节了，通过实现这四部分 Lab，顺利掌握了 Raft 共识算法的实现细节，也学到了 debug 的技巧，提升了解决问题的能力\n说实话，Lab3 的实现还是有一定难度，即使是完全按照论文描述实现，还是会遇到一些 corner case\n只有自己 独立完成 Lab3，才能真正搞清楚 Raft 的细节\n遇到困难，不要害怕，相信办法永远比困难多\n这句话虽然简单，但是正是这句话激励着我不断思考，调试，最终实现了整个 Lab3\n","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/lab3/lab3d/note/","tags":["分布式","MIT-6.824","Raft"],"title":"MIT-6.824: Lab 3D: log compaction"},{"categories":["Distributed-System","Raft"],"content":"角色 Raft 协议包含三种角色：\nLeader Follower Candidate 选举计时器超时（Leader 超时没有发送心跳） 选举成功（获得半数以上投票） 选举失败（投票数不够，或者有了新的 Leader） 有了比自己 term 更大的 Leader（通常发生在旧 Leader 网络分区问题恢复时） Leader 选举 什么时候开始选举？\n每个 Follower 会维护一个 选举计时器\n如果选举计时器超时，那么该 Follower 认为 Leader 挂了，准备开始选举\nFollower 此时会变成 Candidate：\n将自己的 Term 加一 并给集群剩余的 Followers 发送 VoteRPC，要求 Followers 给自己投票 投票采取 FIFO 机制：每个 Follower 只有一票，收到一个投票请求，如果：\n自己有票 Candidate 的 Term 大于等于自己的 Term Candidate 的 Log 比自己新或者一致 那么 Follower 会给该 Candidate 投票\n如果某一个 Candidate 拥有超过半数（整个集群中）的票，那么该 Candidate 就升级为 Leader\n某一轮选举可能失败（每个 candidate 都没有拿到半数以上的选票），怎么解决的？\n如果选举的 candidate 同时 开启选票，可能出现选票被「瓜分」的情况\n每个 Candidate 会在选举开始时启动一个 选举超时计时器\n如果计时器超时，那么当前 candidate 会 重新开启 一轮新的投票\n选举超时计时器的超时时间怎么确定的？\n超时时间实际上是一个 随机值，范围在 [min,max] 之间，这样可以避免多次选举都失败的极限情况\n范围怎么确定？\n广播时间（broadcastTime） \u0026laquo; 选举超时时间（electionTimeout） \u0026laquo; 平均故障间隔时间（MTBF）\n论文的解释是：\n广播时间必须比选举超时时间小一个量级，这样 Leader 才能够有效发送心跳信息来组织 Follower 进入选举状态。再加上随机化选举超时时间的方法，这个不等式也使得无果选票（split vote）变得几乎不可能。而选举超时时间需要比平均故障间隔时间小上几个数量级，这样整个系统才可以稳定地运行。\n翻译自 知乎 日志复制 为了保证集群的可用性，Leader 需要将客户端发来的 Command 同步给 Follower\n在 Raft 协议中，将 Command 以及其它元数据封装成一个「日志」\n一个日志包含以下内容：\nCommand Term：收到该 Command 时，Leader 的 CurTerm Term 的作用主要是用于检测 Follower 与 Leader 的 Log 是否一致，即 一致性检查 日志复制的过程 这里以「一个客户端请求执行的过程」来展示日志是如何在节点间复制的：\n客户端向 Leader 发送一条 Command Leader 收到该 Command，将其封装成一个 Log Leader 向所有 Follower 发送 AE 请求，要求 Follower 将该 Log Append 到本地 Follower 收到 AE 请求，执行 一致性检查 ，并返回结果给 Leader 当 Leader 收到半数以上的 Follower 的确认以后，可以认为这个 Command 在整个集群中被「Commit」，推进 CommitIdx，然后： Leader 本地执行 Command，推进 ApplyIdx，告诉客户端执行成功 Follower 看起来仅仅只是将 Log 存储在本地，还没有执行 Command？\n下一次 Leader 向 Follower 发起 AE 请求时，会顺带上 CommitIdx\nFollower 可以根据 CommitIdx，来判断哪些日志已经被提交，并执行这些已经提交的日志\n这个特性，决定了 Raft 协议中，Follower 应用的数据 不总是最新的，因此，客户端实际的 读写操作应该都在 Leader 进行\nFail Raft 通过在 AE 执行时的 一致性检查 来确保即使出现错误，也能保证数据的一致性\n一致性检查，是在 Follower 执行的\n具体来说，Leader 在发送 AE 请求时，会带上 前一个日志条目的索引位置和任期号\nFollower 会检查自己的日志是否包含 Leader 发来的 前一个日志条目的索引位置和任期号，如果不包含，那么一致性检查失败\n正常情况下，一致性检查不会失败，但是如果 Leader 挂了，就有可能出现检查失败的情况\n当一个 Leader 成功当选时（最上面那条日志），Follower 可能是以下 (a-f)几种情况。每一个盒子表示一条日志条目，盒子里面的数字表示任期号。一个 Follower 有可能会缺失一些日志条目（a-b），一个 Follower 也有可能会有一些未被提交的日志条目（c-d），或者两种情况都有 (e-f)。例如，场景 f 有可能是这样发生的：f 对应的服务器在任期 2 的时候是 Leader，它追加了一些日志条目到自己的日志中，一条日志还没提交就宕机了，但是它很快就恢复重启了，然后再在任期 3 重新被选举为 Leader，又追加了一些日志条目到自己的日志中，在这些任期 2 和任期了的日志还没有被提交之前，该服务器又宕机了，并且在接下来的几个任期里一直处于宕机状态。\n翻译自 知乎 在一致性检查失败时，Leader 会 强制 Follower 复制 Leader 的日志来解决一致性问题\n具体来说，Follower 在一致性检查失败后，会给 Leader 一个 reply，告诉 Leader 一致性检查失败了\n为了使 Follower 的日志跟自己（Leader）一致，Leader 必须找到两者达成一致的最大的日志条目索引，删除 Follower 日志中从那个索引之后的所有日志条目，并且将自己那个索引之后的所有日志条目发送给 Follower。\n例如，对于上图中的 e，Leader 会告诉 Follower，删除 idx = 6 及后面所有的 Log，并发送自己 idx = 6 及后面所有的 Log 给 Follower 应用\n如果一个 Leader 在发送 AE 给 Follower 前挂了，那么 Follower 怎么知道哪些 Log 已经被 commit？\n对于这种情况，Follower 无需 知道哪些 Log 被 commit\n怎么理解？\n如果一个 Log 没有被 commit，并且 Leader 挂了，客户端后面会知道这个 command 执行失败了，那么这个 Log 丢了也无所谓\n主要看 Log 被 commit 的情况\n如果一个 Log（假设为 A）被 commit，那么意味着有 半数以上 的节点本地的 Log 列表中，都有 A\n假设 Leader 挂了，自然需要选举一个新的 Leader 出来\n根据前文提到的 Leader 选举过程中，Follower 投票规则：\n自己有票 Candidate 的 Term 大于等于自己的 Term Candidate 的 Log 比自己新或者一致 前面提到了：半数以上 的节点（假设为集合 S）本地的 Log 列表中，都有 A\n那么，根据投票规则，只有集合 S 中的节点，才有机会当选 Leader（不处于 S 中的节点，由于日志比较旧，不会得到大多数投票）\n当选出一个新的 Leader 时，新的 Leader 可以 安全地 认为自己的 Log 都是已经 commit，立即发起一轮 heartbeat（AE）请求\n这样，其它的 Follower 就知道哪些日志已经被 commit 了\n是否存在数据丢失问题？\n不会存在\n原因和上面的问题类似：截断的 Log 是没有被旧 Leader 提交的 Log，这些 Log 即使丢了也无所谓，客户端是有感知的，后面重试即可\n日志在 Raft 中的作用 参考 MIT-6.824，Lecture 6 中，Robert 教授的 讲解 ：\nLog 是 Leader 用来对操作排序的一种手段 对于这些复制状态机来说，所有副本不仅要执行相同的操作，还需要 用相同的顺序 执行这些操作。而 Log 与其他很多事物，共同构成了 Leader 对接收到的客户端操作分配顺序的机制。\n临时存放 Command 这个可以从 Leader 和 Follower 两个方面来理解：\n对于 Leader 来说，如果同步 Command 给 Follower 时，网络出现问题，需要重传，那么缓存的 Log 就可以派上用场\n并且，即使对于 Commit 后的 Command，还是有可能有一些 Follower 没有拿到这个日志，Leader 本地缓存的 Log 还可以用于 Follower 后续的数据恢复\n对于 Follower 来说，即使同步到了 Command，也不知道这些 Command 有没有 Commit，只有 Leader 发来下一次 AE 请求，才知道哪些 Command 已经被提交\n帮助重启的服务器恢复状态 所有节点都需要保存 Log 还有一个原因，就是它可以帮助重启的服务器恢复状态。对于一个重启的服务器来说，会使用存储在磁盘中的 Log。每个 Raft 节点都需要将 Log 写入到它的磁盘中，这样它故障重启之后，Log 还能保留。而这个 Log 会被 Raft 节点用来从头执行其中的操作进而重建故障前的状态，并继续以这个状态运行。所以，Log 也会被用来持久化存储操作，服务器可以依赖这些操作来恢复状态。\n这样，服务器重启，就不需要在 Leader 拉取大量 Log，进一步减少网络压力\n持久化 Logs CurrentTerm VoteFor 为啥要持久化这三个数据？\n首先是 Logs，持久化 Logs，主要是帮助服务器重启时快速恢复状态，而不是让 Leader 给自己发送 AE RPC 全量同步，减少网络压力\n其次是 CurrentTerm，如果重启的 Raft Server 不知道自己之前的 Term，可能会让 Raft 共识产生混乱\n最后是 VoteFor，重启的 Raft Server 除了要知道自己的 Term 以外，还要知道自己在这个 Term 是否给其它 Server 投过票，避免重复投票，造成选举错误\n为啥不持久化其它数据：commitIndex、lastApplied、nextIndex、matchIndex？\n这几个 Index 都可以随着 Raft 共识协议的进行而自更新，无需持久化\n谁来持久化？持久化的过程？怎么写磁盘更高效？\n无论是 Leader 还是 Follower，都要持久化自己的数据到本地磁盘\n如果 Leader 收到了一个客户端请求，在发送 AppendEntries RPC 给 Followers 之前，必须要先持久化存储在本地。因为 Leader 必须要 commit 那个请求，并且不能忘记这个请求。实际上，在回复 AppendEntries 消息之前，Followers 也需要持久化存储这些 Log 条目到本地，因为它们最终也要 commit 这个请求，它们不能因为重启而忘记这个请求。\n一个简单的实现方式是：只要内存中，Logs、CurrentTerm、VoteFor 三者任何一个发生改变，就持久化，Lab3C 也是这样实现的\n这种持久化方式合适吗？\n很容易发现前面提到的持久化方式是不合理的，在实际生产环境显然不可用\n因为无论哪个数据变化，都要做一次 整个数据 的持久化\n数据持久化，是对 磁盘 进行 IO 操作的过程，而磁盘的速率很低，采取这种持久化方式，会产生性能问题\n那应该怎么持久化？\n我们可以 参考 Redis 的持久化方式：\n将持久化分为两个文件：\nmetadata logs.aof 其中，metadata 用于持久化 CurrentTerm 和 VoteFor，因为这两个数据很小，每次更改持久化，也没什么问题\n而 logs.aof 用于持久化 Logs，每次客户端追加 Logs 时，我们只需要向 Logs 追加一条数据即可，这个是顺序 IO，性能较高\n并且，可以引入 批量写入 的机制，具体来说，每次追加 Logs 时，不需要立即写磁盘，而是写到内存(rf.Logs)，然后后台启动一个 goroutine，做定时持久化操作，持久化的频率根据数据的可靠性来确定：\n如果要求比较高的可靠性，那频率应该更高，甚至，放弃批量写入机制，而是直接写入磁盘（write+fsync） 此外，写入磁盘，通常使用的是 write 系统调用，不会立即将文件写入磁盘，而是在 OS 层面做了一个 buffer，还是有数据丢失的风险\n我们还可以再启动一个 goroutine，定期调用 fsync，控制文件真正写入磁盘的频率\n日志压缩 为什么要创建快照？\n随着上层服务不断的请求，Raft Server 存储的 Log 肯定会越来越多，此时，整个 Raft 的性能会受到影响\n因此，需要在合适的时机对 Raft 做一次日志压缩\n实现日志压缩最简单的方式之一就是创建「快照」，将当前 Raft 实例的状态完整记录下来：\n可以发现，前五条日志条目经过「压缩」后，仅剩下 2 条，这个有点 类似 Redis 的 AOF 重写\n快照包含了哪些内容？\n从上图可以发现，快照包含了：\n压缩后的 Logs 列表 LastIncludedIndex LastIncludedTerm 包含 LastIncludedIndex、LastIncludedTerm 的原因是为了 AE RPC 中做的 一致性检查 怎么创建快照？是否依赖客户端？\n通常情况下，节点独立生成快照，并且快照的生成依赖于客户端（上层服务），因为 Raft 并不知道自己存储的 Logs 的定义是什么，只有客户端才知道如何做压缩\n但是 Leader 不可避免偶尔需要发送快照给一些落后的 Follower。这通常发生在 Leader 已经丢弃了需要发给 Follower 的下一条日志条目的时候。幸运的是，这种情况在正常操作中是不会出现的：一个与 Leader 保持同步的 Follower 通常都会拥有该日志条目。\n不过如果一个 Followe 运行比较缓慢，或者是它刚加入集群，那么它就可能会没有该日志条目。这个时候 Leader 会通过网络将该快照发送给该 Follower，以使得该 Follower 可以更新到最新的状态。\nleader 使用 InstallSnapshotRPC 来发送快照给那些太落后的 follower\n如果 Leader 发送 rf.Logs 的所有内容，Follower 还是拒绝了 Leader 的 AE 请求，说明 Follower 同步进度太慢，以至于 Leader 的 rf.Logs 中，没有 Follower 所需的日志（这部分日志由于快照的创建而被截断）\n上面的图片展示了这种情况：\nFollower 的同步速度很慢，即使 Leader 的 nextIndex=7，也无法被 Follower 接受 此时，Leader 需要发送自己的「快照」给 Follower，让 Follower 使用快照来覆盖自己本地的 Logs：\n当然还有一种特殊的情况，即 Follower 收到了描述自己日志前缀的快照：\n这通常是由于重传和错误产生的，对于这种「错误」的情况，Follower 需要：\n截断 被快照覆盖部分的日志 使用该 Snapshot 作为自己的 Snapshot 集群成员变更 在某些时候，我们希望对 Raft 集群做 扩缩容，即向集群中添加或者删除若干个节点，此时应该怎么做呢？\n直接更新 Config 有什么问题 你可能会想：扩缩容，直接修改每个节点的 config 不就好了吗？\n然而，直接修改 config 存在一个问题：某一个 Term 内，集群存在两个 Leader，即脑裂\n因为我们不可能同时修改每个节点的 config，这就意味着在某一时刻，一些节点使用的是旧的 config，一些节点使用的是新的 config\n例如上图中，存在一个时间段，Server-1 与 Server-2 持有旧 config，Server-3、Server-4、Server-5 持有新的 config\n如果在这个时间段内发生了 Leader 选举，那么：\nServer-1 与 Server-2 之间会选出一个 Leader（其中一个获得两票，满足旧 config 的 majority） Server-3、Server-4、Server-5 之间也会选出一个 Leader（其中一个获得三票，满足新 config 的 majority） 整个集群就出现了两个 Term 相同的 Leader，这违背了 Raft 协议\n如何处理集群成员变更 停机，手动修改每个节点的 config 这种方式非常简单，但是存在一个问题：config 修改期间，整个集群是无法对外提供服务的\n单节点变更 这种方式要求：一次性只能添加或删除 一个 节点，只有当新配置被 apply 以后，才能添加或删除下一个节点\n为什么单节点变更，可以保证任意时刻，不会出现脑裂现象呢？\n原有集群奇偶数节点情况下，分别添加和删除一个节点。在上图中可以看出，如果每次只增加和删除一个节点，那么 Cold 的 Majority 和 Cnew 的 Majority 之间一定 存在交集，也就说是在同一个 Term 中，Cold 和 Cnew 中交集的那一个节点只会进行一次投票，要么投票给 Cold，要么投票给 Cnew，这样就避免了同一 Term 下出现两个 Leader。\n但是这种方式变更效率不高，一次只能变更一个节点，如果我们希望一次变更多个节点怎么办呢？\n多节点变更：联合共识（joint consensus） Raft 引入了 联合共识（joint consensus） 的概念来处理集群节点变更的情况\n简单来说，除了旧配置 Cold 与新配置 Cnew 以外，还额外引入了一个 联合共识配置：C(old,new)\nC(old,new) 是新旧配置的并集，例如：\nCold 中有 [A, B, C, D] 四个节点 Cnew 中有 [B, C, D, E] 四个节点 那么 C(old,new) 中有 [A, B, C, D, E] 五个节点\n采用联合共识：\n日志条目被复制给集群中处于新、老配置的所有节点 新、旧配置的节点都可能成为 leader 达成一致（针对选举和提交）需要 分别得到在两种配置上过半的支持 当 Leader 收到客户端的集群变更请求后\n根据新旧配置生成一个联合共识配置 采用日志复制的方式同步联合共识配置 任何节点收到一个描述配置的日志条目时，无需等待提交，立即应用新配置 当联合共识配置被 Commit 并 Apply 后，Leader 可以安全的生成新配置 Cnew 采用日志复制的方式同步 Cnew 当 Cnew 被 Commit 并 Apply 后，Cold 就没有作用了，此时集群完全采用新的配置 上图描述了使用联合共识更新配置文件的全过程，共分为三个阶段：\n阶段 1: C(old,new) 创建后，commit 前：此时只有处于 Cold 配置的节点可以当选 Leader（因为 Cold 是 majority） 阶段 2: C(old,new) commit 后，Cnew commit 前：此时只有处于 C(old, new) 配置的阶段可以当选 Leader（因为 C(old, new) 是 majority） 阶段 3: Cnew commit 后：此时只有处于 Cnew 配置的节点可以当选 Leader（因为 Cnew 是 majority） 可以发现：使用联合共识，可以保证任意阶段，最多只有一个 Leader，确保了配置更新的安全性\n但是使用联合共识会引入额外的复杂度，带来一些问题：\n新的节点需要追赶日志条目\n引入新身份：Learner\n新的节点可能在一开始并没有存储任何的日志条目。当这些节点以这种状态加入到集群中的时候，它们需要一段时间来更新自己的日志，以便赶上其他节点，在这个时间段里面它们是不可能提交一个新的日志条目的。\n为了避免因此造成的系统短时间的不可用，Raft 在配置变更前引入了一个额外的阶段。在该阶段中，新的节点以 没有投票权身份（Leaner） 加入到集群中来（leader 会把日志复制给它们，但是考虑过半的时候不需要考虑它们）。\n一旦新节点的日志已经赶上了集群中的其他节点，那么配置变更就可以按照之前描述的方式进行了。\nLeader 有可能不是新配置中的一员\n为了解决这个问题，Leader 一旦 提交 了 Cnew 日志条目，它就会退位为 Follower\n移除的节点扰乱集群\n新配置中，可能移除了部分节点，新的 Leader 不会给这些节点发送心跳\n在这部分节点下线前，由于收不到 Leader 的心跳，会开始 Leader 选举过程\n它们会发送带有新任期号的 RequestVote RPC，这样会导致当前的 Leader 回到 Follower 状态，然后选出一个新的 Leader。但是这些被移除的节点还是会收不到心跳，然后再次超时，再次循环这个过程，导致系统的可用性很差。\n为了解决这个问题，一个节点在处理另一个节点的投票请求时，如果它认为已经有一个 Leader 存在，那么它会 忽略 投票请求\n具体来说，如果一个节点在最小选举超时时间内收到一个 RequestVote RPC，它会忽略这个请求\n这种方式有效避免了 Term 爆炸增长的问题\nRaft 怎么处理脑裂的？=\u0026gt; 分布式系统应该如何应对脑裂？ 脑裂主要是 网络分区问题 产生的\n当旧 Leader 所在网络分区出现问题，无法与部分 Follower 建立心跳，那么这一部分 Follower 会开始选举\n讨论两种情况：\nLeader 所在网络分区具有节点数大于 n / 2 Leader 所在网络分区具有节点数小于 n / 2 对于第一种情况，Follower 的选举过程会失败，因为没有足够的票数，不存在脑裂\n对于第二种情况，Follower 的选举过程也许会成功，假设成功，会出现脑裂吗？\n如果选举成功，即旧 Leader 所在网络分区具有节点数小于 n / 2，那么客户端在 旧 Leader 的写入操作全部都会失败，只有在新的 Leader 上执行写操作，才有可能成功\n当旧 Leader 所在的网络分区问题修复后，旧 Leader 以及 Follower 会收到新 Leader 的 AE 请求\n由于新 Leader 的 Term 更大，因此，旧 Leader 会降级为 Follower，并从新的 Leader 同步丢失的 Log\nRaft 就是通过 过半票决 + Term 这一规则来规避「脑裂」这一问题的\n分布式系统应该如何应对脑裂 在以前，建立分布式系统时，应对脑裂主要有两种方式：\n建立 绝对可靠 网络：当网络不出现故障时，那就意味着，如果客户端不能与一个服务器交互，那么这个服务器肯定是关机了。可以安全选举出一个 Leader 人工解决问题，不要引入任何自动化 类似 Raft 的 Term 机制，可以为集群的 Leader 引入 Epoch 的概念\n集群中的每个角色都要维护自己 Leader 的 Epoch\n一旦收到了比自己维护的 Epoch 大的，就更新；遇到比自己维护 Epoch 小的，就忽略\n这样，基本可以规避「脑裂」带来的影响\n例如，Redis Cluster 模式下，每个节点会维护一个 currentEpoch，选举 Leader 时，也需要得到绝大多数选票才可以晋升为 Leader\n与 Raft 不同的是，向 Leader 写入数据，不需要同步给 n/2+1 个 Follower 以后，才执行写入，因此，如果客户端向旧的 Leader 写入数据，有可能出现数据丢失的问题\n当旧的 Leader 网络分区恢复以后，会收到新的 Leader 的心跳，发现自己的 currentEpoch 还小，于是降级为 Follower，并主动从新的 Leader 拉取同步数据\nKafka 应对脑裂的措施：\nZookeeper quorum 需要多数派，所以如果 ZK 集群发生分区，最多只有一边会有多数派。\nController 脑裂\n要成为 Controller 需要与 ZK 保持一个活跃的会话（临时 znode 注册）。如果当前 Controller 被分隔到 ZK quorum 之外，它应该 自愿停止认为自己是 Controller。这应该最多需要 zookeeper.session.timeout.ms = 6000。仍然连接到 ZK quorum 的 Nodes 应该在他们之间选举一个新的 Controller。（基于这个：https://stackoverflow.com/a/52426734）\nPartition Leader 脑裂\nPartition 的 Leader 也需要与 ZK 保持一个活跃的会话。失去与 ZK quorum 连接的 Leader 应该 自愿放弃 Leader 身份。被选举的 Controller 将检测到旧 Leader 断开，然后由 Controller 执行新的 Partition Leader 的选举过程\n那么，在 ZK 超时窗口期间，被分割的前 Leader 收到的 Producer 请求会发生什么？有几种可能性。\n如果 Producer 的 acks = all 并且 Topic 的 min.insync.replicas = replication.factor，那么所有 ISR 应该 有完全相同的数据。前 Leader 最终会拒绝正在进行的写入，Producer 会重新尝试它们。新选出的 Leader 将不会丢失任何数据。另一方面，在网络分区修复之前它将无法处理任何写请求。是否拒绝客户请求或在后台尝试一段时间，将取决于 Producer。\n否则，新的 Leader 很可能会缺失 zookeeper.session.timeout.ms + replica.lag.time.max.ms = 16000 毫秒的记录，这些记录在网络分区修复后将从前 Leader 那里被截断。\nFollower 和 candidate 崩溃（Follower and candidate crashes） Follower 和 candidate 崩溃，无需特别处理\n当 Follower 或者 candidate 崩溃，发送给它们的 RPC 请求会失败，Raft 采取尝试重新发送来解决这个问题，因为 AE RPC 和 RV RPC 是 幂等 的\n多次发送相同的 AE 是无害的，Follower 收到重复的 AE 会直接忽略 多次发送相同的 RV 是无害的，Follower 会记录当前 Term 投票的对象，不会存在对于相同 Term，某一个 Candidate 的投票，第一次成功，第二次失败的问题 总结 Raft 是一种 强一致性 的分布式共识协议，它以牺牲可用性和部分性能为代价，保证了数据的可靠性 Raft 的强一致性体现在 Leader 写入日志时，需要过半节点都写入以后，才写入 Raft 使用 过半票决 + Term 处理脑裂问题，过半票决思想在 Leader 选举、Append Entries 都有体现 为了 Raft 的正确进行，以及宕机重启快速恢复状态，Raft 需要持久化一些元数据到磁盘 随着日志条目的增加，日志维护成为额外的性能开销，Raft 通过日志压缩来解决这个问题 参考资料 Lecture 06 - Raft1 Lecture 07 - Raft2 6.5840 Lab 3: Raft In Search of an Understandable Consensus Algorithm(Extended Version) 【译文】Raft 协议：In Search of an Understandable Consensus Algorithm (Extended Version) 大名鼎鼎的分布式共识算法 Redis cluster specification How does kafka handle network partitions? ","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/raft/note/","tags":["分布式","MIT-6.824","Raft"],"title":"Raft --- The famous distributed consensus protocol"},{"categories":["Distributed-System","Raft"],"content":"Lab3C 整体实现非常简单，实现 Raft 的持久化功能，帮助宕机重启的 Raft 实例快速恢复\n持久化哪些内容 Logs CurrentTerm VoteFor 怎么持久化 Raft 的测试程序无法真正让一个 Raft 实例宕机，而是一种模拟的方式\n实现持久化时，只要 Logs、CurrentTerm、VoteFor 任意一个发生了变化，就做一次持久化\n实现时需要注意持久化操作在 rf.mu 解锁之前进行，保证原子性\n这种持久化方式合适吗？ 很容易发现前面提到的持久化方式是不合理的，在实际生产环境显然不可用\n因为无论哪个数据变化，都要做一次 整个数据 的持久化\n为什么能通过测试？是因为 Lab 的持久化操作是在 内存 进行的，并不是磁盘\n内存的 IO 很高，即使采用这种方式也不会产生太大的瓶颈\n但实际应用，应该是将数据持久化到 磁盘 才对，而磁盘的速率很低，采取这种持久化方式，就会产生性能问题\n那应该怎么持久化？\n我们可以 参考 Redis 的持久化方式：\n将持久化分为两个文件：\nmetadata logs.aof 其中，metadata 用于持久化 CurrentTerm 和 VoteFor，因为这两个数据很小，每次更改持久化，也没什么问题\n而 logs.aof 用于持久化 Logs，每次客户端追加 Logs 时，我们只需要向 Logs 追加一条数据即可，这个是顺序 IO，性能较高\n并且，可以引入 批量写入 的机制，具体来说，每次追加 Logs 时，不需要立即写磁盘，而是写到内存(rf.Logs)，然后后台启动一个 goroutine，做定时持久化操作，持久化的频率根据数据的可靠性来确定：\n如果要求比较高的可靠性，那频率应该更高，甚至，放弃批量写入机制，而是直接写入磁盘（write+fsync） 此外，写入磁盘，通常使用的是 write 系统调用，不会立即将文件写入磁盘，而是在 OS 层面做了一个 buffer，还是有数据丢失的风险\n我们还可以再启动一个 goroutine，定期调用 fsync，控制文件真正写入磁盘的频率\n截断 上面提到的都是追加日志的情况，但是 Leader 可能会要求 Follower 截断本地日志\n如果全局使用一个 aof 文件，可能不太好实现，我们可以拆分成多个 aof 文件，当一个 aof 文件大小达到设置的上限时，新建一个新的 aof 文件\n如果要截断本地日志，除了截断内存的 Log 以外，只需要删除部分 aof 文件即可\n快照 此外，Redis 有一个 AOF 重写机制\n实际上 Lab3D 部分要求实现的快照机制，就和 AOF 重写非常类似，这里不再过多描述\n总结 Lab 3C 实现的持久化非常简单，生产上不可用，如果要实现一个生产可用的 Raft，可以参考 Redis AOF 的实现\n正确性验证 使用如下脚本进行测试：\n#!/bin/bash # 定义执行次数 runs=100 # 循环执行命令 for (( i=1; i\u0026lt;=$runs; i++ )); do echo \u0026#34;Running test $i\u0026#34; go test -run 3C -v -count=1 \u0026gt; log.log # 将日志输出重定向到 log.log if [ $? -ne 0 ]; then echo \u0026#34;Error: Test failed. Exiting...\u0026#34; exit 1 fi done echo \u0026#34;All tests passed successfully.\u0026#34; 结果如下：\n踩的坑 Lab3C 无法通过测试，基本上都是 3A、3B 的代码有 bug 导致的，而不是持久化本身\n在真正编写 Lab3C 的代码之后，我发现自己之前的代码有很多小 bug，导致 3C 的测试一直过不去，可以看看在我提交 3C 部分代码前，修改了多少 bug。。。\nSky_Lee@SkyLeeMacBook-Pro 6.5840 % git log --oneline 27a3090 fix: Fixed a bug that caused nextIndex and matchIndex to be updated incorrectly due to out-of-order RPC responses. 8dae858 fix: Fixed an issue where network instability caused a single AE request to take too long 5543b60 fix: Fixed an election bug: candidate\u0026#39;s LastLogTerm parameter was wrong ba8ad08 fix: leader can only submit logs for its own term(Figure8) 1a1b879 fix: rf.matchIndex should start from -1, not 0 73d7edc fix: any goroutine with a long-running loop should call killed() to check whether it should stop. 97aa9fb fix: Fixed an election bug: candidate\u0026#39;s LastLogIndex parameter was wrong c9b2239 refactor: change struct field to be exported 问题 1: 忘记调用 killed 测试文件无法「真正」杀掉一个 Raft 实例，而是通过调用 Kill 方法\n在实现时，对于任何一个无限循环的 goroutine，都应该在循环入口检查当前实例是否被杀掉，如果被杀掉，退出循环\n这个问题在 Lab3C 的 Figure8 测试中会被暴露出来：Figure8 会通过杀掉一个 Raft 实例模拟宕机的情况\n发现这个问题，主要是在看 Log 时，发现了一些「不正常」的日志，遂想起自己是不是忘记调用 Killed 来检查当前 Raft 的状态\n问题 2: rf.matchIndex 应该从 -1 开始，而不是 0 因为我在实现 Raft 时，Index 是从 0 开始的，因此，如果 rf.matchIndex 从 0 开始，意味着 Index=0 这个日志已经被提交，这是错误的\n当然，也可以在初始化 Logs 切片时，Append 一个 「哨兵」 日志，这样实际的第一条 Log 的 Index 就是 1 开始，与 Raft 论文表述一致，方便测试，也不用编写 if 语句判断 Logs 切片为空的情况，代码更加简洁\n问题 3: Leader 只能提交自己 Term 内的日志 在 Lab3B 中，提到了数据不一致的问题\n当时的解决方案是通过限制 Follower 更新 CommitIndex 的条件\n但是在 Figure8 的测试中，仅仅限制「更新 CommitIndex 的条件」已经无法满足了，而是需要限制 Leader 更新 CommitIndex 的条件：\nLeader 不能提交之前任期的日志，只能通过提交自己任期的日志，从而间接提交之前任期的日志。\n具体来说，Leader 在更新 CommitIndex 时，目标 CommitIndex 对应的 Log.Term 必须 与当期 Term 一致，如果不一致，那么就不能使用这个 CommitIndex；如果一致，那么更新 CommitIndex，相当于「间接」提交了之前的所有 Log\n为什么要这样限制？主要还是为了避免新的 Leader 提交的 Log(index=i) 与旧 Leader 提交的 Log(index=j) 的 Command 不一致的问题\n来看看 Figure8 描述的问题：\na：初始状态 b：S1 在将 Term=2 的日志复制到大半节点前，宕机了，S5 选举成为新的 Leader，并接受了客户端的请求 c：S5 宕机，S1 重新选举为新的 Leader，并成功将 Term=2 的日志复制到大半节点 接下来，由于 Term=2 的日志已经复制到大半节点，如果我们 不遵循前文提到的限制，那么这里 S1 就可以提交 Term=2 的日志\n假设，S1 提交了 Term=2 的日志，考虑后续两种情况：\n情况 d\n这里指 S1 又宕机，并且没有来得及将 Index=3, Term=4 的日志复制到大半节点，于是 S5 可能选举成新的 Leader（S2、S3、S4 都可以投票给 S5），S5 将自己的 Index=2, Term=3 的日志成功复制到 S2、S3、S4\n由于 Index=2, Term=3 日志已经复制到大半节点，S5 自然可以放心提交该日志，然而，Index=2 这个位置的日志已经被 S1 提交过了，这里发生了 数据不一致 的问题\n情况 e\n这里指 S1 在宕机前，成功将 Index=3, Term=4 的日志复制到大半节点，这样即使 S1 宕机，S5 也拿不到大多数节点的投票，自然不会出现情况 d 的数据不一致问题\n为了避免情况 d 的发生，Leader 不能提交之前任期的日志，只能通过提交自己任期的日志，从而间接提交之前任期的日志。\n加上限制条件后，在 c 阶段，S1 就无法提交 Index=2 的日志\nno-op 日志 加上限制条件后，在情况 4，S5 的 Term \u0026gt;= 5，无法提交 Index=2, Term=3\n如果没有客户端继续发起新的请求，那么这个日志永远无法得到提交，给上层 Service 的感觉就是 Raft「卡住了」\n因此，新的 Leader 当选后，可以发送一条 cmd 为空的日志给全体 Follower，这个日志就叫做 no-op 日志\n只要 no-op 日志 被复制到绝大多数节点，新的 Leader 就可以更新 CommitIndex 到 no-op 日志的位置，间接提交 no-op 日志之前的所有日志，避免客户端请求被阻塞\n问题 4: AE 请求，同步还是异步发送？ 在 3B 部分，startReplica 函数中，发送 AE 请求是同步发送的，因为涉及到 offset 的处理，多个 goroutine 操作 offset，边界情况太多\n当时还能通过测试，只不过平均耗时较长，大概 50s，勉强符合「提示」中的小于 1min\n但是在 3C 的 Figure8:Unreliable 中，这个问题就暴露出来了：经常出现 \u0026ldquo;One(xxx) failed to reach agreement.\u0026rdquo; 错误\n通过阅读 Figure8:Unreliable 的源码，发现该部分模拟的是网络不稳定的情况，让集群认为某个节点宕机了（实际上并没有，这也是常说的分布式的网络分区问题），这里网络不稳定包括两个场景：\n延迟响应单个 RPC 请求 断开某个 Server 与集群的连接 第一种情况，如果同步发送 AE RPC，假设该 RPC 等待了 5s 才响应，或者干脆不响应，那么 RPC 库会等「很久」才会将执行结果返回给我们的 startReplica 函数，这样，整个 Replica 的频率就异常的低，之前设置的频率根本就没起作用\n解决方式有两种：\n缩小 RPC 超时时间 异步发送 RPC 实际采用的是第二种，伪代码如下：\nfor !rf.Killed() { time.Sleep(getReplicaTimeout()) // init... // send AE(async) go func() { ok := sendAERequest(args, reply) // other op... }() } 采取异步发送，可以控制整个 Replica 的频率，而不会受到单次 RPC 的影响\n但是需要采取「新的方式」来应对 Follower 拒绝 AE 请求的情况\n采取 Raft 论文提到的方法，下面的描述来自 Lab3C 的提示部分：\n你可能需要一种优化，允许 nextIndex 一次后退多个条目。查看 Raft 论文扩展版，从第 7 页底部到第 8 页顶部（灰线标记处）。论文对细节描述模糊；你需要填补这些空白。一个可能的方案是让拒绝消息包括：\nXTerm: 冲突条目的任期（如果有的话） XIndex: 第一个具有该任期的条目的索引（如果有的话） XLen: 日志长度 然后领导者的逻辑可以是这样的：\n情况 1：跟随者的日志太短：nextIndex = XLen\n情况 2：领导者没有 XTerm：nextIndex = XIndex\n情况 3：领导者有 XTerm：nextIndex = 领导者最后一个 XTerm 条目的索引\n具体实现时：\n每个日志条目需要维护 XIndex Leader 使用 二分查找 的方式确定自己是否具有 XTerm 的条目 问题 5: 乱序 RPC 解决前面的 4 个问题后，出现了数据不一致问题\n前面提到了单个 RPC 的响应可能延迟，如果在此期间，Leader 接受了几个新的客户端请求，就会出现问题\n这是之前的部分代码：\n// Send AE request asynchronously go func() { DPrintf(\u0026#34;{%v}%v: sending AE to follower{%v}\\n\u0026#34;, rf.CurrentTerm, rf.me, serverID) reply := RequestAppendEntriesReply{} if ok := rf.sendRequestAppendEntries(serverID, \u0026amp;args, \u0026amp;reply); !ok { return } rf.mu.Lock() defer rf.mu.Unlock() // 危险！rf.CurrentTerm 有没有改变？ if reply.Term \u0026gt; rf.CurrentTerm { rf.State = StateFollower return } // 危险！rf.logs 有没有改变？ // rf.nextIndex[serverID]、rf.matchIndex[serverID] 有没有被其它 goroutine 更新？ if reply.Success { rf.nextIndex[serverID] = len(rf.Logs) rf.matchIndex[serverID] = len(rf.Logs) - 1 } else { DPrintf(\u0026#34;{%v}%v: follower{%v} reject append Logs\\n\u0026#34;, rf.CurrentTerm, rf.me, serverID) rf.matchIndex[serverID] = 0 // ... } }() 因此，当 RPC 响应时，需要判断 rf.CurrentTerm、rf.nextIndex[serverID]、rf.matchIndex[serverID] 是否改变，如果已经改变了，本次响应就当作过期响应，不做处理\n","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/lab3/lab3c/note/","tags":["分布式","MIT-6.824","Raft"],"title":"MIT-6.824: Lab 3C: persistence"},{"categories":["Distributed-System","Raft"],"content":"Server State 要实现日志复制，Server 需要维护以下状态：\ntype Raft struct { // ... commitIndex int // index of highest log entry known to be committed applyIndex int // index of highest log entry to be applied nextIndex []int // for each server, index of the next log entry to send to that server (initialized to leader last log index + 1) matchIndex []int // for each server, index of highest log entry known to be replicated on server (initialized to 0, increases monotonically) applyCh chan ApplyMsg // a channel on which the tester or service expects Raft to send ApplyMsg messages. } commitIndex：可以提交的日志的最大 Index applyIndex：已经提交的日志的最大 Index nextIndex[i]：要发送给 peers[i] 的起始日志 Index matchIndex[i]：peers[i] 已经同步的日志 Index applyCh：当一个日志被提交，Leader 将这个日志发送到 applyCh，相当于在本地执行命令 Start 方法 Leader 需要实现 Start 方法：\nfunc (rf *Raft) Start(command interface{}) (int, int, bool) Start 方法会被上层客户端调用，请求在 Raft 集群执行 Command\n返回值：\nindex：该 command 如果被提交，在整个 Logs 中的 Index Term：Leader 当前的 Term isLeader：自己是不是 Leader 具体步骤：\n检查自己是不是 Leader，如果不是，直接返回 将 Command 封装成 Log，追加到 rf.logs 中 返回 为了提高性能，不要每次调用 Start 方法，都尝试发送一次 AE RPC，而是「批量」发送，真正的发送操作，在后台任务 startReplica 这里\nAppendEntries RPC 参数与返回值：\ntype RequestAppendEntriesArgs struct { Term int // leader\u0026#39;s term LeaderId int // leader\u0026#39;s id PrevLogIndex int // index of log entry immediately preceding new ones PrevLogTerm int // term of prevLogIndex entry LeaderCommitIndex int LeaderCommitTerm int // term of lastCommitLog Entries []*Entry // Logs } type RequestAppendEntriesReply struct { Term int Success bool } AppendEntries RPC 逻辑：\n如果 term 小于自己的 term，直接 return false 如果 rf.logs[args.LeaderCommitIndex].Term == LeaderCommitTerm，更新 rf.commitIndex（建议只要 Term 合法，就更新 commitIndex，以便快速应用 Command 到本地） 如果自己的 Logs 中，在 prevLogIndex 处，Log 对应的 term 与 prevLogTerm 不一致，return false 基于 args.PrevLogIndex 截断本地日志 判断 AE RPC 是否乱序，根据判断结果决定是否 append entries 到本地日志 return true 2024.5.28 更新：AE RPC 也需要检查是否乱序的问题\n这个问题之前测试一直没有发现，直到 Lab4A 才显现出来\n实现时，需要检查每个 entry，如果 entry 不在 rf.Logs 中，或者 entry 在 rf.Logs 对应位置处的 Term 不相等，才从这条 entry 开始 append\n否则，Follower 可能出现日志部分丢失的情况\n2024.5.30 更新：如果是一个过期的 AE RPC，不应该更新 commitIndex！\n假设原来的 commitIndex=106，如果收到过期的 AE，可能包含的 LeaderCommitIndex 比 commitIndex 小（例如 104）\n如果 Follower 已经 apply 了 [0~106] 的日志，再将 commitIndex 更新为 104，显然不合理\n如果 Follower 返回 false，Leader 怎么处理？\nFollower 返回 false，说明还需要发送 entries 以外的数据给 Follower，Leader 需要给 nextIndex[i] 减一，然后再次尝试发送 AE，直到成功\n「减一」，这样会不会发送很多次 RPC 以后，才能成功？能不能优化？\n论文给出的解释是这样的：\n如果需要的话，下面的协议可以用来优化被拒绝的 AppendEntries RPCs 的个数。\n比如说，当拒绝一个 AppendEntries RPC 的时候，follower 可以包含冲突条目的任期号和自己存储的那个任期的第一个 index。借助这些信息，leader 可以跳过那个任期内所有的日志条目来减少 index。这样就变成了每个有冲突日志条目的任期只需要一个 AppendEntries RPC，而不是每一个日志条目都需要一次 AppendEntires RPC。\n在实践中，我们认为这种优化是没有必要的，因为失败不经常发生并且也不可能有很多不一致的日志条目。\n当然，我在实现时，还是做了一个相对简单的优化：\n类似 TCP 的慢开始阶段，每次发送失败，rf.nextIndex[serverID] = max(rf.nextIndex[serverID] - offset, 0)，其中，offset 会随着 AE 失败而 指数 递增，当 offset 超过门限值 threshold = 64 时，线性递增\n当然这个方式存在一个小问题：无法准确找到第一个 Log 一致的位置，会存在多发送 Log 的情况，可以控制 threshold 的大小来减缓这个情况\n不过，这个开销像较于多次发送 AE RPC 来说，还是比较小的\n2024.5.19 更新：\n实现 Lab3C 时遇到了 one(xxx) failed to reach agreement 的问题，于是将发送 AE 这个操作改成「异步」发送\n修改过后，使用这个方式，不太好控制 offset（多个 goroutine 乱序，offset 很难修改正确）\n于是，放弃了这个优化，最终采用论文提到的优化方式\nBackground Task 主要有两个后台任务：\nstartApply startReplica startApply startApply 会定时将已经提交的日志应用到本地（无论是 Leader 还是 Follower）\n步骤如下：\n更新 commitIndex（如果是 Leader） 向 applyCh「提交」日志 怎么更新？\n使用「二分」思想快速确定：\nL = commitIndex（旧的），R = len(rf.logs) - 1 检查 (L + R) / 2 这个 Index 是否可以提交 剩余二分逻辑\u0026hellip; 而「检查」一个 Index 是否可以提交，需要看看 rf.matchIndex，如果超过半数的 rf.matchIndex 都大于等于该 Index，那么是可以被提交的\nstartReplica Leader 在初始化时，会新建 len(rf.peers) - 1 个 startReplica 后台任务，分别处理与 Follower 的日志同步\n步骤如下：\n检查自己是不是 Leader，如果不是，休眠等待 封装 RequestAppendEntriesArgs 向自己负责的 peer 发送 AE 请求，异步 等待响应 如果发送失败，休眠等待下一次重试 检查 peer 的响应结果： 如果成功，更新 nextIndex[i] 为 len(logs)，matchIndex[i] 为 len(logs)，offset = 1 如果失败： 如果 Follower 的日志太短，更新 nextIndex[i] 为 Follower 的 len(logs) 否则，更新 nextIndex[i] 为本地最后一条 Term 为 XTerm 日志的 index（核心思想是跳过一整个 Term，而不是 nextIndex[i]\u0026ndash;，具体可以看我在 Lab3C 部分提到的：如何应对 Follower 拒绝 AE 请求 ， 上面 也有提到） 重置休眠计时器 正确性验证 单次测试结果：\n循环测试 100 次结果：\n踩的坑 Log 的 Index 从 1 开始，而不是 0 lab 提供的 test_test.go 文件认为 Log 的 Index 从 1 开始，实现时需要注意封装 ApplyMsg 时，将 index 设置为 1 开始，而不是 0\n2024.5.20 更新\n可以引入一个「哨兵日志」来解决这个问题\n同时减少冗余代码\n日志不一致问题 测试用例是 TestRejoin3B 函数\nfunc TestRejoin3B(t *testing.T) { servers := 3 cfg := make_config(t, servers, false, false) defer cfg.cleanup() cfg.begin(\u0026#34;Test (3B): rejoin of partitioned leader\u0026#34;) cfg.one(101, servers, true) // leader network failure leader1 := cfg.checkOneLeader() cfg.disconnect(leader1) // make old leader try to agree on some entries cfg.rafts[leader1].Start(102) cfg.rafts[leader1].Start(103) cfg.rafts[leader1].Start(104) // new leader commits, also for index=2 cfg.one(103, 2, true) // new leader network failure leader2 := cfg.checkOneLeader() cfg.disconnect(leader2) // old leader connected again cfg.connect(leader1) cfg.one(104, 2, true) // all together now cfg.connect(leader2) cfg.one(105, servers, true) cfg.end() } 该函数的执行逻辑：\n先在 Raft 集群中确认 Command = 101 让 Leader1 离线 调用 Leader1 的 Start 方法，在 Leader1 本地追加 102、103、104 三条 Command（因为已经离线，无法得到 Follower 的 ACK，只能调用 Start 而不是 one） 剩余的两个 Follower 选举成功，在 Raft 集群中确认 Command = 103 让 Leader2 离线 让 Leader1 上线 在 Raft 集群中确认 Command = 104 让 Leader2 上线 在 Raft 集群中确认 Command = 105 主要问题在 index=2 处，出现了数据不一致的问题\n逐步分析一下问题在哪：\n起初，三个 Server 都在线，状态如下：\n然后，Leader1 离线，客户端在 Leader1 执行三次 Start：\n然后，集群选举出新的 Leader（Leader2），客户端在集群确认 Command = 103（确认成功，然后 apply，此时，就已经确定了 index=2 处，Command = 103 了）:\n然后，让 Leader2 离线，Leader1 上线，Leader1 在给 Server3 发送心跳时，会发现 Server3 的 Term 比自己大，于是 Leader1 切换状态为 Follower\n现在，整个集群就一个 Leader2，并且离线，自然选出新的 Leader\n假设 Server3 选举成新的 Leader（Leader3）\nLeader3 会给 Server1 发送心跳，于是，Server1 无条件 更新 rf.commitIndex = min(args.LeaderCommitIndex, len(rf.logs) - 1)，即更新为 2\n更新以后，后台任务 startApply 自然会提交 Index=2 处的日志（Command = 102）\n但是 Index=2 的日志已经被 Server2 提交过了，这里就出现了 数据不一致\n如何解决？\n问题出现在 Server1 在没有依靠新的 Leader（即 Leader3）覆盖本地日志前，提交了 过期 的 Log\n我们只需要 添加更新 CommitIndex 的条件 即可：\nif args.LeaderCommitIndex \u0026gt;= 0 \u0026amp;\u0026amp; args.LeaderCommitIndex \u0026lt; len(rf.logs) \u0026amp;\u0026amp; rf.logs[args.LeaderCommitIndex].Term == args.LeaderCommitTerm { // Update commitIndex // Need to pay attention to the situation when LeaderCommitIndex is greater than len(rf.logs) - 1 rf.commitIndex = min(args.LeaderCommitIndex, len(rf.logs) - 1) } 复制时间太长 在最后一个测试 TestBackup3B 中，复制日志的时间太长导致超时无法通过\n后面使用 前文 提到的 RPC Call 优化方法，就 ok 了\n总测试时间太长 6.824 Lab3B 的提示说到了：\nThe \u0026ldquo;ok 6.5840/raft 35.557s\u0026rdquo; means that Go measured the time taken for the 3B tests to be 35.557 seconds of real (wall-clock) time. The \u0026ldquo;user 0m2.556s\u0026rdquo; means that the code consumed 2.556 seconds of CPU time, or time spent actually executing instructions (rather than waiting or sleeping).\nIf your solution uses much more than a minute of real time for the 3B tests, or much more than 5 seconds of CPU time, you may run into trouble later on. Look for time spent sleeping or waiting for RPC timeouts, loops that run without sleeping or waiting for conditions or channel messages, or large numbers of RPCs sent.\n一开始通过全部测试用例，总用时为 80s 左右，比较慢\n后面发现 startReplica 和 startApply 每一轮的睡眠时间较长，均为 200ms\n修改为：\nfunc getApplyTimeout() time.Duration { return time.Millisecond * 25 } func getReplicaTimeout() time.Duration { return time.Millisecond * 50 } 即可将总用时平均控制在 50s 以内\n2024.5.25 更新\nLab4A 要求 Service Operation 的速度比较快，50ms 执行一次 replica 还是太慢了，后面均改成了 10ms\n","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/lab3/lab3b/note/","tags":["分布式","MIT-6.824","Raft"],"title":"MIT-6.824: Lab 3B: log"},{"categories":["Distributed-System","Raft"],"content":"Server State type Raft struct { mu sync.Mutex // Lock to protect shared access to this peer\u0026#39;s state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer\u0026#39;s persisted state me int // this peer\u0026#39;s index into peers[] dead int32 // set by Kill() // Your data here (3A, 3B, 3C). // Look at the paper\u0026#39;s Figure 2 for a description of what // state a Raft server must maintain. State RaftState // state of current Raft Node currentTerm int // latest term server has seen (initialized to 0 on first boot, increases monotonically) votedFor int // candidateId that received vote in current term (or null if none) logs []*Entry // log entries; each entry contains command for state machine, and term when entry was received by leader (first index is 1) electionTimer *time.Timer // election timer heartbeatTimer *time.Timer // heartbeat timer } Server 需要维护：\n当前的身份 当前的 term 当前 term 投票给谁 日志列表 选举计时器与心跳计时器 func Make(peers []*labrpc.ClientEnd, me int, persister *Persister, applyCh chan ApplyMsg) *Raft { rf := \u0026amp;Raft{} rf.peers = peers rf.persister = persister rf.me = me // Your initialization code here (3A, 3B, 3C). rf.State = StateFollower rf.currentTerm = 0 rf.votedFor = -1 rf.logs = make([]*Entry, 0) rf.electionTimer = time.NewTimer(getElectionTimeout()) rf.heartbeatTimer = time.NewTimer(getHeartbeatTimeout()) // initialize from state persisted before a crash rf.readPersist(persister.ReadRaftState()) // start ticker goroutine to start elections go rf.ticker() return rf } State transition relationship 选举计时器超时（Leader 超时没有发送心跳） 选举成功（获得半数以上投票） 选举失败（投票数不够，或者有了新的 Leader） 有了比自己 term 更大的 Leader（通常发生在旧 Leader 网络分区问题恢复时） RPC AppendEntries RPC type RequestAppendEntriesArgs struct { Term int // leader\u0026#39;s term LeaderId int // leader\u0026#39;s id } type RequestAppendEntriesReply struct { } // RequestAppendEntries RPC handler. // expired leader, follower and candidate use this func (rf *Raft) RequestAppendEntries(args *RequestAppendEntriesArgs, reply *RequestAppendEntriesReply) { } 在 3A 部分，AE Request 主要用于心跳请求，Follower 会在一段时间没有收到心跳请求后，开始选举过程\nServer（Follower、Candidate、或者是 过期的 Leader），收到 AE 后，应该立即：\n更新自己的状态为 Follower 重置选举计时器 RequestVote RPC type RequestVoteArgs struct { Term int // candidate’s term CandidateId int // candidate requesting vote LastLogIndex int // index of candidate’s last log entry (§5.4) LastLogTerm int // term of candidate’s last log entry (§5.4) } type RequestVoteReply struct { Term int // currentTerm, for candidate to update itself VoteGranted bool // true means candidate received vote } // example RequestVote RPC handler. func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { } 参数需要包括：\nCandidate 的 term，用于 Receiver 判断此次投票请求是否过期 Candidate 的 ID，用于 Receiver 记录投票对象，以及判断是否可以投票 LastLogIndex、LastLogTerm：用于 Receiver 判断 candidate 的 Log 是否比自己新（如果 candidate 的日志比自己旧，不能让其成为 leader，否则会丢失较多数据，因为 Follower 会根据 Leader 来截断自己的 Log） 返回值包括：\nReceiver 的 Term：如果 Receiver 的 Term 比自己（Candidate）大，说明已经有了新的 Leader，本次选举失败，更新自己的 Term VoteGranted：Receiver 是否给自己投票 一个 Server 在收到 RV 请求后，需要判断自己是否可以给这个 Candidate 投票，主要考虑：\nCandidate 的 Term 是否比自己大（或者相等） Candidate 的 Log 是否比自己新 receiver 怎么判断 log 是否 up-to-date？\ncurTerm \u0026lt; lastLogTerm(candidate\u0026rsquo;s term) curTerm == lastLogTerm \u0026amp;\u0026amp; len(rf.log) \u0026lt; lastLogIndex(candidate\u0026rsquo;s last log idx) 无论哪个 RPC，只要 args 中的 Term 比自己大，就要修改自己的身份为 Follower\nBackground Task 主要有两个后台任务：\n选举任务 发送心跳任务 func (rf *Raft) ticker() { for rf.killed() == false { // Your code here (3A) // Check if a leader election should be started. select { case \u0026lt;-rf.electionTimer.C: rf.mu.Lock() if rf.State != StateLeader { rf.currentTerm += 1 rf.startElection() } rf.electionTimer.Reset(getElectionTimeout()) rf.mu.Unlock() case \u0026lt;-rf.heartbeatTimer.C: rf.mu.Lock() if rf.State == StateLeader { rf.startHeartbeat() } rf.heartbeatTimer.Reset(getHeartbeatTimeout()) rf.mu.Unlock() } } } startElection 修改自己的状态为 Candidate 给自己投一票 异步 向所有 peer（除了自己）发送 RV 请求 发送 RV 请求的 goroutine 执行逻辑如下：\n发送 RV 请求 判断是否发送成功：如果失败返回，否则： 判断返回的 Term 与自己的 Term 的关系： 如果比自己大，说明有新的 Leader，更新自己的 Term，修改状态为 Follower，放弃本次选举，否则： 如果 Receiver 给自己投票了，那么： 票数 ++ 如果票数超过一半的 peer，说明选举成功 选举成功，立即向所有 peer 发送心跳，告诉 peer 自己是新的 leader startHeartbeat 向所有 peer（除了自己）异步发送 AE 请求，忽略发送失败的情况即可\n正确性验证 执行下面的 bash 脚本\n#!/bin/bash # 定义执行次数 runs=100 # 循环执行命令 for (( i=1; i\u0026lt;=$runs; i++ )); do echo \u0026#34;Running test $i\u0026#34; go test -timeout 30s -run \u0026#34;^TestInitialElection3A$\u0026#34; -count=1 6.5840/raft if [ $? -ne 0 ]; then echo \u0026#34;Error: Test failed. Exiting...\u0026#34; exit 1 fi go test -timeout 30s -run \u0026#34;^TestReElection3A$\u0026#34; -count=1 6.5840/raft if [ $? -ne 0 ]; then echo \u0026#34;Error: Test failed. Exiting...\u0026#34; exit 1 fi go test -timeout 30s -run \u0026#34;^TestManyElections3A$\u0026#34; -count=1 6.5840/raft if [ $? -ne 0 ]; then echo \u0026#34;Error: Test failed. Exiting...\u0026#34; exit 1 fi done echo \u0026#34;All tests passed successfully.\u0026#34; 结果如下：\n踩的坑 大量使用原子操作（锁粒度太小） 例如使用原子操作读取和写入 Term、VoteFor\n实际上就是将锁的粒度控制得 非常小，看起来可以提升并发，但是有可能会写入不符合预期的结果\n可以将锁的粒度控制得比较大，实现时：\n将整个 RPC 内的操作锁住，使 Handler 趋近于线性执行 将 startElection 和 startHeartbeat 锁住 即：全局使用同一个 mutex\n同步发送 AE 和 RV 如果网络不稳定，单个发送操作比较耗时，可能导致选举时间太长，导致超时\n同样的，如果同步发送 AE，某一个心跳发送的时间太长，会影响到下一次心跳\n因此，发送过程应该是异步的\n选举超时时间与心跳频率 实验要求 1s 内，最多发送 10 次心跳（100ms 一次）\n这就相当于要求我们的选举超时时间 必须大于 100ms\n考虑到 RTT，可以略大一些：\nfunc getElectionTimeout() time.Duration { ms := 200 + (rand.Int63() % 200) return time.Duration(ms) * time.Millisecond } func getHeartbeatTimeout() time.Duration { return time.Millisecond * 100 } 收到 RV 请求，没有重置自己的选举超时计时器 这可能会带来不必要的选举\n","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/lab3/lab3a/note/","tags":["分布式","MIT-6.824","Raft"],"title":"MIT-6.824: Lab 3A: leader election"},{"categories":["Distributed-System"],"content":"分布式存储的难点 为什么要分布式存储？因为提升单台计算机的能力是有限的，要想存储大量的数据，不得不引入 多台 计算机，提高 性能\n但是，计算机是不可靠的，集群中总会有一些计算机会出现一些问题，这要求我们的系统具有 错误忍受（Fault Tolerance） 的能力\n实现容错，最常用的方式就是引入多个副本存储冗余数据，这样，即使某个副本出现问题，还可以切换到另一个副本。但是，副本之间的数据可能不完全同步，也就是 一致性保障\n如果要保证副本间数据完全一致，这就是我们通常意义上的「强一致性」，那么需要引入很多额外的操作，系统的 性能又会降低\n可以发现，整个形成了一个闭环，我们不得不 在性能与一致性之间做权衡，这也是分布式理论常常聊到的 C、A\nGFS 的设计目标 GFS（Google File System），是 Google 内部使用的一种分布式文件系统\nGFS 追求的是：\n大文件的读写 高吞吐 顺序读写 具有容错能力：分片（shard）、故障恢复 较弱的一致性 架构 下面是 GFS 论文的架构图：\nMaster 节点 职责 Master 节点在 GFS 中起到一个协调作用，整个 GFS 集群中 只有一个 Master\nGFS 把单个文件分成了若干个 块（chunk），ChunkServer 负责存储实际数据，而 Master 负责管理文件与 chunk 的信息\n维护了什么数据 为了将客户端的读写请求正确路由到 ChunkServer，Master 需要维护：\nFileName 与 Chunk 数组的映射关系 ChunkID 与 Chunk 数据的映射关系 这里的 Chunk 数据包括：\nChunkServer 数组：一个 Chunk 会存储在多个 Server 上 Version：该 Chunk 的版本号 Primary Chunk Server：对 Chunk 的写操作需要在 Primary Chunk Server 上顺序处理 租期时间：Primary Chunk Server 在租期时间后，不再是 Primary Server 关于租期时间，后面 还会聊到\n这些维护的数据（元数据）都存储在内存中\n不过仅仅存储在内存中，是不够的，Master 应该将：\nFileName 与 Chunk 数组的映射关系 Version 持久化到磁盘，以保证重启时可以恢复数据\n为什么需要将这两个数据持久化？\n假设不持久化 FileName 与 Chunk 数组的映射关系，那么 Master 重启时，需要向所有 ChunkServer 请求，并汇总，得到映射关系，这个过程非常耗时\n而 Version 的作用主要是用于判断某个 ChunkServer 上的某个 Chunk 是不是 最新 的\n为啥不直接使用最大的 Version？\n假设 Master 重启后，向所有 ChunkServer 请求每个 Chunk 的 Version，这里存在问题：有可能 Version 最大的 ChunkServer 无法与 Master 通信（但是可以和客户端通信，即网络分区问题），那么 Master 拿到的 Version，就不是真实最大的 Version\n因此，Version 需要持久化\n怎么持久化？\nMaster 会以「操作日志」的方式，间接的将这些数据持久化到磁盘\n当 Master 故障重启，并重建它的状态，你不会想要从 log 的最开始重建状态，因为 log 的最开始可能是几年之前，所以 Master 节点会在磁盘中创建一些 checkpoint 点，这可能要花费几秒甚至一分钟。这样 Master 节点重启时，会从最近的 checkpoint 恢复\nGFS 读取数据的过程 Client 要想读取数据，需要给 Master 提供要读取的文件名、读取的起始偏移 Offset\nMaster 收到了 Client 的读取请求，会先从 FileName 到 Chunk 数组的映射 中获取 Chunk 数组\n由于每个 Chunk 数组的大小为 64M，因此，可以很轻松的根据 Offset 获取待读取的 Chunk\n知道应该读取哪个 Chunk 后，Master 会读取 ChunkID 到 ChunkServer 数组的映射，并将 ChunkServer 数组返回给 Client\nClient 有了 ChunkServer 数组，会从里面选择一个网络位置距离自己最近的 ChunkServer 读取数据（根据 IP 地址判断，Google 机房的 IP 是连续的）\nClient 会缓存 ChunkServer 数组，这样后续读取数据，就可以不用经过 Master，减小 Master 的请求压力\n读取跨越多个 chunk 的情况 Client 可能会读取不止一个 Chunk 的数据，例如，一个 Client 读取了大于 64M 的数据，或者 Client 仅仅读取两个字节，但是这两个字节恰好处于两个不同的 Chunk 中\n客户端本身依赖了一个 GFS 的库，这个库会注意到读请求跨越了 Chunk 的边界 ，并会将读请求拆分，之后再将它们合并起来。所以这个库会与 Master 节点交互，Master 节点会告诉这个库说 Chunk7 在这个服务器，Chunk8 在那个服务器。之后这个库会说，我需要 Chunk7 的最后两个字节，Chunk8 的头两个字节。GFS 库获取到这些数据之后，会将它们放在一个 buffer 中，再返回给调用库的应用程序。\nMaster 节点会告诉库有关 Chunk 的信息，而 GFS 库可以根据这个信息找到应用程序想要的数据。应用程序只需要确定文件名和数据在整个文件中的偏移量，GFS 库和 Master 节点共同协商将这些信息转换成 Chunk。\n上面引用的内容来自 MIT-6.824 Lecture03-GFS 课程，Robert 教授的解释，翻译自 3.5 GFS 读文件（Read File） GFS 写入数据的过程 我们这里仅讨论 Append 的情况\nMaster 收到了 Client 的写入请求，会先从 FileName 到 Chunk 数组的映射 中获取 Chunk 数组\n由于 Chunk 的写入，必须在 Primary Server（主副本） 上进行，因此，Master 需要确定最后一个 Chunk 的 Primary Server 和 Secondary Server\n在某一个时间点，Master 不一定指定了 Chunk 的主副本。所以，写文件的时候，需要考虑 Chunk 的主副本不存在的情况。\n假设没有指定 Chunk 的主副本，Master 会在所有可用副本中，选择 Chunk Version 与 Master 上保存 Chunk Version 一致 的副本集合（即数据是最新的）\n然后，这些副本集合中，会选择一个作为 Primary Server，其余的作为 Secondary Server\n然后，Master 通知 Primary 和 Secondary 服务器，你们可以修改这个 Chunk，并给 Primary Server 设定租期（60s），当租期到了以后，Primary Server 就不再是 Primary 的了（租期的作用后面会聊）\n现在，Master 需要更新本地存储的 Version，并持久化\n有了 Primary Server 和 Secondary Server，Master 就可以将这些副本集合发给 Client\n再来看看 Client 有了副本集合以后，数据的发送过程：\nClient 给所有副本集合发送待追加的数据 副本集合收到数据后，会将数据存到一个临时文件中，并给 Client 发送 ACK Client 收到 ACK 以后，会告诉 Primary Server，说：“你可以将数据追加到 Chunk 中了” Primary Server 收到请求后，会检查是否可以写入，如果可以，那么写入，并通知所有的 Secondary Server，可以写入 每个 Secondary Server 收到请求后，会检查是否可以写入，如果可以，那么写入，并给 Primary Server 发 \u0026ldquo;yes\u0026rdquo;，否则写入失败，发送 \u0026ldquo;fail\u0026rdquo; 只有到 Primary Server 收到了所有 Secondary Server 的 \u0026ldquo;yes\u0026rdquo;，才会给 Client 发送写入成功，否则告诉 Client，写入失败了 GFS 论文说，如果客户端从 Primary 得到写入失败，那么客户端应该重新发起整个追加过程。客户端首先会重新与 Master 交互，找到文件末尾的 Chunk；之后，客户端需要重新发起对于 Primary 和 Secondary 的数据追加操作。\n一些问题 下面的问题来自 MIT-6.824 Lecture03-GFS 课程 学生提问\n写文件失败之后 Primary 和 Secondary 服务器上的状态如何恢复\nGFS 没有做什么额外措施（例如回滚成功写入的数据），因此，如果写入失败，那么副本间的数据可能会 不一致\n写文件失败之后，读 Chunk 数据会有什么不同\n取决于读的哪个副本\n如果读的是写入成功的副本，那么就可以读到之前追加的数据；否则读不到\n什么时候版本号会增加\n版本号只在 Master 节点认为 Chunk 没有 Primary 时才会增加。\n在一个正常的流程中，如果对于一个 Chunk 来说，已经存在了 Primary，那么 Master 节点会记住已经有一个 Primary 和一些 Secondary，Master 不会重新选择 Primary，也不会增加版本号。它只会告诉客户端说这是 Primary，并不会变更版本号。\n可不可以通过版本号来判断副本是否有之前追加的数据\n所有的 Secondary 都有相同的版本号。版本号只会在 Master 指定一个新 Primary 时才会改变。\n所以，副本（参与写操作的 Primary 和 Secondary）都有相同的版本号，你没法通过版本号来判断它们是否一样\n客户端将数据拷贝给多个副本会不会造成瓶颈\n在 GFS 论文中（包括前文提到的 写入数据过程 ），都说到「Client 给所有副本集合发送待追加的数据」\n那 Client 的性能与网络带宽不会成为瓶颈吗？\n实际上，之后，论文又改变了说法，说 客户端只会将数据发送给离它最近的副本，之后那个副本会将数据转发到另一个副本，以此类推形成一条链，直到所有的副本都有了数据。这样一条数据传输链可以在数据中心内减少跨交换机传输（否则，所有的数据吞吐都在客户端所在的交换机上）。\n如果 Master 节点发现 Primary 挂了会怎么办 =\u0026gt; 租期的作用\nMaster 指定了一个 Primary 后，会定期的 Ping 它，如果超时没有收到应答，那么 Master 认为这个 Primary 挂了\n如果此时立即指定一个新的 Primary，会出现 脑裂 的问题：Primary 可能并没有挂，只是网络分区问题\n立即指定一个新的 Primary，那么该 Chunk 就会有两个 Primary，这是我们不想看到的\n因此，GFS 在实现这一点时，并没有立即指定新的 Primary，而是等待当前 Primary 的 租期到期 以后，才指定新的 Primary，这样，就可以安全指定 Primary 而无需担心脑裂的问题\n如果是对一个新的文件进行追加，那这个新的文件没有副本，会怎样\n基本上与 写入数据过程 一致\nMaster 会从客户端收到一个请求说，我想向这个文件追加数据。\nMaster 节点会发现，该文件没有关联的 Chunk。Master 节点会创造一个新的 Chunk ID。\n之后，Master 节点通过查看自己的 Chunk 表单发现，自己其实也没有 Chunk ID 对应的任何信息。\n之后，Master 节点会创建一条新的 Chunk 记录说，我要创建一个新的版本号为 1，再随机选择一个 Primary 和一组 Secondary 并告诉它们，你们将对这个空的 Chunk 负责，请开始工作。论文里说，每个 Chunk 默认会有三个副本，所以，通常来说是一个 Primary 和两个 Secondary。\nGFS 的一致性 前面提到，如果写入失败，副本间会出现数据不一致的问题\n因此，GFS 的一致性保障是比较弱的，这样设计与 GFS 的追求相同：高吞吐，高性能，允许一定的数据不一致\n如果要追求强一致性，会不可避免的提高整个系统的复杂度，性能也会降低\n如果要将 GFS 设计成强一致的，可以考虑以下几点：\nPrimary 有重复检测的能力：实现 Exactly Once Secondary 必须成功执行写入请求，而不是返回一个错误 Secondary 有可能同步比较慢，客户端可能需要强制请求 Primary 以获取最新的数据 GFS 的局限性 GFS 最严重的问题在于：只有一个 Master 节点\n只有一个 Master，应对大量的 Client 的请求，可能处理不过来 Master 挂了，故障转移是手动的 Client 很难发现自己读取的数据可能是「错误」的 参考资料 MIT-6.824 Lecture03-GFS The Google File System ","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/gfs/note/","tags":["分布式","MIT-6.824"],"title":"GFS --- The Google File System"},{"categories":["Distributed-System"],"content":"容错方案：复制 设计分布式系统，必须要考虑的一个点就是容错（Fault Tolerance）\n常见的容错方案就是 复制，提供多个副本，如果其中一个副本挂了，其它的副本能快速补上，整个过程对用户是无感的，实现容错\n复制能处理什么样的故障 复制只能处理：单台计算机的 fail-stop 故障\n什么是 fail-stop 故障？\nFail-stop 是一种容错领域的通用术语。它是指，如果某些东西出了故障，比如说计算机，那么它会单纯的停止运行。当任何地方出现故障时，就停止运行，而不是运算出错误结果。例如，某人将你服务器的电源线踢掉了，那就会产生一个 fail-stop 故障。类似的，如果某人拔了你的服务器的网线，即使你的服务器还在运行，那也算是一个 fail-stop 故障。\n如果编写的软件本身就有问题（例如计算上的问题），那复制肯定解决不了这类问题\n当然，如果有一些不相关的软件运行在你的服务器上，并且它们导致了服务器崩溃，例如 kernel panic 或者服务器重启，虽然这些软件与你服务的副本无关，但是这种问题对于你的服务来说，也算是一种 fail-stop。\n副本间遵循原则 副本间应该 互不相关，也就是说，副本之间的错误是相互独立的\n例如，我们在同一供应商采购了 1000 台计算机，如果其中一台计算机存在设计问题，那剩余的计算机有很大可能也会有这个问题\n在这种情况下，即使有冗余副本，副本本身的可用性就值得怀疑，整个系统的可用性是无法得到保障的\n还有一种情况就是：所有副本在同一个机房\n假设机房电源或者网络出现问题，那么所有副本都将变得不可用\n复制是否值得 复制使用了我们实际需要的 2-3 倍的计算机资源。GFS 对于每个数据块都有 3 份拷贝，所以我们需要购买实际容量 3 倍的磁盘。VMware FT 复制了一份，但这也意味着我们需要两倍的计算机，CPU，内存。这些东西都不便宜，所以自然会有这个问题，这里的额外支出真的值得吗？\n这个问题主要是从 经济角度 上考虑的，取决于你的服务能够提供的价值\n假设你的系统是为银行服务的，如果系统挂了，那么很有可能失去你的用户的信任，造成很大的经济损失\n状态转移与复制状态机 假设有两台服务器，其中一个为 Primary，另一个为 Backup\n我们希望 BackUp 与 Primary 保持同步，这样即使 Primary 挂了，Backup 也可以顶上\n状态转移 状态转移背后的思想是，Primary 将自己完整状态，比如说内存中的内容，拷贝 并发送给 Backup。Backup 会保存收到的最近一次状态，所以 Backup 会有所有的数据。当 Primary 故障了，Backup 就可以从它所保存的最新状态开始运行。所以，状态转移就是发送 Primary 的状态。\n状态转移实现比较简单，但是网络传输成本较高：内存的完整快照是一个很大的数据量\n复制状态机 计算机软件可以分为确定部分和不确定部分：\n确定部分：程序内部的执行逻辑 不确定部分：用户的输入 通常情况下，如果一台计算机没有外部影响，它只是一个接一个的执行指令，每条指令执行的是计算机中内存和寄存器上确定的函数，只有当外部事件干预时，才会发生一些预期外的事。\n基于这个事实，复制状态机不会在不同的副本之间发送状态，相应的，它只会从 Primary 将这些外部事件，例如外部的输入，发送给 Backup。\n复制状态机传输的数据量远远小于状态转移，但是实现较为复杂，需要考虑到底要复制哪些状态\nVMWare-FT 的工作原理 VMWare-FT 的实现需要两个 物理机：\nPrimary Backup Primary、Backup 以及 Client 会使用网络连接：\n当 Client 向 Primary 发送一个请求：\nClient 发送给 Primary 的请求首先会被 VMM 观察到\nVMM 会向上传输数据给 OS，并且给 Backup 同步这一网络数据包\nPrimary、Backup 的 App 执行完毕以后，需要返回结果给 Client：\nBackup 的返回结果会在 VMM 这一层被拦截，只需要 Primary 响应就可以了，不需要重复输出\n注意： Primary 在响应结果前，必须收到 Backup 的 ACK，这样才能保证客户端视角下的一致性（在输出控制 那一块会提到）\nPrimary 与 Backup 同步的数据流的通道称之为 Log Channel\n对于「怪异指令」的处理 什么是「怪异指令」？\n简单来说，在 Primary 和 Backup 中产生不同结果的指令，就是「怪异指令」\n比如获取当前时间，或者获取当前处理器序号，或者获取已经执行的的指令数，或者向硬件请求一个随机数用来加密，这种指令相对来说都很少见。\n如果一个指令是怪异指令，在 Backup 上重放时会被特殊处理\n输出控制 输出控制就是在 前面 提到的注意事项\n具体直接看 Lecture4，Robert 教授的讲解 吧\nTest-And-Set 服务 当 Backup 在较长时间内都没有收到 Primary 在 Log Channel 上的同步请求，Backup 会认为 Primary 挂了\n同样地，Primary 在较长一段时间内没有收到 Backup 的 ACK，也会认为 Backup 挂了\n发生的条件有两种：\nPrimary 或者 Backup 真的挂了 网络分区问题 Primary 或者 Backup 如果要想上线，不能直接上线（因为可能是单纯的网络分区问题，两个 Primary 会导致数据不一致），而是要经过 Test-And-Set 服务\nTest-and-Set 服务不运行在 Primary 和 Backup 的物理服务器上，VMware FT 需要通过网络支持 Test-and-Set 服务。这个服务会在内存中保留一些标志位，当你向它发送一个 Test-and-Set 请求，它会设置标志位，并且返回旧的值。Primary 和 Backup 都需要获取 Test-and-Set 标志位，这有点像一个锁。为了能够上线，它们或许会同时发送一个 Test-and-Set 请求，给 Test-and-Set 服务。当第一个请求送达时，Test-and-Set 服务会说，这个标志位之前是 0，现在是 1。第二个请求送达时，Test-and-Set 服务会说，标志位已经是 1 了，你不允许成为 Primary。对于这个 Test-and-Set 服务，我们可以认为运行在单台服务器。当网络出现故障，并且两个副本都认为对方已经挂了时，Test-and-Set 服务就是一个仲裁官，决定了两个副本中哪一个应该上线。\n也就是说，VMware-FT 的主从切换还是要依靠外部系统，不是独立的，这要求 Test-And-Set 服务本身具有一定的可用性（应该也是一个具有 FT 的系统）\n","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/vmware-ft/note/","tags":["分布式","MIT-6.824"],"title":"VMWare-FT"},{"categories":["Distributed-System"],"content":"实验要求 直接看这个链接：https://pdos.csail.mit.edu/6.824/labs/lab-kvsrv.html\n实现 本次实验较为简单，提一点即可：\n为了正确处理 RPC 请求 丢失 或其它异常，服务端需要实现 Exactly Once，即对于写请求，应该 有且仅执行一次\n实现方式比较简单，参考 Kafka 的实现方式\n客户端生成一个随机 ID 作为 client_id，以及一个 epoch\n服务端维护 client_id 与 epoch 的状态\n如果某一次写请求，客户端提供的 epoch 小于等于服务端保存的该客户端的 epoch，那么服务端直接忽略请求\n注意：\n实验要求即使是过期的消息，也要返回执行结果，因此，服务端还需要维护 client_id 与 lastReply 的映射关系 实验要求服务端应该及时释放内存 Your scheme for duplicate detection should free server memory quickly, for example by having each RPC imply that the client has seen the reply for its previous RPC. It\u0026rsquo;s OK to assume that a client will make only one call into a Clerk at a time.\n在做这个 lab 的时候，释放内存 这块卡了一会，最终的实现方式如下：\n仅当 client 发起一个 Append 请求，我们才校验 epoch（对于 Get 请求，直接返回最新的执行结果；对于 Put 请求，直接写即可） 仅当 client 发起一个 Append 请求，我们才缓存 client_id 与 lastReply 的映射关系 当 client 发起一个 Get/Put 请求，需要删除 client_id 与 lastReply 的映射关系 放一段核心代码感受一下：\nfunc (kv *KVServer) do(req *Request) *Reply { reply := \u0026amp;Reply{ status: FetchSuccess, } var err error // handle epoch status if req.reqType == AppendRequest \u0026amp;\u0026amp; !kv.checkAndUpdateEpoch(req) { DPrintf(\u0026#34;client_id: %v, cur epoch: %v, last: %v\u0026#34;, req.clientID, req.epoch, kv.clientEpoch[req.clientID]) if kv.lastReply[req.clientID] != nil { reply = kv.lastReply[req.clientID] } reply.status = FetchExpired return reply } // kv.lastReply[req.clientID] = nil switch req.reqType { case GetRequest: err = kv.get(req, reply) case PutRequest: err = kv.put(req, reply) case AppendRequest: err = kv.append(req, reply) default: log.Printf(\u0026#34;Invalid Request Type: %v\u0026#34;, req.reqType) reply.status = FetchFail } if err != nil { log.Printf(\u0026#34;do failed, err: %v\u0026#34;, err.Error()) reply.status = FetchFail } if req.reqType == AppendRequest { kv.lastReply[req.clientID] = reply } else { // remove epoch status delete(kv.clientEpoch, req.clientID) delete(kv.lastReply, req.clientID) } return reply } 正确性验证 注意：截止 2024.4.19，从 git://g.csail.mit.edu/6.5840-golabs-2024 6.5840 clone 的仓库，srv/kvsrv/test_test.go 的第 598 行为：\nif m1 \u0026gt;= 3*MEM*N { 这个应该是一个 bug，要检验内存是否超出使用限制，应该将 m1 - m0，即终止内存占用与起始内存占用的差，与 limit，即 3*MEM*N 做比较\n正确代码应该是：\nif m1 \u0026gt;= m0+3*MEM*N { 修改之后，就能通过所有测试用例了：\n参考资料 6.5840 Lab 2: Key/Value Server ","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/lab2/note/","tags":["分布式","MIT-6.824"],"title":"MIT-6.824: Lab 2: Key/Value Server"},{"categories":["Distributed-System"],"content":"MapReduce MapReduce 是一种分布式编程模型，用于大规模数据处理。它由 Google 开发，用于处理 Google 搜索引擎索引的网页数据。\nMapReduce 利用了 分布式计算 的优势，将大规模数据分成小的块（分治思想），然后在每个块上应用一个 Map 函数和一个 Reduce 函数，最后将结果合并。\n架构 直接引用 MapReduce 论文 的一张图片：\n整体有两个角色：\nCoordinator（图中的 Master） Worker Coordinator 负责分布式协调，将任务分发给不同的 Worker\n而 Worker 负责执行 Coordinator 分发的任务即可\n值得一提的是，这里的 MapReduce 实现是基于本地文件系统的\n然而在更常见的分布式情况下，使用的一般是分布式文件系统（例如 GFS）\n下文的 Lab 实现是基于 Local File System 的\nMap Map 阶段主要是根据用户提供的 map 方法，将大规模的数据分割成一个 K-V List（分治），定义如下：\ntype KeyValue struct { Key string Value string } func Map(filename string, contents string) []mr.KeyValue { } 每一个分割得到的 K-V List 会被一个 worker 写到 若干个 中间文件中（intermediate file）\n分布式体现在哪里？\n一般来说，会传给 Coordinator 多个 file，这些 file 就可以被分发到不同的 worker 上去执行\nReduce 当 Map 阶段结束后，可以开始执行 Reduce 阶段\nReduce 更像是一个「聚合」操作，将刚才生成的 K-V List 聚合起来\n具体来说，每一个 worker 会负责处理，某一个 Map 之后，得到的若干个 intermediate file\nworker 会根据 Key 来整合这些 intermediate file：一个 Key，对应多个 Value\nReduce 操作的定义如下：\nfunc Reduce(key string, values []string) string { } 这样，我们就可以得到巨大文件中，某一个 Key Reduce 之后的结果了\n分布式体现在哪里？\nMap 阶段，每一个 file 都会按照 key 的哈希值，分成若干个 reduce 部分\n例如，假设 Map 处理了 5 个 file，那么在 Reduce 阶段，就可以有 5 个 worker 并行处理这 5 * nReduce 个 intermediate file\n瓶颈 如果是基于分布式文件系统实现的 MapReduce，瓶颈主要在：\n节点性能 网络性能 下面用 Lecture1 学生提的一个问题来解释：\n这里的箭头代表什么意思？\n下文引用自 Robert 教授，翻译自 肖宏辉 通常情况下，如果我们在一个例如 GFS 的文件系统中存储大的文件，你的数据分散在大量服务器之上，你需要通过网络与这些服务器通信以获取你的数据。在这种情况下，这个箭头表示：\nMapReduce 的 worker 需要通过网络，与存储了输入文件的 GFS 服务器通信 通过网络将数据读取到 MapReduce 的 worker 节点，进而将数据传递给 Map 函数。 这是最常见的情况。并且这是 MapReduce 论文中介绍的工作方式。但是如果你这么做了，这里就有 很多网络通信。 如果数据总共是 10TB，那么相应的就需要在数据中心网络上移动 10TB 的数据。而数据中心网络通常是 GB 级别的带宽，所以移动 10TB 的数据需要大量的时间。在论文发表的 2004 年，MapReduce 系统最大的限制瓶颈是网络吞吐。如果你读到了论文的评估部分，你会发现，当时运行在一个有数千台机器的网络上，每台计算机都接入到一个机架，机架上有以太网交换机，机架之间通过 root 交换机连接（最上面那个交换机）。\n如果随机的选择 MapReduce 的 worker 服务器和 GFS 服务器，那么至少有一半的机会，它们之间的通信需要经过 root 交换机，而这个 root 交换机的吞吐量总是固定的。如果做一个除法，root 交换机的总吞吐除以 2000，那么每台机器只能分到 50Mb/S 的网络容量。这个网络容量相比磁盘或者 CPU 的速度来说，要小得多。所以，50Mb/S 是一个巨大的限制。\n在 MapReduce 论文中，讨论了大量的 避免使用网络的技巧。其中一个是 将 GFS 和 MapReduce 混合运行在一组服务器上。所以如果有 1000 台服务器，那么 GFS 和 MapReduce 都运行在那 1000 台服务器之上。当 MapReduce 的 Master 节点拆分 Map 任务并分包到不同的 worker 服务器上时，Master 节点会找出输入文件具体存在哪台 GFS 服务器上，并把对应于那个输入文件的 Map Task 调度到同一台服务器上。\n所以，默认情况下，这里的箭头是 指读取本地文件，而不会涉及网络。虽然由于故障，负载或者其他原因，不能总是让 Map 函数都读取本地文件，但是几乎所有的 Map 函数都会运行在存储了数据的相同机器上，并因此节省了大量的时间，否则通过网络来读取输入数据将会耗费大量的时间。\n我之前提过，Map 函数会将输出存储到机器的本地磁盘，所以存储 Map 函数的 输出不需要网络通信，至少不需要实时的网络通信。但是，我们可以确定的是，为了 收集 所有特定 key 的输出，并将它们传递给某个机器的 Reduce 函数，还是需要网络通信。\n实现 Worker Worker 只需要执行 Coordinator 分配的任务：\n// // main/mrworker.go calls this function. // func Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { // Your worker implementation here. lastTaskID := -1 for { reply := CallFetchTaskHandler(lastTaskID) if reply == nil { // call failed return // just return } var err error switch reply.Type { case MapTask: err = doMapTask(reply.TaskID, reply.FileName, reply.NReduce, mapf) case ReduceTask: err = doReduceTask(reply.TaskID, reply.NMap, reply.NReduce, reducef) case WaitTask: time.Sleep(time.Second) case ExitTask: return default: log.Println(\u0026#34;Unexpected task type\u0026#34;) } if err != nil { // this round of work failed log.Println(err.Error()) lastTaskID = -1 } else { lastTaskID = reply.TaskID } } // uncomment to send the Example RPC to the coordinator. // CallExample() } 然后分别实现 Map 和 Reduce 的逻辑即可\nCoordinator Coordinator 是 MapReduce 的控制中心，负责协调所有 Worker 的运行。\n我们可以使用两个队列来分别维护处于等待状态的任务和处于运行状态的任务：\nwaittingTaskQueue []Task runningTaskQueue []Task finnishTask map[int]struct{} // 记录已经完成的 task_id Task 的定义如下：\ntype Task struct { ID int StartTime time.Time Type TaskType FileName string } 任务分配 处理该 Worker 上一次任务的执行结果\nWorker 会发来自己上一次执行的 task_id，这样，Coordinator 就知道哪些任务已经完成，哪些任务没有完成\n尝试从 runningTaskQueue 中获取任务\n如果 runningTaskQueue 队头的任务执行完毕（使用 finnishTask 判断），移除，重新执行第一步 如果 runningTaskQueue 队头的任务执行超时（可以根据 StartTime 判断），我们认为之前的 Worker 挂了，将这个任务分配给当前 Worker 否则，没有超时，获取任务失败 尝试从 waitingTaskQueue 中获取任务\n如果从 runningTaskQueue 获取任务失败，就尝试从 waitingTaskQueue 中获取任务\n注意：如果获取成功，需要将任务移到 runningTaskQueue，并设置开始时间\n判断获取状态\n如果成功获取到了任务，那么直接返回，否则：\n如果 runningTaskQueue 为空，说明当前阶段任务全部执行完毕，进入下一阶段 否则，说明还没有全部执行完毕，让当前 worker 等待 无锁编程 Coordinator 对 Worker 提供的 FetchTask 接口肯定是并发执行的，如果我们直接将上面的逻辑写在 FetchTask 中，会不可避免的加锁保护 Coordinator 的临界资源\n怎么实现无锁编程？\n「串行化」\n如果只有一个 goroutine 处理上面的事情，不就可以避免加锁了吗\n因此，在 Coordinator 刚启动时，启动一个后台 goroutine（即 scheduler），负责整体的调度\n然后，让 rpc 接口与 scheduler 使用 channel 进行 goroutine 的通信\n部分定义 type FetchMsg struct { args *FetchTaskArgs reply *FetchTaskReply done chan struct{} } const MAX_TASK_ID = 1 \u0026lt;\u0026lt; 31 type Coordinator struct { // Your definitions here. waittingTaskQueue []Task runningTaskQueue []Task finnishTask map[int]struct{} nReduce int nMap\tint taskID int // next TaskID fetchMsgChan chan FetchMsg doneChan chan struct{} done atomic.Bool status StatusType } // // create a Coordinator. // main/mrcoordinator.go calls this function. // nReduce is the number of reduce tasks to use. // func MakeCoordinator(files []string, nReduce int) *Coordinator { c := Coordinator{} // Your code here. // ... go c.schedule() // ... } // Your code here -- RPC handlers for the worker to call. func (c *Coordinator) FetchTaskHandler(args *FetchTaskArgs, reply *FetchTaskReply) error { msg := FetchMsg{ args: args, reply: reply, done: make(chan struct{}), // no buffer } // notify c.schedule c.fetchMsgChan \u0026lt;- msg // block, until c.schedule done \u0026lt;- msg.done return nil } // // Schduler // Responsible for worker scheduling // func (c *Coordinator) schedule() { for { select { case msg := \u0026lt;- c.fetchMsgChan: c.doSchedule(msg) msg.done \u0026lt;- struct{}{} case \u0026lt;- c.doneChan: // exit c.done.Store(true) return } } } 正确性验证 执行 100 次 test-mr-many，全部通过：\n总结 MapReduce 是一种分布式编程模型，用于大规模数据处理。它由 Google 开发，利用分布式计算的优势，将大规模数据分成小的块，然后在每个块上应用 Map 函数和一个 Reduce 函数，最后将结果合并。\nMapReduce 的核心组件包括 Worker 和 Coordinator，Worker 负责执行 Map 和 Reduce 函数，Coordinator 负责协调所有 Worker 的运行。\n参考资料 MapReduce: Simplified Data Processing on Large Clusters 6.5840 Lab 1: MapReduce Lecture 01 - Introduction ","permalink":"https://blogs.skylee.top/posts/distributed-system/mit-6.824/lab1/note/","tags":["分布式","MIT-6.824"],"title":"MIT-6.824: Lab 1: MapReduce"},{"categories":["Distributed-System","Kafka"],"content":"作为 MQ 的代表，Kafka 天生就是分布式的，支持服务注册、发现、请求动态路由、数据自动分片、主从复制、故障转移、分布式事务\n结构 每个 Topic 的 Partition 在多个节点冗余存储，避免单点故障，保证可用性 对于某一个 Partition 来说，其读写操作都在同一个 Broker 上进行 Broker 之间相互关联，分享集群状态，支持故障转移 如何实现服务注册、服务发现 Broker 如何发现彼此 与 Redis 类似，向现有集群添加一个新的 Broker，只需要指定 ZooKeeper 的地址即可\n这是怎么做到的？\n来看看具体步骤：\n初始化 ZooKeeper Client Broker 加入集群，尝试在 ZooKeeper 将自己注册为 Controller 如果注册失败，说明集群中已经有 Controller 了，向 Controller 请求集群的元数据（Metadata） 有了元数据，这个 Broker 就知道其它 Broker 的网络位置、以及负责的 Partition 了 Kafka Controller 在集群中唯一存在，负责处理 ZooKeeper 上的事件，以及管理集群的 Metadata，从而减轻 ZooKeeper 的负担，使整个集群尽可能的脱离 ZooKeeper\nProducer 怎么发现 Broker 在配置 Producer 时，只需要指定集群中任意一个 Broker 的网络地址即可\n发现其它 Broker 的具体实现也是依托于前面提到的 Metadata\nProducer 第一次会尝试与 Broker 建立连接，由于每个 Broker 都保存了集群的 Metadata，因此可以直接返回 Metadata 给 Producer\n这样，Producer 就知道其它 Broker 的网络位置、以及负责的 Partition 了，下次生产数据时，就可以直接与对应的 Broker 建立连接\n当然，Producer 手中的 Metadata 可能过期，当某次发送数据失败时，Producer 会尝试更新 Metadata，进而将 Message 发送给正确的 Broker\nConsumer 怎么发现 Broker Consumer 发现 Broker 的方式与 Producer 类似：\n与任意 Broker 建立连接，并带上所属 Group-IDs Broker 计算 Group-IDs 由哪些 Broker 管理，并返回 Group Coordinators 的网络位置 拿到 Group Coordinators 的网络位置后，Consumer 就可以给 Group Coordinators 发送 join group 请求\nGroup Coordinators 会与 Consumer Group 的 Leader 通信\nConsumer Group Leader 会重新分配每个 Consumer 消费的 Topic 以及 Partition，并将分配结果发送给 Group Coordinators\nGroup Coordinators 负责将分配结果分发给各个 Consumer，包括新加入的 Consumer\n这样，每个 Consumer 就知道了自己要消费的 Topic、Partition，以及 Partition 所属 Leader 的网络位置\n如何将请求路由到正确的节点 Producer 对于 Producer 来说，第一次请求后保存了 Metadata，后续请求就可以根据 Metadata 来将请求路由到正确的 Broker\n当然，Metadata 可能过期，如果某一次生产数据失败了，Producer 会请求更新 Metadata，然后再重新发送\nConsumer 对于 Consumer 来说，在 join group 以后，就拥有了 Consumer Metadata\n消费数据时，可以从 Metadata 中获取要消费的 Topic、Partition 以及对应的 Broker 的网络位置，从而将 Fetch 请求路由到正确的 Broker\nC、A 如何做权衡（trade-off） 一致性保障 对于同一个 Partition 而言，读写都是在 Leader 上进行的，不支持读写分离\n从这里看出，Kafka 还是 更倾向于一致性保障（C），避免在多个 Broker 间读取产生的不一致问题\n但是这种方式，Leader 的读写压力可能较大\n事实真的如此吗？\n我们知道：一个 Topic 是有多个 Partition 的，这样就间接减少了单个 Broker 的读写压力\n可用性保障 Kafka 的可用性保障，主要体现在一个 Partition 会冗余存储在多个 Broker 上，这样即使 Leader 挂了，也有 follower 来顶上\nACK 机制 我们能够通过配置 ACK 的值，来进一步做 C、A 的权衡：\nacks=0（Fire and Forget）： 生产者发送消息后 不等待 服务器的响应，直接认为消息发送成功。（可用性保障） acks=-1（Full Ack）： 生产者在消息被 写入主题的所有副本（leader 和所有 follower）并得到确认后才认为消息发送成功。（一致性保障） acks=1（Leader Ack）： 生产者在消息被写入主题的分区 leader 后会收到服务器的确认。（比较均衡） 如何做数据分片 Kafka 数据分片体现在 Partition 上\n每个 Topic 被分成了多个 Partition\n每个 Partition 有一个 Leader，多个副本\n这样，对于单个 Topic 来说，读写请求就被分散到多个 Broker 上\n并且，对于单个 Partition 来说，读写请求在同一个 Broker 上，保障了一致性\n可以看出，相较于传统的读写分离方式，Kafka 这种分片方式，既可以分散读写请求，还能保证数据的一致性\n数据如何同步（复制）到各个节点 关键概念：\nack 推 vs. 拉 ISR LW/HW/LEO 逻辑：\nleader 写完 local log 后，leader LEO++ 判断 producer 的 required ack 的值，如果为 0 或 1，立即调用 callResponse（返回结果给 producer），否则： 调用 delayCallResponse，即等待所有的 replica 都同步完毕后，返回 resp delayCallResponse 做的事情：\n检查当前是否同步完毕，如果完毕，resp，否则： 监听（watch）leader 该 partition 的 HW（HW 会随着所有 replica 的同步动态更新） 是否达到了 requiredOffset（即该消息在队列的 offset），如果达到了： 判断当前 ISR 与 minISR 的关系，ISR \u0026gt;= minISR（保证可用性，minISR 默认值为 1，生产环境建议修改），resp Follower 同步数据时，采取的是「拉」还是「推」 Kafka 选择的是「拉」这个策略，具体来说，Follower 会有一个后台线程，定期在 Leader 拉取新的 Msg\n为什么不使用「推」，即让 Leader 主动推送 Msg 给 Follower 呢？\n让 Leader 推 Msg 给 Follower，如果推送数据太快，Follower 的写压力会很大\n而 Follower 也是一个 Broker，它可能也是若干个 Partition 的 Leader，如果 Follower 把大量资源分配给同步数据上，那么它所 Lead 的 Partition 可能读写性能就不太理想了\n因此，采取「拉取」这种策略，将同步速率决定权交给 Follower，优先保证 Follower 可以处理它所 Lead 的 Partition 的读写请求\n这实际上是优先保证了可用性，而放弃一些一致性保障（如果 Leader 挂了，并且 ACK != all，那么会丢失一些数据）\n故障转移 Kafka 的 failover 是一个可以聊很多的话题\n故障的产生，不一定是节点本身宕机，也有可能是网络原因\nZookeeper Failover Kafka 对 ZooKeeper 是强依赖的，如果 ZooKeeper 挂了，会导致：\n无法添加新的 Broker：连接不上 ZooKeeper，进而找不到 Controller，拿不到 metadata 无法选举新的 Controller ZooKeeper 的故障转移过程这里就不再赘述\n麻烦的是 ZooKeeper hang，也就是 ZooKeeper 没有挂，但是整体响应速度很慢，会导致 ZK 和 Brokers 之间 Session Timeout（由 ZK 引起的）\n进而，ZooKeeper 会 错误 认为：\nController 挂了 Broker 挂了 这两种情况都会带来 Kafka 集群 不必要 的故障转移\n导致 ZooKeeper hang 住的原因主要是 ZK 机器的压力太大，负载过高\n因此，应该尽量减少对 ZK 的负担（Kafka 也意识到了这个问题，并在逐步减少对 ZK 的依赖，最新版本甚至不需要 ZK）\nController Failover Controller 发生故障，会导致集群：\n无法添加新的 Topic 无法进行 Partition 重分配 无法完成 Partition Leader 的选举 由于 Kafka 集群只有一个 Controller，因此存在单点故障问题\n当 Controller 挂掉的时候，会触发 failover 机制，选举出新的 Controller 进行工作。\n检测 每个 broker 会 watch ZK 的 /controller 目录\nZK 会与 Controller 建立心跳机制，如果超时，ZK 就认为 Controller 挂掉，会删除 /controller 目录\n其它 broker 会监听到这一事件，并开始 Controller 的选举过程\n转移 选举过程具体来说，与一个 broker 加入集群的步骤类似：\n试图去在 /controller 目录抢占创建 ephemeral node； 如果已经有其他的 broker 先创建成功，那么说明新的 Controller 已经诞生，更新当前的元数据即可； 如果自己创建成功，说明我已经成为新的 Controller，下面就要开始做初始化工作， 初始化主要就是创建和初始化 partition 和 replicas 的状态机，并对 partitions 和 brokers 的目录的变化设置 watcher。 新的 Controller 选举出来以后，如果原来的 Controller 重新上线，集群中就会出现多个 Controller，就是俗称的 脑裂 现象\n每当新的 Controller 产生的时候就会在 zk 中生成一个全新的、数值更大的 Controller epoch 的标识，并同步给其他的 broker 进行保存，这样当另一个 Controller 发送指令时，较小的 epoch number 请求就会被忽略。\nBroker Failover 这里讨论的 Broker，指的是一个 Partition 的 Leader\n一个 Partition 会有一个 Leader 和多个 Follower，Leader 负责 Partition 的全部读写操作\n当一个 Leader 挂了，为了不让这个 Partition 离线（Offline），需要在 Follower 中选举一个来当新的 Leader\n检测 Broker Failover 的检测和转移全部都是由 Controller 完成的\nController 会 Watch ZooKeeper 的 brokers 目录\n当 ZK 认为一个 Broker 发生了 Session Timeout，会将失效的 Broker 标记为不可用，并将其状态设置为 DOWN。\n如果一段时间内，这个 DOWN 掉的 Broker 还没有没有恢复连接或重新注册，ZooKeeper 会删除对应的 brokers 目录（例如 brokers/ids/1）\n这一过程会被 Controller 监听到，于是 Controller 就知道哪个 Broker 出现了 Failure\n转移 Controller 会在 AR 集合 中按顺序选择一个 Broker，如果该 Broker 在该 Partition 的 ISR 集合 中，那么该 Broker 被选中\n被选中的 Broker 就会成为这个 Partition 新的 Leader，故障转移就完成了\n当然这里又可以在 C、A 之间做 trade-off：\nKafka 有一个配置项，允许 Unclean Election\n启用这个配置项，如果 ISR 中没有 broker，那么就会在没有 catch up 的副本集合中选举一个 Broker 作为 Leader\n这意味着会 丢失部分未同步的数据，是可用性（A）优先的一种体现\n为什么不采用「少数服从多数」的投票算法 在 Redis Cluster 的故障转移中，选举新的 master 节点采用的是「少数服从多数」的投票算法\n那 Kafka 为什么不使用呢？\n因为这种算法需要 较高的冗余度\n如果只允许一台机器失败，需要有三个副本；而如果只容忍两台机器失败，则需要五个副本。\n而 Kafka 的 ISR 集合方法，分别只需要两个和三个副本，可用性更高\n如何实现分布式事务 Kafka 天生就是分布式的，它提供的事务支持，自然也是分布式事务\nkafka 的事务是面向一个 producer 的 我们知道，kafka 的有序性建立在：单个 producer，partition 内部有序，多个 producer 的有序性无法保证\n讨论下面三个场景：\nN(N \u0026gt; 1) producer =\u0026gt; N(N \u0026gt; 0) Partition：无序性 1 producer =\u0026gt; 1 Partition：有序性 1 producer =\u0026gt; N(N \u0026gt; 1) Partition：原子性 第一个场景肯定要用到 分布式事务，多个 producer 之间的协调属于分布式事务问题，不在本次讨论范畴\n对于第二个、第三个场景，需求就是多条消息写入的原子性\n1 个 producer 向 N(N \u0026gt;= 1) 个 partition 生产一批数据（这个是由业务决定的），生产者希望这些数据要么整体都投递到 kafka 中，要么整体失败，即保证多条消息写入的原子性\n这个就是 kafka 事务的概念\n综上：Kafka 的事务是面向一个 producer 的\n多条消息发送事务 多个 producer 向同一个 partition 写入数据的情况是很常见的\n下面讨论这种情况（注意：这些 producer 之间没有任何关联）\n如果仅仅依靠 pid + seq，显然不能区分同一个 partition 内，来自不同 producer 的事务\nTransaction ID 为了解决上述问题，kafka 引入了 Transaction ID 的概念\n每个 producer 都有自己的 Transaction ID，这是一个配置项\n一个 producer 对应 一个 Transaction ID\nTransaction Marker 有了 Transaction ID 还不够，为啥？\n对于同一个 producer 而言，生产的 msg 的 Transaction ID 都是一样的，无法区分哪条消息处于哪个事务\n因此，为了区分不同的事务，Kafka 引入了 Transaction Marker 的概念\nTransaction Marker 包含了：\nTxID：事务 ID PID：生产者 ID Epoch：生产者版本号（后面会讲） Flag：标志，成功或者失败 为了保证性能，Kafka 在「回滚」事务时，不会真正的从磁盘中删除 Msg（随机 IO，慢） ，而是打上标记，代表该 msg 被「软删除」\nconsumer 在消费数据时，消费到一批数据，且 Transaction Marker 的 flag 字段表明该事务处于「回滚」状态，就 不会 将该消息提交给我们的应用程序\nProducer Epoch 讨论以下场景：\n一个 producer 在正常的生产数据，但是突然挂了，为了保证可用性，我们通常会再启动一个 producer，此时就会发生问题：\n发生了重复写入问题\n分析原因，其本质是多个 producer 写入消息。之前讨论过，Kafka 事务是针对单 producer 的\n为了解决这个问题，Kafka 引入了 Producer Epoch（版本号）的概念\nProducer Epoch，它是由 Kafka Producer 在创建事务时生成的。当一个 Producer 出现故障或重新初始化的时候，它的 Epoch 值会递增，Producer ID 通常保持不变。这个机制可以帮助 Kafka 避免一种称为 \u0026ldquo;zombie producers\u0026rdquo; 的情况，也就是当一个旧的、失败的 Producer 再次尝试向 Kafka 写入数据时可能导致数据重复的问题。\nbroker 对于相同的 Transaction ID，只会接受 Epoch 值更大的 producer 的写入请求\n当 producer 出现故障或者重新初始化时，Epoch 值递增\n总结 Kafka 天生就是分布式的，本身对分布式提供了较为完善的支持\n通过 ZooKeeper 进行服务注册与发现，Kafka 能够动态地将新的 Broker 添加到集群中，并在 Producer 和 Consumer 发送请求时根据集群的元数据进行动态路由，从而实现了服务的自动发现和请求的动态分发。 Topic 分片以及 Partition 副本确保了数据的可用性和容错性 故障转移机制则能够在 Controller 和 Partition Leader 出现故障时自动进行故障转移，保证了整个系统的稳定运行。 通过 Transaction ID 和 Transaction Marker 确保了事务的原子性和一致性 在一致性和可用性方面的权衡上，Kafka 偏向于保障一致性，但也能够通过：\n配置 ACK 的值 配置是否允许 Unclean Elect 来调整一致性和可用性之间的平衡。\n参考资料 Apche Kafka 的生与死 – failover 机制详解 【Kafka】Leader选举（broker /分区） ","permalink":"https://blogs.skylee.top/posts/distributed-system/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9Fkafka/note/","tags":["分布式","Kafka"],"title":"分布式系统——Kafka"},{"categories":["Distributed-System","MySQL"],"content":"作为传统关系型数据库，MySQL 的分布式倾向于存储方面：将数据分散存储到各个节点，减少单个节点的读写压力、存储压力\n同时，MySQL 原生支持了主从复制，实现了数据的冗余存储，保障了可用性\n结构 数据分散存储到各个主节点，减少单个节点的读写压力、存储压力 从节点冗余存储主节点数据，保障了可用性，还可以实现读写分离，进一步减少主节点读压力 子集群之间没有直接关联，需要应用层实现 如何实现服务注册、服务发现 假设有一个主节点，我们要想为这个主节点添加从节点，需要在从节点上 手动 指定主节点的位置：\nCHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password master_auto_position=1 -- 指定使用 GTID 确定位点 指定以后，从节点就知道了主节点的位置，同样的，主节点也会知道从节点的位置\n但是主节点之间是无法发现彼此的，此外，对于应用层来说，MySQL 原生并不支持服务注册、服务发现，通常需要手动实现，或者引入一个 Proxy 层\n如何将请求路由到正确的节点 我们将请求分为两类：\n读请求 写请求 假设我们做了数据分片（分库分表），那么在执行读写请求前，就要先确定数据存储的位置\n实现方式有两种：\n应用层代码手动实现 引入 Proxy 层 应用层代码手动实现 应用程序主动做负载均衡，这种模式下一般会把数据库的连接信息放在客户端的连接层。也就是说，由客户端来选择后端数据库进行查询。\n这种方式的性能较好，缺点就是需要对已有代码进行修改\n基于 Proxy（代理层）实现 这个方式就是在 Client 到 Server 之间引入一个代理，自动分发客户端的请求：\nProxy 会负责请求的分发与负载均衡\n这种方式的优点就是不用修改已有代码，但是有一定的性能损耗\n如果要实现读写分离，同样的，也是上面两种方式\nC、A 如何做权衡（trade-off） 这里讨论的主要是读写分离的情况\n由于主从复制会存在延迟，在读写分离的情况下，我们就需要在 C、A 之间做 trade-off，通常有以下几种实现方式\n强制走主库 一些对数据一致性要求很高的业务（如金融），可以采取这种方式，保证绝对的数据一致性，纯粹的 C\n缺点就是：如果很多业务都有很高实时性要求，都走主库，会 对主库造成很大压力\n强制走从库 一些对数据一致性要求不高的业务（如互联网应用），可以采取这种方式，保证可用性，纯粹的 A\nSleep 既然同步需要时间，那我们可以「sleep」一下，等它同步完了再去访问不就好了吗\n例如，一个用户发布了一个商品，前端可以基于本地已有信息进行展示，就好像获取到了最新的数据一样\n用户待会刷新页面，其实已经过了一段时间，也就达到了 sleep 的目的，只要等待时间内同步完成，访问的就是最新的数据\n这种方式适用于对数据实时性要求低的业务，就算访问到过期数据也影响不大\n缺点就是：我们无法保证在规定时间内主从同步完毕，还是 有访问到过期数据的可能性\n判断主从无延迟方案\n这种方式比较折中\n在访问数据之前，判断主从是否同步完毕： 如果同步完毕，那么走从库，获取的是最新数据 如果没有，可以等待一段时间，重复第一个步骤 如果等待时间 超过一定阈值，那么根据实际需求，看看是走主库，获取最新数据；或者走从库，获取过期数据 如何做数据分片 我们可以将数据分散存储到不同的主节点上\n通常的实现方式有：\n哈希分片 一致性哈希 范围分片 哈希分片\n根据指定 key（比如 id） 的哈希值，算出这个数据在哪个库（表）中\n哈希分片算法比较适合 随机 查询的场景，并能 一定程度避免单个库上的热点问题，不适合范围查询\n范围分片\n按照指定范围区间来分配数据，例如，将 id 为 1 ～ 10000 的存在第一个 DB，id 为 10001 ～ 20000 的存在第二个 DB\n范围分片算法比较适合 范围 查询的场景，但有可能 存在 单个库上的 热点问题\n数据如何同步（复制）到各个节点 这里讨论的是 MySQL 的主从复制\n简单来说，就是从库的 IO_Thread 将主库发来的 binlog 写到本地的 relay log，然后由 SQL_Thread 负责拉取 relay log 完成主从复制\nbinlog 是主库主动推送，还是从库主动拉取？\n一开始创建主备关系的时候，是由备库指定的。\n比如基于位点的主备关系，备库说“我要从 binlog 文件 A 的位置 P”开始同步， 主库就从这个指定的位置开始往后发。\n而主备复制关系搭建 完成以后，是 主库 来 决定“要发数据给备库” 的。\n所以主库有生成新的日志，就会发给备库。\n如何实现故障转移 主节点挂了，从节点怎么办？\nMySQL 没有自动的故障转移，需要手动转移\n大致步骤如下：\n选择一个从库 将其设置为新的主库 修改剩余从库的 master 为新的主库 但问题是：其它从节点应该怎么去同步新的主节点的数据呢？\n难点：从库去同步新的主库的数据，不是全量同步，而是增量同步，如何寻找这个同步的「起始点」？\nMySQL 5.6 引入了 全局事务 ID（GTID），是一个事务的唯一标识\n在 GTID 模式下，从库获取新的主库的位点这件事情就在 MySQL 内部做好了：\nCHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password master_auto_position=1 -- 指定使用 GTID 确定位点 想要更深入的了解 GTID 的实现，可以看看这篇文章：27 | 主库出问题了，从库怎么办？ 如何实现分布式事务 MySQL 使用两阶段提交（2PC）实现分布式事务：\n两阶段提交将 事务提交过程 分为两个阶段，保证主从节点间数据的一致性：\n准备阶段（prepare） 提交阶段（commit） 使用两阶段提交时，MySQL 内部开启一个 XA 事务：\n在 prepare 阶段，将 XID 写入 redo log，并 将 redo log 状态设置为 prepare，然后调用 write、fsync（类似于 innodb_flush_log_at_trx_commit = 1），将 redo log 持久化到磁盘\n在 commit 阶段，将 XID 写入 binlog，然后调用 write、fsync（类似于 sync_binlog = 1），将 binlog 持久化到磁盘，并 将 redo log 状态设置为 commit\n当然，上面所指的事务，涉及到的数据应该仅在同一个主库中，分布式体现在 一主多从\n如果涉及到多个主库（多主多从），MySQL 原生就无能为力了，可以使用：\n基于消息队列的异步处理：将事务操作写入消息队列，由消费者进行处理。这种方式可以利用 MQ 的事务机制实现跨多个数据库的分布式事务，但可能会引入一定程度的异步性和延迟。 分布式事务中间件：使用第三方的分布式事务中间件（如 Seata）来实现分布式事务。这些中间件通常提供了对分布式事务的原生支持，能够简化开发和管理复杂的分布式事务场景。 应用程序层面的解决方案：在应用程序层面实现分布式事务逻辑。例如，通过在应用程序中使用 分布式锁、分布式协调服务（如 ZooKeeper）、分布式一致性算法（如 Paxos 或 Raft）等技术来保证事务的一致性。 总结 MySQL 作为传统关系型数据库的代表，其在分布式上的实现 并不完善，原生仅支持主从节点的复制\n","permalink":"https://blogs.skylee.top/posts/distributed-system/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9Fmysql/note/","tags":["分布式","MySQL"],"title":"分布式系统——MySQL"},{"categories":["Distributed-System","Redis"],"content":"作为 K-V 存储的代表，Redis 原生支持了集群模式（Redis Cluster），支持服务注册、发现、请求动态路由、数据自动分片、主从复制、故障转移\n结构 数据分散存储到各个主节点，减少单个节点的读写压力、存储压力 从节点冗余存储主节点数据，保障了可用性，还可以实现读写分离，进一步减少主节点读压力 主节点之间互相关联，分享集群状态，支持故障转移、副本迁移 如何实现服务注册、服务发现 首先看看怎么向已有集群添加一个主节点\nredis-cli --cluster add-node \u0026lt;new_host:new_port\u0026gt; \u0026lt;existing_host:existing_port\u0026gt; 可以看到：只需要指定集群中任意一个节点的 host+ip 即可，那么新的节点是怎么发现其它节点的位置的呢？\n实际上，集群中的每一个节点都会缓存 节点与 host 的映射关系，添加新节点时，会与集群中任意一个节点建立 TCP 链接，该节点会发送 节点与 host 的映射关系给新的节点\n这样，新的节点就知道其余节点的网络位置了\n节点间怎么通信的？ 和 Redis-Server 与 Redis-Client 的 RESP 通信协议不同，集群中任意节点通过 Cluster Bus（集群总线）通信\nCluster bus protocol: a binary protocol composed of frames of different types and sizes. Every node is connected to every other node in the cluster using the cluster bus.\nCluster Bus 是基于 Gossip 协议的\n什么是 Gossip 协议？ Gossip 协议是一种在分布式系统中用于节点间通信的协议，它通过 流言蜚语（即 Gossiping）的方式来交换信息，从而实现信息的传播和一致性。\nGossip 协议与我们平常所说的流言传播相似，一个节点会 随机选择其他几个节点 分享信息，这些被选择的节点又会同样选择其他节点进行信息的传播，这样信息就 像病毒一样快速扩散开 来。\nGossip 的特点就是 去中心化，具有可伸缩性和鲁棒性\n为什么 Redis Cluster bus 要使用 gossip 协议呢？\n来看看 Redis 官方怎么说的：\nWhile Redis Cluster nodes form a full mesh, nodes use a gossip protocol and a configuration update mechanism in order to avoid exchanging too many messages between nodes during normal conditions, so the number of messages exchanged is not exponential.\n在一个大型集群中，如果节点需要以直接的方式交换状态信息或配置更新，那么每个节点都需要与其他每个节点进行通信。这意味着在 N 个节点的集群中，每个节点都需要发送 N-1 个消息来通知每个其他节点，总共就有 N*(N-1) 次通信。对于大型集群，这个数字会非常大，随着节点数目线性增加，需要的通信次数会以平方级别(N 的平方)增长\n如果使用 gossip 协议，每个节点发送数据，仅仅会随机选择一些节点发送，然后慢慢传播开来，就像流言（病毒）传播一样，这样，整个集群内的消息数量处于一个可控范围，不会造成太大的压力\n当然，gossip 协议存在 消息不及时 的缺点：消息在节点间传播是需要时间的，经过多个节点的转发，时效性不如直接发送好（不过大型集群，直接发送的压力太大，可能造成网络拥塞，时效性可能还不如 gossip）\n总结 通过缓存 节点与 host 的映射关系，以及基于 Gossip 的 Cluster Bus 通信机制，Redis Cluster 实现了服务注册与服务发现\n如何将请求路由到正确的节点 这里假设你知道了 Redis Cluster 数据分片的方式 客户端 第一次 向集群发起请求时，只需要请求任意一个节点 就可以了，Redis Cluster 实现了路由的动态重定向\n具体来说：\n客户端第一次向集群发起请求时，请求任意一个节点 该节点会「判断」请求的数据是否存储在本节点，如果是，直接响应 否则，给客户端发送 MOVED 重定向 ，告诉客户端应该请求哪个节点 无论请求的数据是否存储在本节点，都会发送一份 hash slot 到 cluster node 的映射 给客户端 客户端会 缓存 这个映射关系，下一次请求就有很大概率直接请求到正确节点，避免重定向 可以看出，与 MySQL 不同，Redis 原生支持了请求的动态路由\nC、A 如何做权衡（trade-off） 这里讨论的是读写分离的情况\n默认情况下，即使一个节点有若干个从节点，读请求也不会在从节点进行，如果客户端尝试在从节点读取数据，会收到一个 MOVED 重定向\n也就是说，Redis Cluster 默认情况下，选择的是 C，即一致性保障\n但是我们也可以更改配置：READONLY\nhowever clients can use replicas in order to scale reads using the READONLY command.\nREADONLY tells a Redis Cluster replica node that the client is willing to read possibly stale data and is not interested in running write queries.\n要实现这一点，客户端可以使用 READONLY 命令通知从节点接受读请求。一旦进入只读模式，客户端就可以向该从节点发送读请求，读取存储在该节点的数据。\n当客户端给从节点发送了 READONLY 命令后，从节点之后再收到读请求，只要这个 key 对应的 hash slot 由自己的 master 负责，那么就不会重定向客户端的请求，而是自己处理，降低 master 的压力\n启用 READONLY，说明选择的是 A，即可用性保障\n注意\n虽然 Redis 支持读写分离，但是请求的负载均衡是不被原生支持的，这意味着 读请求的负载均衡需要在应用层手动做\n如何做数据分片 实现数据的分片，依赖的就是 key 分发模型\nRedis 给整个集群分配了 16384 个哈希槽（hash slots），每个 cluster 节点都拥有一部分 hash slots\n确定 key 在哪个 hash slot，这个过程使用 CRC16 算法，具体来说：\nhash_slot = CRC16(key) % 16384 在客户端读写数据时：\n先计算出 hash slot 读取本地缓存，获取这个 hash slot 在哪个 cluster 节点上存储 请求对应的 cluster 节点 cluster 节点收到数据后：\n校验一下这个 key 是否在当前 cluster 节点 如果在，那么处理客户端的请求即可 如果不在，读取本地缓存，确定这个 hash slot 在哪个 cluster 节点上存储，并发送 MOVE 错误给客户端，以重定向到正确的 cluster 节点 hash slots 和一致性哈希思想上是一样的，都是尽可能的将数据分散来存储，避免热点问题，虽然范围查询能力不太好，但是作为 K-V 存储的 Redis 来说，范围查询的需求不是特别高\n数据如何同步（复制）到各个节点 这里讨论的是 Redis 的主从复制\n第一次同步（全量复制） 从节点第一次同步主节点的数据的过程如下：\n从节点向主节点发起同步请求(PSYNC)，并携带主服务器的 server_id（这里为？）、offset（这里为 -1） 主节点收到请求后，发送自己的 server_id 和 offset 给从节点 从节点记录下主节点的 server_id 和同步的 offset 主节点 fork 一个子进程后台生成 RDB 文件，发送给从节点 从节点收到 RDB 文件后，开始同步主节点数据 这个过程存在一个问题：在生成 RDB 期间，如果有新的写命令，是不会包含在 RDB 中的，那么从节点如何获取这些增量数据呢？\n为了解决这个问题：\n主进程会在生成 RDB 期间，将 增量写命令 写到 replication buffer 中 从节点同步完 RDB 以后，会给主节点发送同步完成的消息 主节点收到同步完成的消息以后，会将 replication buffer 的命令发给从节点 从节点重放 replication buffer 的命令，第一次同步完成 图片来自小林 coding 增量同步 第一次同步结束后，从节点与主节点会建立 TCP 长连接，用于同步增量数据\n当主节点执行了一个写命令后：\n主节点将该命令添加到 repl_backlog_buffer 中； 主节点通过已经建立的 TCP 长连接，将这个写命令发送给所有的从节点； 从节点接收到这个命令后，会将其放入自己的本地队列中，然后按序执行这个命令来更新自己的数据集； 主节点在发送命令后不会等待从节点的响应，它会继续处理自己的操作请求。 如果同步进度差距太大，怎么办？\n由于网络问题，从节点的同步进度较慢，落后的数据可能较多\n由于 repl_backlog_buffer 是一个环形缓冲区，如果落后的数据被覆盖了，就只能重新做全量同步\n如何实现故障转移 Redis Cluster 的故障转移是基于心跳机制实现的\n为了检测整个集群的状态，节点间会定期地互相发送心跳包（Heartbeat Packet），包含了节点本身的元数据，以及 发送节点视角下的集群状态信息\n这里详细说一下 发送节点视角下的集群状态信息，对于后面理解 Redis Cluster 的错误检测会有帮助：\n当一个节点发出心跳包时，它会在包里面包含它所观察到的集群状态的信息。这有助于其他节点获得关于集群健康状况的信息，比如：\n下线状态（down）：如果发送心跳包的节点观察到集群或者特定的节点出现了问题（比如无法达到或不再发送心跳信号），它会在心跳包中报告该节点或集群处于 down 状态。 正常状态（ok）：相反，如果发送心跳包的节点认为集群状态良好，所有节点都是活跃的并且响应心跳信号，那么它会报告集群状态是 ok 的。 这样的设计能让集群中的其他节点根据接收到的心跳信息来更新自己的状态视图，从而使整个集群能够对节点失效做出快速响应，并相应地进行故障转移或重组。\n检测 当节点超过 NODE_TIMEOUT 时间无法访问时，其他节点会给该节点标记 PFAIL。 无论是主节点还是副本节点，都可以为其它类型的节点标记 PFAIL。Redis 集群中节点不可达的概念是指我们发送了 ping，但在 NODE_TIMEOUT 时间内还未收到回复。\n一个节点单独的 PFAIL 标志只是该节点关于其他节点的本地信息，不足以触发故障转移。要视为节点已经 down 掉，需要将 PFAIL 状态升级为 FAIL。\n那么如何将 PFAIL 状态升级为 FAIL？\n升级需要满足三个条件：\n某节点（称为 A）将另一个节点（称为 B）标记为 PFAIL。 通过心跳检测机制，A 获取到了其它主节点对于 B 是否为 down 的意见 大多数的 有效（在 NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT 内报告 PFAIL 或者 FAIL 状态） 意见都认为这个节点为 down 那么：\n节点 A 将 B 标记为 FAIL 给其它节点发送 FAIL message FAIL message 会强制其它节点将 B 标记为 FAIL\n转移 当一个主节点被标记为 FAIL，并且 有可用的从节点，那么故障转移可以进行\n故障转移的过程是这样的：\n首先，每个从节点都是新的主节点的候选者\n从节点会递增 currentEpoch（投票轮数）\n每个从节点会通过 cluster bus 发送 FAILOVER_AUTH_REQUEST 包，请求其它主节点给自己投票\n每个主节点在 NODE_TIMEOUT * 2 时间内，只有一次投票机会（防止多个从节点当选），当主节点收到 FAILOVER_AUTH_REQUEST 包后，如果有投票机会，就会给这个从节点投票（发送 FAILOVER_AUTH_ACK）\n从节点会 抛弃 不属于当前 currentEpoch 的投票\n当一个从节点收到大多数主节点的投票后，它就成为了新的主节点\n如果超过 NODE_TIMEOUT * 2 时间，还没投票完毕，本轮投票失败，等待 NODE_TIMEOUT * 4 开启新的一轮投票\n一旦选举出新的主节点，其他节点会被通知这一变更，确保整个集群中所有节点的配置信息保持一致。\n旧的主节点重新上线，不再担任主节点的角色，而是成为新的主节点的从节点\n此外，Redis 还引入了一个 副本迁移机制\n当一个主节点没有从节点时，副本迁移机制就会起作用：\n选择某个拥有最多从节点的主节点 将该主节点的部分从节点迁移到孤儿主节点，成为孤儿主节点的从节点 副本迁移可以进一步减少整个集群不可用的情况\n如何实现分布式事务 分两种情况：\n如果事务内涉及的 Key 均在同一个节点上 如果事务内涉及的 Key 不在同一个节点上 对于第一种情况，比较简单，使用 Redis 原生的事务就可以\n对于第二种情况，只能引入第三方中间件，来实现多节点要么同时成功，要么同时失败\n总结 作为 K-V 存储的代表，Redis 原生支持了集群模式（Redis Cluster）\n要想进一步了解 Redis Cluster，可以看看这篇文章：Redis 集群 ，也可以直接看 Redis 官方文档：Redis cluster specification ","permalink":"https://blogs.skylee.top/posts/distributed-system/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9Fredis/note/","tags":["分布式","Redis"],"title":"分布式系统——Redis"},{"categories":["Distributed-System"],"content":"分布式系统是由 多个 独立的计算机或者节点组成的系统，这些计算机通过网络进行通信和协作，共同完成特定的任务或提供服务。\n分布式系统的设计目标是：提高系统的性能、可伸缩性、可靠性和可用性，以及实现系统资源的有效利用。\n接下来，主要围绕 如何学习分布式系统 来讨论\n分布式系统解决了什么问题？ 为什么要引入分布式系统？\n在传统单体架构下，存在以下问题：\n可用性低：一个机器挂了，整个服务就不可用了 扩展性成本高：单个机器的性能提升是比较有限的，升级成本高 因此，引入分布式系统，来解决单体架构下的 可用性问题 和 扩展成本问题\n那么分布式系统是如何解决这两个问题的呢？\n简单理解：就是通过网络，将许多物理机连起来，组成一个集群，对外提供服务，并通过在系统中提供冗余来保障可用性\n分布式系统带来了什么问题 整个分布式系统具有多个物理节点，那么如何协调这些节点，就是一个比较重要的问题\n我们将分布式系统分为两类：\n分布式计算 分布式存储 分布式计算带来了什么问题 如何找到服务 在整个分布式系统中，物理节点间的身份可能不同\n那么节点间如何发现彼此呢？\n常见的解决方案是「服务注册与发现」\n实现这个功能，通常需要一个「服务注册中心」\n服务注册：当一个服务启动时，它会向服务注册中心注册自己的信息，包括服务名称、IP 地址、端口号等。注册中心会将这些信息保存起来，以便其他服务或客户端查询。 服务发现：当一个服务需要调用其他服务时，它会向服务发现组件发送查询请求，询问特定服务的位置信息。服务发现组件会从注册中心获取服务的信息，并将其返回给请求方，使得请求方能够直接与目标服务通信。 如何找到（实例）节点 确定了服务以后，如何确定将当前请求路由到哪个节点上呢？\n分两种情况：\n无状态：同一个服务之间，每个（实例）节点完全一致，例如 Redis 集群，主从节点存储数据（几乎）一致 有状态：同一个服务之间，（实例）节点不完全一致，例如 Redis 集群，主节点之间存储的数据不一致 对于「无状态」这种情况，做好普通的负载均衡就 ok 了，例如：\n简单的轮询机制 一致性哈希 fair 公平调度 对于「有状态」这种情况，由于节点间不一致，这种情况通常需要一个「路由服务」，因提前确定要请求的数据在哪个节点上，然后再访问\n如何避免服务雪崩 一个服务内，某个节点挂了，那么该节点负责的请求就会打到其它节点上\n如果剩余节点中，有一个节点因为承受不了多的请求，挂了，那么该节点负责的请求又会打到其它节点上\n如此循环，导致整个服务的节点依次挂掉，就像「雪崩」一样\n如何避免？\n整体分为两个思路：\n快速失败与降级机制：服务降级、熔断机制、限流 弹性扩容：添加新的节点到服务中 快速失败会导致部分请求无法正确响应\n而弹性扩容成本较高，并且扩容这个过程本身需要时间\n如何监控告警 对于分布式系统，如果我们不知道整个系统的负载情况（状态），那么整个系统的可用性是无法得到保障的\n可以监控某个接口的时延、成功响应数\n也可以监控整个系统的负载，并在高负载时告警\n分布式存储带来了什么问题？ CAP 理论 CAP 理论是分布式系统设计中的一个重要理论，描述了分布式系统中三个核心属性之间的冲突：\n一致性（Consistency）：即所有的节点在同一时间具有相同的数据视图。在一致性模型中，如果一个操作在一个节点上执行成功后，其他节点上的数据也应该立即变为一致。 可用性（Availability）：即系统能够对用户的请求做出响应，即使系统中的一部分组件出现故障。在可用性模型中，系统在出现故障时仍然能够继续对外提供服务。 分区容错性（Partition Tolerance）：即系统能够容忍网络分区（分布式系统中节点之间的通信可能出现故障），而不会导致系统的整体性能下降。在分区容错性模型中，系统能够在网络分区的情况下继续运行，并且不会受到太大影响。 一般来说，C 和 A 无法同时保障：\n保证强一致性 C，那么在数据同步期间，整个服务无法读取（避免读到过期数据）\n保证高可用性 A，那么即使数据未同步完毕，也要对外提供服务，弱一致性保障\n对于银行这类系统，一致性非常重要，保障 CP；\n对于互联网应用，可能不需要强一致性，最终一致就行，保障 AP\n如何做数据分片 随着数据量的增加，单机肯定无法存储所有数据，因此需要将数据分配到多个节点存储\n常见分片方式：\n范围分片 哈希分片 一致性哈希分片 如何做节点间数据复制 为了保障可用性：即使一个节点挂了，它所负责的数据也要能获取到\n可用性的保障，是基于数据的冗余存储的\n那么如何将数据复制到其它节点呢？\n中心化方案（主从复制、一致性协议比如 Raft） 去中心化方案 这两种方案，对外表现的一致性是不同的\n如何实现分布式事务 要实现事务，首先需要有对并发事务进行排序的能力，这样在事务冲突的时候，确认哪个事务提供成功，哪个事务提交失败。\n对于单机系统，只需要 时间戳 + TxID 即可\n但是分布式系统，每个节点的时间肯定无法完全同步，这种方式就不太行了\n不过可以 整体分布式，局部单体 来实现：用一台机器负责生成 时间戳 + TxID\n这种方式在节点到中心节点的 距离较短 还可以，但是，如果节点到中心节点的距离较长，那么请求的 TTL 可能就不太能够接受了，时间成本太高\n总结 学习分布式系统，可以从以下问题入手：\n如何实现服务注册、服务发现 如何将请求路由到正确的节点 如何监控集群状态 C、A 如何做权衡（trade-off） 如何做数据分片 数据如何同步（复制）到各个节点 如何实现分布式事务 进一步地，可以：\n从实践出发：学习 MySQL（传统关系型数据库）、Redis（分布式缓存）、Kafka（分布式 MQ）的分布式实现方式 从理论出发：学习 MIT-6.824 参考资料 掘金：如何系统性地学习分布式系统? ","permalink":"https://blogs.skylee.top/posts/distributed-system/intro/note/","tags":["分布式"],"title":"分布式系统"},{"categories":["Algorithm"],"content":"线段树是算法竞赛中常用的用来维护 区间信息 的数据结构。\n线段树可以在 O(log N) 的时间复杂度内实现：\n单点修改 区间修改 区间查询（区间求和，求区间最大值，求区间最小值）等操作。 建立线段树 先来看看线段树的结构\n假设有一个数组：[10,11,12,13,14]，那么建立好的线段树长这样：\n图片来自 OI-wiki 不难写出以下代码：\ntype SegmentTreeNode struct { sum int // 负责区间的和 left int // 负责区间左边界 right int // 负责区间右边界 lazy int // 懒标记 } type SegmentTree struct { nums []int tree []*SegmentTreeNode } func NewSegmentTree(nums []int) *SegmentTree { n := len(nums) st := \u0026amp;SegmentTree{ nums: nums, tree: make([]*SegmentTreeNode, n*4), } st.build(1, 0, n-1) return st } 具体地，build 的实现如下：\n// 根据给定数组建立线段树 func (st *SegmentTree) build(curNode, left, right int) { st.tree[curNode] = \u0026amp;SegmentTreeNode{} st.tree[curNode].left = left st.tree[curNode].right = right if left == right { st.tree[curNode].sum = st.nums[left] return } mid := left + (right-left)/2 st.build(curNode*2, left, mid) st.build(curNode*2+1, mid+1, right) st.tree[curNode].sum = st.tree[curNode*2].sum + st.tree[curNode*2+1].sum } 区间查询 要想查询一个区间的和，我们可以这样做：\n判断 当前节点负责区间 是否为 查询区间 的子集： 如果是，那么直接返回区间和即可 否则，递归的查询左右子节点 代码实现如下：\n// 获取 [left, right] 的和 func (st *SegmentTree) sumRange(curNode, left, right int) int { node := st.tree[curNode] // 当前节点的区间为查询区间的子集 if left \u0026lt;= node.left \u0026amp;\u0026amp; node.right \u0026lt;= right { return node.sum } // 懒标记下移 st.maintain(curNode) mid := node.left + (node.right-node.left)/2 res := 0 // 左节点负责区间 [node.left, mid] 与查询区间有交集 if left \u0026lt;= mid { res += st.sumRange(curNode*2, left, right) } // 左节点负责区间 [mid + 1, node.right] 与查询区间有交集 if mid+1 \u0026lt;= right { res += st.sumRange(curNode*2+1, left, right) } return res } 这里涉及到了一个操作：懒标记下移，我们待会再讲\n修改区间 要给区间的每一个数加上一个 offset，如果我们直接依次更新每一个节点，时间复杂度是无法承受的，因此，我们这里引入一个 懒标记 的概念：\n给区间的每一个数加上一个 offset，不是更新每个节点，而是直接修改 负责这个区间的根节点的 sum，并打上「懒标记」 那「懒」体现在哪里呢？\n修改时，不会修改每一个节点 当后续查询遍历到当前节点时，我们才将修改操作下沉到子节点 文字描述有点抽象，用代码来解释：\n// 给 [left, right] 内的数加上 offset func (st *SegmentTree) update(curNode, left, right, offset int) { node := st.tree[curNode] // 当前节点的区间为修改区间的子集 if left \u0026lt;= node.left \u0026amp;\u0026amp; node.right \u0026lt;= right { node.lazy += offset // 打上懒标记 node.sum += (node.right - node.left + 1) * offset return } // 否则，无法直接修改，继续遍历 // 先 maintain 一下 st.maintain(curNode) mid := node.left + (node.right-node.left)/2 // 左节点负责区间 [node.left, mid] 与查询区间有交集 if left \u0026lt;= mid { st.update(curNode*2, left, right, offset) } // 左节点负责区间 [mid + 1, node.right] 与查询区间有交集 if mid+1 \u0026lt;= right { st.update(curNode*2+1, left, right, offset) } // 更新和 node.sum = st.tree[curNode*2].sum + st.tree[curNode*2+1].sum } 可以看到，我们并不是直接修改每个节点的值，而是只修改了负责该区间的节点的 sum\n懒标记下移 懒标记下移是如何实现的？\n每次遍历到某一节点，如果该节点有懒标记，就需要下沉到子节点，具体地：\nfunc (st *SegmentTree) maintain(curNode int) { node := st.tree[curNode] if node.lazy == 0 || node.left == node.right { // 不需要懒标记下移 return } left := st.tree[curNode*2] right := st.tree[curNode*2+1] // 懒标记下移 left.lazy += node.lazy right.lazy += node.lazy // 修改左右节点的区间和 left.sum += (left.right - left.left + 1) * node.lazy right.sum += (right.right - right.left + 1) * node.lazy node.lazy = 0 } 完整代码 下面给出线段树的模版：\ntype SegmentTreeNode struct { sum int // 负责区间的和 left int // 负责区间左边界 right int // 负责区间右边界 lazy int // 懒标记 } type SegmentTree struct { nums []int tree []*SegmentTreeNode } func NewSegmentTree(nums []int) *SegmentTree { n := len(nums) st := \u0026amp;SegmentTree{ nums: nums, tree: make([]*SegmentTreeNode, n*4), } st.build(1, 0, n-1) return st } func (st *SegmentTree) SumRange(left, right int) int { return st.sumRange(1, left, right) } func (st *SegmentTree) Update(left, right, offset int) { st.update(1, left, right, offset) } func (st *SegmentTree) build(curNode, left, right int) { st.tree[curNode] = \u0026amp;SegmentTreeNode{} st.tree[curNode].left = left st.tree[curNode].right = right if left == right { st.tree[curNode].sum = st.nums[left] return } mid := left + (right-left)/2 st.build(curNode*2, left, mid) st.build(curNode*2+1, mid+1, right) st.tree[curNode].sum = st.tree[curNode*2].sum + st.tree[curNode*2+1].sum } // 获取 [left, right] 的和 func (st *SegmentTree) sumRange(curNode, left, right int) int { node := st.tree[curNode] // 当前节点的区间为查询区间的子集 if left \u0026lt;= node.left \u0026amp;\u0026amp; node.right \u0026lt;= right { return node.sum } // 懒标记下移 st.maintain(curNode) mid := node.left + (node.right-node.left)/2 res := 0 // 左节点负责区间 [node.left, mid] 与查询区间有交集 if left \u0026lt;= mid { res += st.sumRange(curNode*2, left, right) } // 左节点负责区间 [mid + 1, node.right] 与查询区间有交集 if mid+1 \u0026lt;= right { res += st.sumRange(curNode*2+1, left, right) } return res } // 给 [left, right] 内的数加上 offset func (st *SegmentTree) update(curNode, left, right, offset int) { node := st.tree[curNode] // 当前节点的区间为修改区间的子集 if left \u0026lt;= node.left \u0026amp;\u0026amp; node.right \u0026lt;= right { node.lazy += offset // 打上懒标记 node.sum += (node.right - node.left + 1) * offset return } // 否则，无法直接修改，继续遍历 // 先 maintain 一下 st.maintain(curNode) mid := node.left + (node.right-node.left)/2 // 左节点负责区间 [node.left, mid] 与查询区间有交集 if left \u0026lt;= mid { st.update(curNode*2, left, right, offset) } // 左节点负责区间 [mid + 1, node.right] 与查询区间有交集 if mid+1 \u0026lt;= right { st.update(curNode*2+1, left, right, offset) } // 更新和 node.sum = st.tree[curNode*2].sum + st.tree[curNode*2+1].sum } func (st *SegmentTree) maintain(curNode int) { node := st.tree[curNode] if node.lazy == 0 || node.left == node.right { // 不需要懒标记下移 return } left := st.tree[curNode*2] right := st.tree[curNode*2+1] // 懒标记下移 left.lazy += node.lazy right.lazy += node.lazy // 修改左右节点的区间和 left.sum += (left.right - left.left + 1) * node.lazy right.sum += (right.right - right.left + 1) * node.lazy node.lazy = 0 } 例题 LC.307. 区域和检索 - 数组可修改（模版题） ","permalink":"https://blogs.skylee.top/posts/algorithm/segment-tree/note/","tags":["线段树"],"title":"线段树"},{"categories":["Algorithm"],"content":"链接 常规解法 容易想到使用记忆化搜索：\ntype TreeAncestor struct { memo [][]int // memo[node][k] parent []int n int } func Constructor(n int, parent []int) TreeAncestor { memo := make([][]int, n) for i := 0; i \u0026lt; n; i++{ memo[i] = make([]int, n) for j := 0; j \u0026lt; n; j++ { memo[i][j] = -2 } } return TreeAncestor{ memo: memo, parent: parent, n: n, } } func (this *TreeAncestor) GetKthAncestor(node int, k int) int { if k \u0026gt;= this.n { return -1 } if k == 0 { return node } if this.memo[node][k] != -2 { return this.memo[node][k] } ancestor := -1 if father := this.parent[node]; father != -1 { ancestor = this.GetKthAncestor(father, k - 1) } this.memo[node][k] = ancestor return ancestor } 这种做法会 OOM，即使优化内存，也会 TLE\n倍增法 使用倍增法，思路如下：\n预处理，存储距离每个节点为 1、2、4\u0026hellip; 的节点\nfunc Constructor(n int, parent []int) TreeAncestor { memo := make([][]int, n) for i, father := range parent { memo[i] = make([]int, 0) memo[i] = append(memo[i], father) // memo[i][0] = father, dis = 2^0 = 1 } // 预处理 dis := 1 allNegative := false for !allNegative { allNegative = true // memo[node][dis] = memo[memo[node][dis - 1]][dis - 1] \u0026lt;= 先找到距离 node 为 2^(dis-1) 的节点 v1 // 再找距离节点 v1 为 2^(dis-1) 的节点 v2 for node := 0; node \u0026lt; n; node++ { v1 := memo[node][dis - 1] v2 := -1 if v1 != -1 { v2 = memo[v1][dis - 1] } if v2 != -1 { allNegative = false } memo[node] = append(memo[node], v2) } dis++ } return TreeAncestor{ memo: memo, } } GetKthAncestor 时，可以按照 k 的二进制位来分解查询\n例如，k = 7 = 1 + 2 + 4，二进制表示为：0000 0111\n那么，可以：\n先寻找距离当前 node 为 1 的节点 v1 再寻找距离 v1 为 2 的节点 v2 最后寻找距离 v2 为 4 的节点 v3 那么，v3 就是我们要寻找的节点\n相较于原来的暴力搜索（最坏需要 7 次查询），使用倍增法，只需要 3 次查询\n这种方式，每次查询最多 32 次（当然本题的数据范围，最多查询 16 次，取决于 k 中 1 的个数）\n完整代码如下：\ntype TreeAncestor struct { memo [][]int // memo[node][dis]: 距离 node 2^dis 的节点 } func Constructor(n int, parent []int) TreeAncestor { memo := make([][]int, n) for i, father := range parent { memo[i] = make([]int, 0) memo[i] = append(memo[i], father) // memo[i][0] = father, dis = 2^0 = 1 } // 预处理 dis := 1 allNegative := false for !allNegative { allNegative = true // memo[node][dis] = memo[memo[node][dis - 1]][dis - 1] \u0026lt;= 先找到距离 node 为 2^(dis-1) 的节点 v1 // 再找距离节点 v1 为 2^(dis-1) 的节点 v2 for node := 0; node \u0026lt; n; node++ { v1 := memo[node][dis - 1] v2 := -1 if v1 != -1 { v2 = memo[v1][dis - 1] } if v2 != -1 { allNegative = false } memo[node] = append(memo[node], v2) } dis++ } return TreeAncestor{ memo: memo, } } func (this *TreeAncestor) GetKthAncestor(node int, k int) int { res := node dis := 0 for k != 0 \u0026amp;\u0026amp; res != -1 { if dis \u0026gt;= len(this.memo[res]) { return -1 } if k \u0026amp; 1 != 0 { res = this.memo[res][dis] } dis++ k \u0026gt;\u0026gt;= 1 } return res } 整体还是 动态规划 的思想\n拓展 假设给你很多查询，每个查询都要寻找任意两个节点的 LCA，怎么办？\n如果按照 LC.236 的方式，每次都去查询一次，肯定超时\n我们还是可以利用倍增法来简化查询过程：\n先预处理一个 depth 数组，记录每个节点的深度 对于每一个查询 (x, y)，我们保证 y 的深度大于 x（可以 swap 一下） 利用 GetKthAncestor 来得到距离 y 为 depth[y] - depth[x] 的节点 z 这样，x、z 的深度相同，我们利用倍增法快速寻找 x、z 的 LCA： dis 从一个较大值开始 分别寻找距离 x、z 为 2^dis 的节点 v1、v2 如果 v1 == -1，说明 dis 太大，我们让 dis\u0026ndash; 如果 v1 == v2，说明 v1 就是 x、z 的共同祖先，但不一定是最近的，我们让 dis\u0026ndash;，看看能不能得到更近的 如果 v1 != v2，说明 dis 太小，共同祖先还在上面，我们让 dis++（当然代码实现与这里有点不同） 可以发现，还有一点 二分 的思想\n完整实现代码如下：\ntype TreeAncestor struct { memo [][]int // memo[node][dis]: 距离 node 2^dis 的节点 depth []int // depth[node]: node 的深度 } func Construct(n int, parent []int) TreeAncestor { return ConstructByParent(n, parent) } func ConstructByParent(n int, parent []int) TreeAncestor { edges := make([][]int, 0, n-1) for i := 0; i \u0026lt; n; i++ { if parent[i] != -1 { edges = append(edges, []int{parent[i], i}) } } return ConstructByEdges(edges) } func ConstructByEdges(edges [][]int) TreeAncestor { n := len(edges) + 1 graph := make([][]int, n) for _, edge := range edges { graph[edge[0]] = append(graph[edge[0]], edge[1]) graph[edge[1]] = append(graph[edge[1]], edge[0]) } // 初始化 depth 和 memo depth := make([]int, n) memo := make([][]int, n) var dfs func(cur, father int) dfs = func(cur, father int) { memo[cur] = make([]int, 0) memo[cur] = append(memo[cur], father) // memo[i][0] = father, dis = 2^0 = 1 for _, next := range graph[cur] { if next != father { depth[next] = depth[cur] + 1 dfs(next, cur) } } } dfs(0, -1) // 预处理 memo dis := 1 allNegative := false for !allNegative { allNegative = true // memo[node][dis] = memo[memo[node][dis - 1]][dis - 1] \u0026lt;= 先找到距离 node 为 2^(dis-1) 的节点 v1 // 再找距离节点 v1 为 2^(dis-1) 的节点 v2 for node := 0; node \u0026lt; n; node++ { v1 := memo[node][dis-1] v2 := -1 if v1 != -1 { v2 = memo[v1][dis-1] } if v2 != -1 { allNegative = false } memo[node] = append(memo[node], v2) } dis++ } return TreeAncestor{ depth: depth, memo: memo, } } func (this *TreeAncestor) GetKthAncestor(node int, k int) int { res := node dis := 0 for k != 0 \u0026amp;\u0026amp; res != -1 { if dis \u0026gt;= len(this.memo[res]) { return -1 } if k\u0026amp;1 != 0 { res = this.memo[res][dis] } dis++ k \u0026gt;\u0026gt;= 1 } return res } // 寻找任意两个节点的 LCA func (this *TreeAncestor) GetLCA(x, y int) int { depthX, depthY := this.depth[x], this.depth[y] if depthX \u0026gt; depthY { // swap x, y = y, x depthX, depthY = depthY, depthX } // 将 x、y 置于同一层 y = this.GetKthAncestor(y, depthY-depthX) if x == y { return x } // 逐层向上找（倍增） for dis := len(this.memo[x]) - 1; dis \u0026gt;= 0; dis-- { dx := this.memo[x][dis] dy := this.memo[y][dis] if dx == -1 { // dis 太大，减小 dis 重试（二分思想） continue } if dx == dy { // dis 太大，LCA 在下面，减小 dis continue } if dx != dy { // dis 太小，LCA 在上面，增加 dis // 这里并没有让 dis++，而是同时向上跳 2^dis 步 // 留给下一次循环判断 x, y = dx, dy } } return this.memo[x][0] } 事实上，GetLCA 方法不仅适用于二叉树，还可以用于多叉树\n参考资料 【模板讲解】树上倍增算法（以及最近公共祖先） ","permalink":"https://blogs.skylee.top/posts/algorithm/binary-tree/lc.1483.-%E6%A0%91%E8%8A%82%E7%82%B9%E7%9A%84%E7%AC%AC-k-%E4%B8%AA%E7%A5%96%E5%85%88/note/","tags":["动态规划","倍增","二叉树","二分"],"title":"树节点的第 K 个祖先"},{"categories":null,"content":"个人简介 平平无奇的大二码农\u0026hellip; 目前的目标是进入大厂实习\n记录这些博客，主要是为了后续查阅（好记性不如烂笔头），当然能够帮助到你就更好了\n时间线 2022.10 至 2022.11: 学习 C 语言，正式进入编程世界 2022.11 至 2022.12: 学习数据结构，并行学习 C++ 的基本语法 2022.12 至 2023.02: 学习基本算法，并开始 LeetCode 刷题之旅，同时学习 C++ 的 STL 2023.02 至 2023.05: 深入学习 C++，阅读了大部分 C++ Primer 中的内容 2023.06 至 2023.07: 学习操作系统的基本知识，以及 Linux 进程、线程、内存管理、IO 相关知识 2023.07 至 2023.08: 学习计算机网络 2023.08 至 2023.08: 学习 MySQL 数据库 2023.08 至 2023.09: 基于 OS 和 Network 的知识，使用 C++ 编写了一个简单的 RPC Framework 2023.09 至 2023.10: 转换方向，学习 Go 语言 2023.10 至 2023.12: 使用 Go 编写了一个 论坛：BlueBell 2023.12 至 2024.01: 考虑到 BlueBell 帖子创建、评论创建的写入高峰问题，了解到可以使用 MQ 削峰，于是开始学习 Kafka 2024.01 至 2024.01: 在 BlueBell 中引入 Kafka 2024.01 至 2024.02: 继续完善 BlueBell，并完成 BlueBell 的上线工作，第一次上线成功！ 2024.02 至今: 开始复习前面所学知识，准备寻找实习 联系方式 可以通过邮箱联系我：1350650238@qq.com\n也可以在 GitHub 上找到我：SkyLee424 ","permalink":"https://blogs.skylee.top/about/","tags":null,"title":"关于"},{"categories":["Golang"],"content":"Single Flight 是 Go pkg 中提供的一个工具，通常用于防止缓存击穿\n假设这个场景：\n大量客户端请求的缓存均过期了，不得不直接访问 DB，如此高的并发，很有可能将 DB 服务打垮\n引入 SingleFilght 工具，可以在一定程度上减缓这种情况\nPackage singleflight provides a duplicate function call suppression mechanism.\nsingleflight 包提供了重复函数调用 抑制 机制\n使用 SingleFlight 提供了以下 API：\nfunc (g *Group) Do(key string, fn func() (interface{}, error)) (v interface{}, err error, shared bool) func (g *Group) DoChan(key string, fn func() (interface{}, error)) \u0026lt;-chan Result func (g *Group) Forget(key string) Do 方法接收一个函数作为参数，返回函数的执行结果（v、err），以及这个结果是不是被多个 goroutine shared\nDoChan 在 Do 方法的基础上，引入了超时控制，返回一个 channel\nForgety 方法用于删除一个 key，防止获取过期的结果\n示例 0：基本使用 先来看看不用 SingleFlight：\npackage main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var ErrCacheMiss = errors.New(\u0026#34;cache miss\u0026#34;) func GetUserInfo() (any, error) { userInfo, err := ReadFromCache() if err != nil { userInfo, err = ReadFromDB() if err != nil { return nil, err } // 缓存回源 // 这里省略 } return userInfo, err } var wg sync.WaitGroup func main() { const N = 100 // 模拟 100 个 Client 的并发请求 wg.Add(N) for i := 0; i \u0026lt; N; i++ { go func() { defer wg.Done() GetUserInfo() }() } wg.Wait() } func ReadFromCache() (v any, err error) { return nil, ErrCacheMiss } func ReadFromDB() (v any, err error) { fmt.Println(\u0026#34;Read From DB\u0026#34;) time.Sleep(time.Second) // 模拟从数据库读取数据 return \u0026#34;read from DB\u0026#34;, nil } 输出：\nSky_Lee@SkyLeeMBP test % ./test Read From DB Read From DB Read From DB Read From DB Read From DB Read From DB Read From DB Read From DB Read From DB ... Read From DB 所有请求都直接走到 DB，那么使用 singleflight 呢？\nfor i := 0; i \u0026lt; N; i++ { go func() { defer wg.Done() sg.Do(\u0026#34;GetUserInfo\u0026#34;, GetUserInfo) // 使用 single flight }() } 输出：\n可以看到，仅仅走了一次 DB！\n示例 1：超时控制 引入超时控制：\n// ... for i := 0; i \u0026lt; N; i++ { go func() { defer wg.Done() ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second) // 控制超时时间为 3s defer cancel() ch := sg.DoChan(\u0026#34;GetUserInfo\u0026#34;, GetUserInfo) var res singleflight.Result select { case res = \u0026lt;-ch: case \u0026lt;-ctx.Done(): // 超时返回 fmt.Println(\u0026#34;timeout\u0026#34;) return } fmt.Println(res.Val) }() } // ... // 修改 ReadFromDB 的时间 func ReadFromDB() (v any, err error) { fmt.Println(\u0026#34;Read From DB\u0026#34;) time.Sleep(time.Second * 5) // 模拟长查询 // 模拟从数据库读取数据 return \u0026#34;read from DB\u0026#34;, nil } 输出：\nSky_Lee@SkyLeeMBP test % ./test Read From DB timeout timeout ... timeout 成功将时间控制在 3s 内\n示例 2：定期 forget 为了防止获取到过期数据，可以定期 forget，以多请求 DB 来换取数据的时效性\n例如：\nfor i := 0; i \u0026lt; N; i++ { go func() { defer wg.Done() sg.Do(\u0026#34;GetUserInfo\u0026#34;, GetUserInfo) go func() { time.Sleep(100 * time.Millisecond) sg.Forget(\u0026#34;GetUserInfo\u0026#34;) // 100ms 后删除 GetUserInfo 这个 key，接下来的并发请求，将会走 GetUserInfo，而不是等待 }() }() } 睡眠的时间可以根据下游服务的 rps 确定，例如允许的 rps 为 10req/s，那么可以设置 100ms 后删除这个 key\n当然，在 GetUserInfo 返回后，为了数据的时效性，singleflight 内部是会将这个 key 删除的\n这里 forget 针对的是在 GetUserInfo 返回前，100ms 后如果有新的请求，那么就再调用一次 GetUserInfo，而不是继续等待\n原理 数据结构 type Group struct { mu sync.Mutex // protects m m map[string]*call // lazily initialized } singleflight 的数据结构非常简单，只有两个字段：\nmu：保护 m 的互斥访问，进一步保证 m 的 goroutine 安全 m：存放 key ～ call 的映射关系 其中，call 的定义如下：\ntype call struct { wg sync.WaitGroup // 用于等待一个 goroutine 执行完毕 val interface{} // 存放执行结果 err error // 存放执行中产生的错误 dups int // 存放当前等待的 goroutine 的个数 chans []chan\u0026lt;- Result // 存放 DoChan 生成的 channel } Group.m 是懒加载的：\n使得 Group 开箱即用 Do 接下来看看 Do 方法：\nfunc (g *Group) Do(key string, fn func() (interface{}, error)) (v interface{}, err error, shared bool) { g.mu.Lock() if g.m == nil { g.m = make(map[string]*call) // 懒加载 map } // 如果这个 key 存在，说明已经有一个 goroutine 正在执行 fn，我们应该等待 if c, ok := g.m[key]; ok { c.dups++ // 当前等待的 goroutine +1 g.mu.Unlock() c.wg.Wait() // 等待 if e, ok := c.err.(*panicError); ok { // 如果执行 fn 的过程 panic 了，我们将 panic 返回给 caller panic(e) } else if c.err == errGoexit { // 如果执行 fn 的过程中，fn 调用了 runtime.GoExit() runtime.Goexit() } return c.val, c.err, true // 返回第一个调用 fn 的调用结果 } // 如果这个 key 不存在，说明我们是第一个执行 fn 的 // new 一个 call c := new(call) c.wg.Add(1) g.m[key] = c // 让后续来的 goroutine 能够知道已经有一个 goroutine 在执行 fn g.mu.Unlock() g.doCall(c, key, fn) // 这里面会同步执行 fn return c.val, c.err, c.dups \u0026gt; 0 } 过程全部放在注释中了，还是比较简单的\ndoCall doCall 是 singleflight 的核心了，我们直接上源码：\n// doCall handles the single call for a key. func (g *Group) doCall(c *call, key string, fn func() (interface{}, error)) { normalReturn := false recovered := false // use double-defer to distinguish panic from runtime.Goexit, // more details see https://golang.org/cl/134395 defer func() { // the given function invoked runtime.Goexit if !normalReturn \u0026amp;\u0026amp; !recovered { c.err = errGoexit } g.mu.Lock() defer g.mu.Unlock() c.wg.Done() if g.m[key] == c { delete(g.m, key) } if e, ok := c.err.(*panicError); ok { // In order to prevent the waiting channels from being blocked forever, // needs to ensure that this panic cannot be recovered. if len(c.chans) \u0026gt; 0 { go panic(e) select {} // Keep this goroutine around so that it will appear in the crash dump. } else { panic(e) } } else if c.err == errGoexit { // Already in the process of goexit, no need to call again } else { // Normal return for _, ch := range c.chans { ch \u0026lt;- Result{c.val, c.err, c.dups \u0026gt; 0} } } }() func() { defer func() { if !normalReturn { // Ideally, we would wait to take a stack trace until we\u0026#39;ve determined // whether this is a panic or a runtime.Goexit. // // Unfortunately, the only way we can distinguish the two is to see // whether the recover stopped the goroutine from terminating, and by // the time we know that, the part of the stack trace relevant to the // panic has been discarded. if r := recover(); r != nil { c.err = newPanicError(r) } } }() c.val, c.err = fn() normalReturn = true }() if !normalReturn { recovered = true } } 有点长，我们分解成若干部分来看：\n部分 0 // 调用匿名函数 func() { // 匿名函数返回前，执行 defer 语句 defer func() { if !normalReturn { // 说明 fn 执行过程 panic 了，先 recover 住 if r := recover(); r != nil { c.err = newPanicError(r) // 给执行结果 c 的 err 字段赋值一个 panicError 类型的 error } } }() c.val, c.err = fn() // 先调用 fn normalReturn = true // 如果执行到这里，说明 fn 没有 panic，也没有调用 runtime.GoExit() }() if !normalReturn { // 如果执行到这里，说明 fn 没有调用 runtime.GoExit，而是 panic 了，但是被我们 recover 了 recovered = true } 这一部分主要注意两个异常的处理：\npanic runtime.GoExit 如果 fn 执行过程 panic 了，会创建一个 panicError，包含了 panic 的堆栈信息：\n// A panicError is an arbitrary value recovered from a panic // with the stack trace during the execution of given function. type panicError struct { value interface{} stack []byte } func newPanicError(v interface{}) error { stack := debug.Stack() // The first line of the stack trace is of the form \u0026#34;goroutine N [status]:\u0026#34; // but by the time the panic reaches Do the goroutine may no longer exist // and its status will have changed. Trim out the misleading line. if line := bytes.IndexByte(stack[:], \u0026#39;\\n\u0026#39;); line \u0026gt;= 0 { stack = stack[line+1:] } return \u0026amp;panicError{value: v, stack: stack} } 部分 1 // 使用 double-defer 来区分 panic 与 Goexit， // 更多细节查看：https://golang.org/cl/134395 defer func() { if !normalReturn \u0026amp;\u0026amp; !recovered { // 如果没有被 recover，说明 fn 内部调用了 runtime.GoExit // 给 err 赋值为 errGoexit c.err = errGoexit } g.mu.Lock() defer g.mu.Unlock() c.wg.Done() if g.m[key] == c { // fn 执行完毕了，删除 key，保证后续请求重新执行 fn // 注意：执行结果已经存储在 c 中，之前等待的 goroutine 直接返回 c.val, c.err 即可 delete(g.m, key) } if e, ok := c.err.(*panicError); ok { // 执行到这，说明 fn 执行过程 panic 了 // 为了保证等待 channel 的 goroutine（针对 DoChan 方法）不被永久阻塞 // 需要保证这个 panic 无法被 recover if len(c.chans) \u0026gt; 0 { // 说明调用过 DoChan 方法 go panic(e) // 新建一个 goroutine 来 panic（父 goroutine 无法 recover 子 goroutine 的 panic） select {} // 让这个 goroutine 阻塞，这样它可以出现在错误堆栈信息中 } else { panic(e) // 没有调用过 DoChan 方法，意味着没有 goroutine 因为 channel 而阻塞，我们直接 panic 就行（允许调用者 recover） } } else if c.err == errGoexit { // 在 fn 中已经调用过 runtime.Goexit，我们不需要再次执行 } else { // 正常返回 // 唤醒所有等待 channel 的 goroutine for _, ch := range c.chans { ch \u0026lt;- Result{c.val, c.err, c.dups \u0026gt; 0} } } }() 细节已经在注释中给出\n根据两个部分的代码，可以看出：doCall 使用 double-defer 来分辨 fn 中的 panic 与 runtime.Goexit\n这里补充一下 runtime.Goexit\nGoexit terminates the goroutine that calls it. No other goroutine is affected.\nGoexit runs all deferred calls before terminating the goroutine. Because Goexit is not a panic, any recover calls in those deferred functions will return nil.\nDoChan DoChan 与 Do 的区别在于：DoChan 会先返回一个 channel 用作超时控制，异步执行 fn：\n// DoChan is like Do but returns a channel that will receive the // results when they are ready. // // The returned channel will not be closed. func (g *Group) DoChan(key string, fn func() (interface{}, error)) \u0026lt;-chan Result { ch := make(chan Result, 1) // 新建一个带 buffer 的 channel（防止 doCall 发送阻塞） g.mu.Lock() if g.m == nil { g.m = make(map[string]*call) } if c, ok := g.m[key]; ok { // 如果已经有 goroutine 执行过 fn c.dups++ c.chans = append(c.chans, ch) // 加到 chans 中，后续可以在 doCall 中批量返回 g.mu.Unlock() return ch } c := \u0026amp;call{chans: []chan\u0026lt;- Result{ch}} c.wg.Add(1) g.m[key] = c g.mu.Unlock() go g.doCall(c, key, fn) // 异步执行 doCall return ch } Forget Forget 方法很简单，就是删除 Group.m 中的某个 key：\nfunc (g *Group) Forget(key string) { g.mu.Lock() delete(g.m, key) g.mu.Unlock() } 总结 Singleflight 是一个并发控制库，用于防止多个 goroutine 同时执行某个函数，避免重复执行，我们可以利用这个特性来防止缓存击穿 Singleflight 开箱即用，提供了三个 API 在一些数据实时性要求较高的应用中，可以使用 forget 方法，来发起新的请求，而不是等待之前的请求返回 doCall 方法使用 double-defer 来分辨 fn 中的 panic 与 runtime.Goexit ","permalink":"https://blogs.skylee.top/posts/go/singlefilght/note/","tags":["Golang","并发编程"],"title":"Go: Single Flight"},{"categories":["Kafka"],"content":"Kafka 核心问题 本文仅包括了一些常见问题，后续会持续更新。\ntopic 分区目的 kafka 为什么这么快？ 幂等性 事务 副本同步机制 AKF ZK 的作用 Offset 持久化 消费逻辑（单线程？多线程？） Producer 生产数据的流程 broker 网络层通信原理（IO 模型？mute-unmute？为什么无法保证多个 producer 生产消息的有序？与 Redis 对比？） 消息持久化原理（日志怎么存储的？每个段包含哪些内容？什么时候分段？日志会被删除吗？日志怎么刷盘的？Index 怎么刷盘的？为什么默认将真正的刷盘交给 OS Kernel？） Replica 同步数据的流程（leader 写完 local log 后，做什么？delayCallResponse 的作用？ISR 是啥？HW 的更新过程？如何让数据流转更快？） Controller 选举过程 Controller 选举完成后，上下文的初始化 Controller 的作用 Partition Leader 的选举流程 Controller 脑裂怎么解决 create topic 的流程 Consumer（leader、coordinator 的概念？coordinator 的选举过程？offset 持久化的位置？加入一个 consumer group 的过程？消费数据的过程？） 在同一个 group 下，一个 partition 只能由一个 consumer 消费吗？=\u0026gt; 并发的关键？ 事务（针对谁？）： 如何保证消息仅被生产一次 为什么要引入 TxID？ 为什么要引入 Tx Marker？ 从事务角度理解为什么 Consumer 只能消费 HW 以下的消息？ 跨多 Topic、多 Partition 的事务支持？ 下面是一些八股：\n你为什么用 Kafka Kafka 如何保证数据不丢失 Kafka 数据丢失的场景 消息积压怎么办 Kafka 为什么高效 2024.06.07\n补充一张 Kafka 的思维导图\n","permalink":"https://blogs.skylee.top/posts/kafka/%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98/note/","tags":["Kafka"],"title":"Kakfa 核心问题"},{"categories":["Golang"],"content":"标记清扫法 Go 1.3 的 GC 采用标记清扫法，分为两个步骤：\n标记 清扫 标记的过程很简单：从对象根节点开始 dfs，每遍历到一个节点就做标记，最后没有标记的节点就是要清理的节点：\n例如上图：对象 4 和对象 5 将在清扫过程中被清理\n为了保证 GC 的过程中，对象之间的引用关系不被改变，go runtime 会 阻塞除了 gc 以外的所有 goroutine，这个过程被称为 STW（stop-the-world）\nSTW 的时间越长，整个应用程序阻塞的时间就会越长，给用户的体验就是进程「卡住了」\n三色标记法 1.3 版本的 Go 采用的标记清扫法的 STW 时间太长了\n在 1.5 版本，Go 引入了三色标记法\n原理 三色标记法将程序中的对象分成白色、黑色和灰色三类。\n白色：不确定对象。 灰色：存活对象，子对象待处理。 黑色：存活对象。 标记过程如下：\nSTW，标记准备，将所有节点标记为白色 解除 STW 将所有根节点标记为 灰色 遍历灰色集合：将灰色节点标记为黑色，灰色节点的字节点标记为灰色 持续步骤 4，直到灰色集合为空 用几张图来展示三色标记法的过程：\n最终，对象 4 和对象 5 处于白色集合，将被 GC 掉\n存在的问题 在标记的过程中，是没有 STW 的，也就是说，GC 和其它 goroutine 是并发执行的\n并发执行，意味着 GC 过程中，其它 goroutine 可能修改了对象间的引用关系 ，这会带来两个问题：\n某一个对象，没有被任何对象引用，但无法被 GC 某一个对象，被其它对象引用，但是被 GC 第一种情况还好，即使这一次没有被 GC，下一轮 GC 也可以把它回收掉\n但第二种情况就比较严重了，回收了正在使用的对象，这会带来严重后果：\n在这个情景下，由于对象 0 已经是黑色，无法将对象 4、5 标记为黑色，会导致对象 4、5 在本轮 GC 错误回收！\n错误回收发生的根本原因还是在于：黑色对象直接引用了一个白色对象，白色对象无法被染色，于是就被回收了\n屏障 为了避免上述问题，我们当然可以在标记时，设置 STW，防止并发修改\n但这样就退化为标记清扫法了，我们应该在没有 STW 的前提（即允许并发修改）下，保证对象不被错误回收\nGo 在实现三色标记法时，引入了写屏障\n插入写屏障 插入写屏障的原理是：不允许一个黑色节点的子节点为白色\n引入写屏障，可以在允许并发修改的条件下，保证对象不被错误回收\n这样，对象 4、5 就不会被回收了\n为了保证栈的读写效率，写屏障只会在堆对象启用，不会在栈对象启用\n这意味着如果对象 4、5 如果存储在栈区，还是有可能被错误回收的\n因此，使用插入写屏障，需要在白色对象回收完毕后，再单独扫描一遍栈区，并且，这个扫描过程 需要 STW\n删除写屏障 原理直接看图吧：\n引入删除屏障，也可以在允许并发修改的条件下，保证对象不被错误回收\n但是这种删除方式会出现 遗漏删除 的问题：\n对象 1、2、3 应该在这一轮被 GC，但是却没有，只能在下一轮 GC\n当然，标准的 Go 实现（至少到目前为止）并未使用传统意义上的删除写屏障，在 1.5 ～ 1.8 版本间，Go 使用的是插入写屏障机制\n混合写屏障 插入写屏障和删除写屏障的短板：\n插入写屏障：结束时 需要 STW 来重新扫描栈，标记栈上引用的白色对象的存活； 删除写屏障：回收精度低 为了弥补这两种方式的短板，Go 1.8 引入了 混合写屏障\nAs of Go 1.7, the one remaining source of unbounded and potentially non-trivial stop-the-world (STW) time is stack re-scanning. We propose to eliminate the need for stack re-scanning by switching to a hybrid write barrier that combines a Yuasa-style deletion write barrier [Yuasa \u0026lsquo;90] and a Dijkstra-style insertion write barrier [Dijkstra \u0026lsquo;78]. Preliminary experiments show that this can reduce worst-case STW time to under 50µs, and this approach may make it practical to eliminate STW mark termination altogether.\nEliminating stack re-scanning will in turn simplify and eliminate many other parts of the garbage collector that exist solely to improve the performance of stack re-scanning. This includes stack barriers (which introduce significant complexity in many parts of the runtime) and maintenance of the re-scan list. Hence, in addition to substantially improving STW time, the hybrid write barrier should also reduce the overall complexity of the garbage collector.\n顾名思义，混合写屏障就是将 插入写屏障 与 删除写屏障 结合起来了，具体规则如下：\nGC 开始将栈上的对象 全部扫描并标记为黑色（这一步需要 STW，但时间很短） GC 期间，任何在栈上创建的新对象，均为黑色。 堆上添加一个新的引用对象，将引用对象标记为灰色 堆上删除一个已有引用对象，将引用对象标记为灰色 这种方式避免了结束时需要 STW 重新扫描栈，但仍然存在 回收精度低 的问题\n所以目前我还没有弄清楚为什么 Go1.8 要引入混合写屏障，直接用删除写屏障不就好了吗？\nGolang 官方给出了混合写屏障的优缺点：\nThe advantage of the hybrid barrier is that it lets a stack scan permanently blacken a stack (without a STW and without write barriers to the stack), which entirely eliminates the need for stack re-scanning, in turn eliminating the need for stack barriers and re-scan lists. Stack barriers in particular introduce significant complexity throughout the runtime, as well as interfering with stack walks from external tools such as GDB and kernel-based profilers.\nAlso, like the Dijkstra-style write barrier, the hybrid barrier does not require a read barrier, so pointer reads are regular memory reads; and it ensures progress, since objects progress monotonically from white to grey to black.\nThe disadvantages of the hybrid barrier are minor. It may result in more floating garbage, since it retains everything reachable from roots (other than stacks) at any point during the mark phase. However, in practice it\u0026rsquo;s likely that the current Dijkstra barrier is retaining nearly as much. The hybrid barrier also prohibits certain optimizations: in particular, the Go compiler currently omits a write barrier if it can statically show that the pointer is nil, but the hybrid barrier requires a write barrier in this case. This may slightly increase binary size.\n用中文来说的话：如果在栈引入屏障，它在运行时 会引入相当的复杂性，同时也干扰了如 GDB 和基于内核的性能分析等外部工具的栈回溯。\n因此，混合写屏障通过将栈上对象全部标记为黑色，彻底避免了重新 STW 来扫描栈（STW stack re-scanning），同时也避免了栈上的写屏障，保证了运行效率\n当然，混合写屏障还是有缺点的：可能产生一些浮动垃圾（但是 dijkstra 风格的插入写屏障还是有浮动垃圾的可能）\nGC 的时机 自动触发 // src/runtime/mgc.go const ( gcTriggerHeap gcTriggerKind = iota gcTriggerTime gcTriggerCycle ) 一共有三种触发方式：\ngcTriggerHeap：堆内存分配 gcTriggerTime：定时触发，以 runtime.forcegcperiod 变量为准，默认 2min 一次 gcTriggerCycle：如果没有启用 GC，那么启用（这种方式出现在手动调用 runtime.GC()） 手动触发 可以调用 runtime.GC() 来手动触发 GC\n自动触发基本流程 go 的 runtime 在初始化时，会启动一个 goroutine 用于 GC：\nfunc init() { go forcegchelper() } func forcegchelper() { forcegc.g = getg() lockInit(\u0026amp;forcegc.lock, lockRankForcegc) for { lock(\u0026amp;forcegc.lock) if forcegc.idle != 0 { throw(\u0026#34;forcegc: phase error\u0026#34;) } atomic.Store(\u0026amp;forcegc.idle, 1) goparkunlock(\u0026amp;forcegc.lock, waitReasonForceGCIdle, traceEvGoBlock, 1) // 将当前 goroutine 挂起，等待被唤醒 // this goroutine is explicitly resumed by sysmon if debug.gctrace \u0026gt; 0 { println(\u0026#34;GC forced\u0026#34;) } gcStart(gcTrigger{kind: gcTriggerTime, now: nanotime()}) } } 这个 goroutine 会先被挂起，等待被其它 goroutine 唤醒\n唤醒这个操作实际上是由一个监控 goroutine 完成的：\nfunc sysmon() { ... for { ... // check if we need to force a GC if t := (gcTrigger{kind: gcTriggerTime, now: now}); t.test() \u0026amp;\u0026amp; atomic.Load(\u0026amp;forcegc.idle) != 0 { lock(\u0026amp;forcegc.lock) forcegc.idle = 0 var list gList list.push(forcegc.g) // 将 forcegc.g 加入到 GRQ 中 injectglist(\u0026amp;list) unlock(\u0026amp;forcegc.lock) } if debug.schedtrace \u0026gt; 0 \u0026amp;\u0026amp; lasttrace+int64(debug.schedtrace)*1000000 \u0026lt;= now { lasttrace = now schedtrace(debug.scheddetail \u0026gt; 0) } unlock(\u0026amp;sched.sysmonlock) } } 这个函数会不停检测 idle time 是否与 forcegcperiod 相等，如果相等，会将 forcegc 这个 goroutine 加入到全局队列中，等待调度\n参考资料 Golang 垃圾回收+混合写屏障 GC 全分析 Proposal: Eliminate STW stack re-scanning ","permalink":"https://blogs.skylee.top/posts/go/gc/note/","tags":["Golang"],"title":"Go: GC 原理"},{"categories":["Golang"],"content":"前置知识 OS Scheduler 在介绍 Go Scheduler 之前，先要了解一下 OS Scheduler\nOS Scheduler 是在 OS 层面实现的调度器，调度的基本单位为（内核级）线程，保证在尽可能公平的条件下，充分利用 CPU 资源\n在 OS Scheduler 的调度下，各个线程看起来是同时运行的，也就是通常意义上的「并发」\ngoroutine 什么是 goroutine 前面提到 OS Scheduler 调度的基本单位为内核级线程，但是内核级线程的创建、切换、销毁的代价相对还是比较高的（涉及到用户态与内核态的切换）\n在 C++、Java 中，并发编程使用到的就是内核级线程，一般都会使用线程池，来复用线程，以减小线程的创建、切换、销毁的代价\n但是 Go 引入了 goroutine，以另一个角度来以减小线程的创建、切换、销毁的代价\ngoroutine 是在 用户态实现 的线程（通常也叫做协程），这意味着 goroutine 的创建、切换、销毁完全是在用户态进行的\n有了 goroutine，Gopher 不会直接面对 kernel thread，我们只会看到代码里满天飞的 goroutine。OS 却相反，goroutine 对 OS 是不可见的，OS 只需要关心内核级线程就可以了\n基于 goroutine，Go 天然支持高并发\ngoroutine 与 kernel thread 的区别 创建、销毁：goroutine 的创建与销毁代价极低，不涉及变态 内存占用：单个 goroutine 的栈空间仅有 2KB，而单个 kernel thread 的栈空间却有 1M 切换：goroutine 的切换，涉及到的寄存器比 kernel thread 少，上下文切换时间短 Scheduler 什么是 Go Scheduler Go Scheduler 是 Go 在用户态实现的调度器，负责 goroutine 的调度\n一个使用 Go 编写的应用，包含两个部分：\nprogram runtime 内存分配、基于 channel 的 通信、goroutine 的创建都是在 runtime 进行的：\n为什么要 Go Scheduler scheduler 是 runtime 最重要的一部分，负责了所有 goroutine 的调度，是 goroutine 能并发执行的基石，如果没有 scheduler，那么 goroutines 就是一盘散沙\n程序执行的效率也与 schduler 的调度策略有关，好的 scheduler 可以更好的利用 CPU 资源，保证性能和响应速度\nScheduler 概览 scheduler 的调度，离不开三个结构体：\nGoroutine (G) Machine（M）：可以理解为一个 M 对应了一个 kernel thread Processor（P）：可以理解为一个 P 对应了一个 CPU 逻辑处理器 Machine（M）\ngoroutines 并不是直接与操作系统线程相绑定的，而是通过 M 来执行。一个 M 往往负责多个 G 的执行\nProcessor（P）\nProcessor（P）的数量取决于 CPU 的逻辑处理器个数\nP 会维护一个本地队列，这个队列存储了可执行（Runnable）状态的 goroutine（G）\nG 的执行依赖于 M，而 M 需要获得 P 才能执行 G\n在一个时刻，一个 P 只能关联一个 M，这样，可以控制整个应用程序中，kernel thread 的数量，减少 kernel thread 创建、切换、销毁的开销\n用一张图来展示 G、M、P 三者关系：\n2024.4.7：\nGlobal Runnable Queue：存储全局可运行的 goroutine 的队列，这些 goroutine 还没有分配到任意的 P\n那么，一个 goroutine 在什么时候会被存放到全局队列（GRQ）？\n刚刚创建的 goroutine 可能被分配到 GRQ 中 goroutine 运行时间过长：Go scheduler 会启动一个后台线程 sysmon，用来检测长时间（超过 10 ms）运行的 goroutine，将其调度到 GRQ，以示惩罚（优先级较低） 待补充。。。 Scheduler 的目标 仅使用较少的 kernel thread（复用 M） 支持高并发（G 的轻量级、较少的 M、优秀的调度） 利用并行性，扩展到 N 个逻辑核（N 个 P） goroutine 被调度的时机 创建一个新的 goroutine 发起 syscall GC goroutine 阻塞（例如因为 channel、mutex） M 是如何选择 goroutine 的 M 选择 goroutine 执行这一过程就是 scheduler 的调度，策略如下：\n在关联的 P 的本地队列拉取一个 goroutine 执行 为了保证公平性，每循环 61 次，从全局队列中拉取一个 goroutine 执行 如果关联的 P 的本地队列中，没有 goroutine，那么 M 会尝试在其它 P 的本地队列中「偷取」一半的 goroutines 到关联的 P 的本地队列。这个过程被称为 Work-stealing // Go 1.9 // 执行一轮调度器的工作：找到一个 runnable 的 goroutine，并且执行它 // 永不返回 func schedule() { // _g_ = 每个工作线程 m 对应的 g0，初始化时是 m0 的 g0 _g_ := getg() // …………………… top: // …………………… var gp *g var inheritTime bool // …………………… if gp == nil { // Check the global runnable queue once in a while to ensure fairness. // Otherwise two goroutines can completely occupy the local runqueue // by constantly respawning each other. // 为了公平，每调用 schedule 函数 61 次就要从全局队列中获取一个 goroutine if _g_.m.p.ptr().schedtick%61 == 0 \u0026amp;\u0026amp; sched.runqsize \u0026gt; 0 { lock(\u0026amp;sched.lock) // 从全局队列最大获取 1 个 gorutine gp = globrunqget(_g_.m.p.ptr(), 1) unlock(\u0026amp;sched.lock) } } // 从 P 本地获取 G 任务 if gp == nil { gp, inheritTime = runqget(_g_.m.p.ptr()) if gp != nil \u0026amp;\u0026amp; _g_.m.spinning { throw(\u0026#34;schedule: spinning with local work\u0026#34;) } } if gp == nil { // 从本地运行队列和全局运行队列都没有找到需要运行的 goroutine， // 调用 findrunnable 函数从其它工作线程的运行队列中偷取，如果偷不到，则当前工作线程进入睡眠 // 直到获取到 runnable goroutine 之后 findrunnable 函数才会返回。 gp, inheritTime = findrunnable() // blocks until work is available } // This thread is going to run a goroutine and is not spinning anymore, // so if it was marked as spinning we need to reset it now and potentially // start a new spinning M. if _g_.m.spinning { resetspinning() } if gp.lockedm != nil { // Hands off own p to the locked m, // then blocks waiting for a new p. startlockedm(gp) goto top } // 执行 goroutine 任务函数 // 当前运行的是 runtime 的代码，函数调用栈使用的是 g0 的栈空间 // 调用 execute 切换到 gp 的代码和栈空间去运行 execute(gp, inheritTime) } 同步与异步 syscall 前面说到：在任意时刻，一个 P 只能关联一个 M\n那么，如果 M 阻塞了，怎么办呢？\nM 阻塞的情况通常出现于 同步系统调用\nM 一旦阻塞，P 本地队列中的 goroutine 就无法运行了\n为了避免这种情况，scheduler 会将阻塞的 M 与关联的 P 分离开，然后创建一个新的 M 关联这个 P\nM 恢复后，对应的 goroutine 会重新回到 LRQ，而 M 则保存下来留着以后使用\n对于异步的系统调用，情况有所不同：发起异步调用的 goroutine 会交给 net poller 来管理，M 得以释放，给剩余的 goroutine 执行机会：\nGo 的网络轮询器（net poller）是 Go 运行时（runtime）用来高效处理 I/O 操作的一部分。网络轮询器底层利用操作系统提供的事件通知机制，例如 Linux 的 epoll，可以非阻塞地等待一组文件描述符上的 I/O 事件。\n当 goroutine 执行一个网络 I/O 操作时，如果数据准备好了，它会立即处理。如果数据没有准备好，goroutine 会被 挂起，并被 加入到等待队列中。网络轮询器会监测对应的文件描述符，等待事件发生。\n当相关事件发生，如连接建立、数据到达、可以发送数据等，系统通知网络轮询器，网络轮询器会 唤醒 之前挂起的 goroutine。\n从这里可以看出来：只要有 Runnable 的 goroutine，scheduler 不会让任何 M 闲下来\nIn Go, it’s possible to get more work done, over time, because the Go scheduler attempts to use less Threads and do more on each Thread, which helps to reduce load on the OS and the hardware.\n为什么需要 P，只要 G、M 不行吗？ 在 Go 1.5 之前，实际上是没有 P 的，只有 G、M\n在只有 G、M 的情况下，所有的 M 都要在一个 全局的队列 中获取 Runnable goroutine，那么这个过程一定要加一把 大锁，锁的粒度大，并发能力不强\n于是 Go 1.5 引入了 P，每个 P 维护一个 LRQ，避免了加锁，并发能力得到提升\n为什么一定要有一个 P，直接让 M 来管理 LRQ 不行吗？\n前面提到了：M 可能被阻塞，如果 M 被阻塞，那么 M 的 LRQ 中剩余的 goroutines 无法被执行，并发能力也降低了\n因此，P 的存在相当于将 LRQ 与 M 解耦，是很有必要的\n参考资料 深度解密 Go 语言之 scheduler 新官上任 —— Go sheduler 开始调度循环（五） 本文仅仅是浅浅介绍了一下 Go Scheduler，要想深入了解，可以看看 这个系列的文章 ","permalink":"https://blogs.skylee.top/posts/go/scheduler/note/","tags":["Golang","并发编程","GMP"],"title":"Go: Scheduler"},{"categories":["Golang"],"content":"Map 是 Go 提供的一个 key-value 数据结构，底层使用 hash，就像 std::unordered_map 一样\n定义一个 Map 的语法：\nmap[keyType]valueType 定义一个 map，初始值是 nil\nMap 有容量吗 Go 的 map 类型在语言规范中没有直接提供容量的概念，这与 C++ 是一致的\n但是内部会对容量进行管理，以确保在大部分情况下，map 的性能是高效的。这意味着在添加元素时，map 会自动进行内存分配和扩容，以适应新的元素。\n尽管 map 的容量由运行时自动管理，但在某些情况下，可以使用内置的 make 函数来预分配一个指定大小的 map。\nmake(map[keyType]valueType, 10) 这可以用于优化内存分配，但需要明确指出，这个容量大小不是 map 的硬性限制，map 仍然会在需要时自动进行扩容。\n基本使用 // 声明并初始化 map users := make(map[string]int, 3) // 添加新元素 users[\u0026#34;NiuMa\u0026#34;] = 1 users[\u0026#34;MaBaoGuo\u0026#34;] = 2 users[\u0026#34;DingZhen\u0026#34;] = 3 // 修改元素 users[\u0026#34;MaBaoGuo\u0026#34;] = 4 // 遍历 for name, money := range users { fmt.Printf(\u0026#34;name: %s, money: %d\\n\u0026#34;, name, money) } 和 C++ 基本一致，注意遍历使用的是 for range\n注意：在使用 map 前，一定要初始化分配内存，否则为 nil，插入元素等操作会 panic：\n判断某个键是否存在 val, ok := users[\u0026#34;NiuMa\u0026#34;] if ok { fmt.Printf(\u0026#34;Has key NiuMa, val: %d\u0026#34;, val) } 删除 key 使用 delete 函数删除一个 key：\nfunc delete(m map[Type]Type1, key Type) 例如，删除用户马保国：\ndelete(users, \u0026#34;NiuMa\u0026#34;) 按照指定顺序遍历 map 底层用到 hash，元素的顺序无法得到保障\n就算元素顺序与插入顺序一致，Go 也会将这些数据打乱，避免新手 Gopher 认为 map 的遍历顺序与插入顺序一致\n具体来说，遍历时，每一轮是随机选择一个 key 返回的\n要想有序遍历，可以使用 sort，将 key 进行排序：\nkeys := make([]string, 0, len(users)) // 避免扩容，提高性能 for k := range users { keys = append(keys, k) } sort.Strings(keys) for _, key := range keys { fmt.Println(key, users[key]) } value 类型为切片的 map 当 map 的 value 的类型为切片时：\ncountry := make(map[string][]string , 3) // 添加新元素 country[\u0026#34;中国\u0026#34;] = append(country[\u0026#34;中国\u0026#34;], \u0026#34;北京\u0026#34;, \u0026#34;上海\u0026#34;) for c, v := range country { fmt.Printf(\u0026#34;Country: %s\\nCities: \u0026#34;, c); for _, _c := range v { fmt.Printf(\u0026#34;%v \u0026#34;, _c) } println() } value 类型为 map 的切片 users := make([]map[string]string, 3) // size == 3 // 初始化切片中的 map for idx := range users { users[idx] = make(map[string]string, 3) } users[0][\u0026#34;name\u0026#34;] = \u0026#34;MaBaoGuo\u0026#34; users[0][\u0026#34;age\u0026#34;] = \u0026#34;69\u0026#34; users[0][\u0026#34;hobby\u0026#34;] = \u0026#34;WuLianBian\u0026#34; map 的拷贝 map 不能直接拷贝，如果要拷贝一个 map，只能遍历：\noriginalMap := make(map[string]int) originalMap[\u0026#34;one\u0026#34;] = 1 originalMap[\u0026#34;two\u0026#34;] = 2 // Create the target map targetMap := make(map[string]int) // Copy from the original map to the target map for key, value := range originalMap { targetMap[key] = value } 如果 value 为指针类型，还需要考虑深拷贝的问题：\noriginalMap := make(map[string]*int) var num int = 1 originalMap[\u0026#34;one\u0026#34;] = \u0026amp;num // Create the target map targetMap := make(map[string]*int) // Copy from the original map to the target map for key, value := range originalMap { var tmpNum int = *value targetMap[key] = \u0026amp;tmpNum } 如果 value 是结构体类型，可以直接赋值：\ntype Foo struct { i int } func main() { m1 := make(map[string]Foo) m1[\u0026#34;111\u0026#34;] = Foo{i: 1} m1[\u0026#34;111\u0026#34;] = Foo{i: 2} // ok } 但是不能修改结构体成员的值：\nm1[\u0026#34;111\u0026#34;].i = 1 // cannot assign to struct field m1[\u0026#34;111\u0026#34;].i in map 建议：如果 value 是结构体类型，将其替换为结构体类型对应的指针类型：\ntype Foo struct { i int } func main() { m2 := make(map[string]*Foo) m2[\u0026#34;111\u0026#34;] = \u0026amp;Foo{i: 1} m2[\u0026#34;111\u0026#34;].i = 1 // ok } map 底层是怎么实现的 建议看看这篇文章：深度解密 Go 语言之 map 这里给出一个简单的总结：\ngo 的 map 底层使用哈希表存储 k-v，使用链地址法解决哈希冲突 key 根据哈希值的不同，会存储到不同的 bucket 中 每个 bucket 有 8 个 ceil，当发生哈希冲突，key 会存到相同 bucket 的不同 ceil 中 如果单个 bucket 的 ceil 全部填满了，将原 bucket 标记为 overflow，并创建一个新的 bucket，基于链表的方式追加在原 bucket 中 负载因子 factor = key 的个数 / bucket 的个数，如果： factor \u0026gt; 6.5 overflow 的 bucket 过多就会触发 扩容 扩容，也叫做 rehash，包括等量扩容和二倍容量扩容，为了避免扩容带来的性能开销，go 采用渐进式 rehash（和 redis 的 dict 一样） ","permalink":"https://blogs.skylee.top/posts/go/map/map/note/","tags":["Golang"],"title":"Go: Map"},{"categories":["Golang"],"content":"原生的 Map 有并发访问的安全问题：\nvar m map[int]int = make(map[int]int, 4) func Store(key, val int) { m[key] = val } func Load(key int) (int, bool) { val, ok := m[key] return val, ok } func main() { var wg sync.WaitGroup wg.Add(100) for i := 0; i \u0026lt; 100; i++ { go func(key int) { defer wg.Done() Store(key, key) Load(key) }(i) } wg.Wait() } 输出：\nfatal error: concurrent map writes map 不允许并发写，可以使用读写锁解决：\nfunc Store(key, val int) { defer rwlock.Unlock() rwlock.Lock() m[key] = val } func Load(key int) (int, bool) { defer rwlock.RUnlock() rwlock.RLock() val, ok := m[key] return val, ok } sync 包也提供了一个 开箱即用、并发访问安全 的 map：\n使用 sync.Map 修改上面的代码：\nvar m sync.Map func Store(key, val int) { m.Store(key, val) } func Load(key int) (int, bool) { val, ok := m.Load(key) if ok { // 确实有 key res, ok2 := val.(int) if ok2 { // 检查 val 的类型 return res, ok } } return -1, false } sync.Map 底层是怎么实现的 数据结构 type Map struct { mu Mutex // 保护 dirty read atomic.Pointer[readOnly] // 允许并发读 dirty map[any]*entry // 非 goroutine 安全的原始 map misses int // 记录了读取 read，miss 的次数 } 再来看看 readOlny 是个啥：\n// readOnly is an immutable struct stored atomically in the Map.read field. type readOnly struct { m map[any]*entry amended bool // true if the dirty map contains some key not in m.（如果 dirty 包含一些不存在于 m 中的 key，那么为 true） } 可以发现：\nMap 使用了两个原生的 map，分别是 read.m、dirty read 的类型是 atomic.Pointer[readOnly]，可以并发地读。但如果需要更新 read，则需要加锁保护 dirty 是一个原始 map，新增加的 key 都存储在这里 mu 是互斥锁，用于保护 dirty 的读写操作 如果一个 key 在 dirty 而不在 read 中（这个 key 是后添加的），那么递增 misses 无论是 read，还是 dirty，其 value 类型都是指向 entry 的指针，那么 entry 是个啥？\n// An entry is a slot in the map corresponding to a particular key. type entry struct { p atomic.Pointer[any] } entry 实际上使用原子指针封装了真实 value\n这里借用 深度解密 Go 语言之 sync.map 的一张图片来帮助理解：\nStore Store 方法用于向 sync.Map 存储数据\n可以先想想实现 Store 的大方向：\n判断 key 是否存在 如果存在，那么修改 value 即可 否则，新增一个 entry 到 map 中 来看看 sync 包是怎么实现的：\nfunc (m *Map) Store(key, value any) { _, _ = m.Swap(key, value) } func (m *Map) Swap(key, value any) (previous any, loaded bool) { read := m.loadReadOnly() // 如果 read 中，包含这个 key，那么直接修改即可（这个修改的过程在 trySwap 中，基于 CAS 实现原子修改 entry） if e, ok := read.m[key]; ok { if v, ok := e.trySwap(\u0026amp;value); ok { if v == nil { return nil, false } return *v, true } } // 准备操作 dirty，加锁 m.mu.Lock() read = m.loadReadOnly() // 双重检查（double-check） if e, ok := read.m[key]; ok { // 如果 read map 中存在该 key，但 p == expunged，则说明 m.dirty != nil 并且 m.dirty 中不存在该 key 值，直接修改 dirty if e.unexpungeLocked() { m.dirty[key] = e } if v := e.swapLocked(\u0026amp;value); v != nil { loaded = true previous = *v } } else if e, ok := m.dirty[key]; ok { if v := e.swapLocked(\u0026amp;value); v != nil { loaded = true previous = *v } } else { if !read.amended { // 说明 dirty 为 nil // We\u0026#39;re adding the first new key to the dirty map. // Make sure it is allocated and mark the read-only map as incomplete. m.dirtyLocked() // 给 dirty 分配内存 m.read.Store(\u0026amp;readOnly{m: read.m, amended: true}) // 修改 amended 为 true } m.dirty[key] = newEntry(value) // 在 dirty 中添加新的 key-value } m.mu.Unlock() return previous, loaded } func (m *Map) dirtyLocked() { if m.dirty != nil { return } read := m.loadReadOnly() // 给 dirty 分配内存 m.dirty = make(map[any]*entry, len(read.m)) // 将 read 的所有元素拷贝到 dirty 中 for k, e := range read.m { if !e.tryExpungeLocked() { m.dirty[k] = e } } } 大致步骤如下：\n检查 read 中是否有这个 key，如果有，直接原子修改 entry 即可（这个修改在 read 和 dirty 都是可见的），返回 第一步没有成功：要么 read 中没有这个 key，要么 key 被标记为删除。则先加锁，再进行后续的操作 双重检查 read，如果 read map 中存在该 key，但 p == expunged，则说明 m.dirty != nil 并且 m.dirty 中不存在该 key 值（被删除了），直接修改 dirty，返回 否则，检查 dirty 是否为 nil，如果为 nil，分配内存，修改 amended 为 true 在 dirty 中添加新的 key-value 可以发现，新增的 key 都是加在 dirty 中的\n在给 dirty 分配内存时，需要将 read 的所有数据（除了删除过的）拷贝到 dirty 中，以便后续快速将 dirty 提升为 read\nLoad 还是先想想实现 Load 的大方向：\n判断这个 key 是否存在： 如果存在，那么返回 否则返回对应类型的零值 核心就在判断 key 是否存在上面\n我们来看看 sync 包是怎么实现的：\nfunc (m *Map) Load(key any) (value any, ok bool) { read := m.loadReadOnly() e, ok := read.m[key] if !ok \u0026amp;\u0026amp; read.amended { // 如果 read 中没有，并且 dirty 不为空 m.mu.Lock() // Avoid reporting a spurious miss if m.dirty got promoted while we were // blocked on m.mu. (If further loads of the same key will not miss, it\u0026#39;s // not worth copying the dirty map for this key.) read = m.loadReadOnly() e, ok = read.m[key] if !ok \u0026amp;\u0026amp; read.amended { // 双重检查 e, ok = m.dirty[key] // 在 dirty 中获取 key 对应的 value // Regardless of whether the entry was present, record a miss: this key // will take the slow path until the dirty map is promoted to the read // map. m.missLocked() // 无论 key 对应的 value 是否存在，都要执行 missLocked（因为访问了 dirty） } m.mu.Unlock() } if !ok { return nil, false } return e.load() } Load 的实现很简单，主要分为两个大的步骤：在 read 中查找，以及在 dirty 中查找：\n在 read 中查找，如果有，直接返回（相当于 cache hit） 如果 read 中没有，并且 dirty 不为空，尝试在 dirty 查找（相当于 cache miss） 双重检查一波，如果确实 read 没有这个 key，那么在 dirty 获取 因为访问了 dirty，无论 key 对应的 value 是否存在，执行 missLocked 返回结果 missLocked 的实现如下：\nfunc (m *Map) missLocked() { m.misses++ if m.misses \u0026lt; len(m.dirty) { return } m.read.Store(\u0026amp;readOnly{m: m.dirty}) m.dirty = nil m.misses = 0 } 这个函数主要做了两件事：\n递增 misses 如果 misses \u0026gt;= dirty 中的 key 总个数，说明出现了较严重的 cache miss，会影响 Store 和 Load 的性能，需要： 将 dirty 提升为 read 将原 dirty 置为 nil 从 Store 和 Load 的源码分析可以看出：sync.Map 还是比较 适合读多写少的场景，如果写比较多（造成主要的数据都在 dirty 中），会导致部分读取要先读 read，再读 dirty，性能不佳\nDelete func (m *Map) Delete(key any) { m.LoadAndDelete(key) // 实际上调用的还是 LoadAndDelete } func (m *Map) LoadAndDelete(key any) (value any, loaded bool) { read := m.loadReadOnly() e, ok := read.m[key] if !ok \u0026amp;\u0026amp; read.amended { // 如果 read 不存在，并且 dirty 不为 nil m.mu.Lock() read = m.loadReadOnly() e, ok = read.m[key] if !ok \u0026amp;\u0026amp; read.amended { // 双重检查 e, ok = m.dirty[key] delete(m.dirty, key) // 在 dirty 中删除 key // Regardless of whether the entry was present, record a miss: this key // will take the slow path until the dirty map is promoted to the read // map. m.missLocked() // misses++，进一步的，可能提升 dirty 为 read } m.mu.Unlock() } if ok { return e.delete() // 原子删除 } return nil, false } func (e *entry) delete() (value any, ok bool) { for { p := e.p.Load() if p == nil || p == expunged { return nil, false } // p 不为 nil，并且 p 之前没有被删除过 if e.p.CompareAndSwap(p, nil) { return *p, true } } } 总结 sync.map 是线程安全的 读取，插入，删除保持常数级的时间复杂度。 通过读写分离（read 负责读，dirty 负责读与写），降低锁时间 来提高效率，适用于 读多写少 的场景。 调用 Load 函数时，如果在 read 中没有找到 key，则会将 misses 值原子地增加 1，当 misses 增加到和 dirty 的长度相等时，会将 dirty 提升为 read，减少「读 miss」。 新写入的 key 会保存到 dirty 中，如果这时 dirty 为 nil，就会先新创建一个 dirty，并将 read 中未被删除的元素拷贝到 dirty。 ","permalink":"https://blogs.skylee.top/posts/go/map/sync.map/note/","tags":["Golang","并发编程"],"title":"Go: sync.Map"},{"categories":["Golang"],"content":"很多并发模型在涉及到线程间通信时，往往采用互斥锁来实现数据的安全性，但这种方式会消耗很多性能\nGo 采用的 CSP 并发模型提倡：通过 通信共享内存 而不是通过共享内存而实现通信。\n如果说 goroutine 是 Go 程序并发的执行体，channel 就是它们之间的连接。\nchannel 是可以让一个 goroutine 发送特定值到另一个 goroutine 的通信机制。\n通道像一个传送带或者队列，总是遵循先入先出（First In First Out）的规则，保证收发数据的顺序。\nchannel 类型 channel 是 Go 的一种类型，定义一个 channel：\nvar \u0026lt;变量名\u0026gt; chan \u0026lt;元素类型\u0026gt; 定义一个 channel 需要指定 channel 存储的元素类型：\nvar ch_int chan int // 元素类型为 int var ch_sl_int chan []int // 元素类型为 int 型的切片 初始化 channel 初始化一个 channel 需要使用 make 函数：\nch := make(chan \u0026lt;元素类型\u0026gt;, [缓冲区大小]) 缓冲区大小是一个可选项，默认为 0\n没有使用 make 初始化的 channel 的值为 nil\nchannel 的操作 ch := make(chan int, 2) ch \u0026lt;- 1 // 写数据到 channel ch \u0026lt;- 2 // 写数据到 channel fmt.Printf(\u0026#34;len(ch): %v\\n\u0026#34;, len(ch)) // 获取 ch 的元素个数 fmt.Printf(\u0026#34;cap(ch): %v\\n\u0026#34;, cap(ch)) // 获取 ch 的缓冲区大小 i := \u0026lt;-ch // 从 channel 读数据 fmt.Printf(\u0026#34;i: %v\\n\u0026#34;, i) \u0026lt;-ch // 从 channel 读数据，忽略结果 close(ch) // 关闭通道 ch \u0026lt;- 1 // panic 注意：\n不同于文件描述符，channel 可以被垃圾回收，这意味着我们可以不用必须关闭通道\n此外，如果要关闭通道，这个操作 一般由发送方执行\n关闭后的通道的特点：\n对一个关闭的通道再发送值就会导致 panic。 对一个关闭的通道进行接收会一直获取值直到通道为空。 对一个关闭的并且没有值的通道执行接收操作会得到对应类型的零值。 关闭一个已经关闭的通道会导致 panic。 channel 的缓冲区 无缓冲 func main() { ch := make(chan int, 0)\t// 缓冲区大小为 0 ch \u0026lt;- 1 fmt.Printf(\u0026#34;Success!\u0026#34;) } 上述代码可以编译通过，但是运行时会输出：\nfatal error: all goroutines are asleep - deadlock! goroutine 1 [chan send]: main.main() /Users/Sky_Lee/Documents/Go/Test/test.go:20 +0x28 exit status 2 发生了死锁！\n由于创建的是无缓冲的通道，向通道发送了数据后，却没有其它线程去接收\n无缓冲的通道只有在有接收方能够接收值的时候才能发送成功，否则会一直处于等待发送的阶段\n这样，当前线程就被阻塞了\n解决方法：可以创建一个 goroutine 去接收 channel 的数据：\nfunc recv(ch chan int) { i := \u0026lt;- ch fmt.Printf(\u0026#34;recv: i: %v\\n\u0026#34;, i) } func main() { ch := make(chan int, 0)\t// 缓冲区大小为 0 go recv(ch) // 创建一个 goroutine 去接收 channel 的数据 ch \u0026lt;- 1 fmt.Printf(\u0026#34;Success!\u0026#34;) } 也可以扩大缓冲区的大小，只要缓冲区没满，发送方就不会阻塞\n有缓冲 通过在 make 时指定 channel 的缓冲区大小：\nfunc main() { ch := make(chan int, 1)\t// 缓冲区大小为 1 ch \u0026lt;- 1\t// 不会阻塞 fmt.Printf(\u0026#34;Success!\u0026#34;) } 阻塞时机 通过上述分析，可以得出：\n向一个缓冲区已满的 channel 发送数据会阻塞 从一个缓冲区已空的 channel 接收数据会阻塞 可以利用无缓存的 channel 实现协程间同步：\nfunc f(ch chan int) { fmt.Printf(\u0026#34;In f...\\n\u0026#34;) time.Sleep(time.Second) // 让 f 的执行周期长一些 \u0026lt;-ch // 只有 f 从 channel 接收了数据，main 函数才能继续执行 } func main() { ch := make(chan int) // 缓冲区大小为 0 go f(ch) ch \u0026lt;- 1 fmt.Printf(\u0026#34;In main...\\n\u0026#34;) } 多返回值 从 channel 接收值时，怎么知道当前 channel 已经被关闭，从而结束接收数据呢？\n事实上，从 channel 接收值时，支持以下多返回值模式：\nv, ok := \u0026lt;-ch v： 从 channel 接收到的值 ok：布尔值，channel 是否关闭 基于此，可以编写循环读取 channel ，直到关闭：\nfunc f1(ch chan int) { for { v, ok := \u0026lt;- ch if !ok { // channel 已经关闭 fmt.Printf(\u0026#34;Channel closed.\\n\u0026#34;) break } fmt.Printf(\u0026#34;v: %v\\n\u0026#34;, v) } } func main() { ch := make(chan int) // 缓冲区大小为 0 go f1(ch) for i := 0; i \u0026lt; 10; i++ { ch \u0026lt;- i } // close(ch) // 不关也行，main 函数结束后，垃圾回收，也会关闭通道 fmt.Printf(\u0026#34;Success!\\n\u0026#34;) } 注意：不要使用 len(ch) 来判断通道是否关闭！\nch := make(chan int) // 缓冲区大小为 0 fmt.Printf(\u0026#34;(len(ch) == 0): %v\\n\u0026#34;, (len(ch) == 0)) // true, 但 ch 并没有关闭 for range 可以使用 for range 循环从 channel 接收数据：\nfunc f1(ch chan int) { for v := range ch { fmt.Printf(\u0026#34;v: %v\\n\u0026#34;, v) } fmt.Printf(\u0026#34;Channel closed.\\n\u0026#34;) } 单向 channel 在生产者消费者模型中，我们可能希望：\n生产者只向 channel 写数据 消费者只从 channel 读数据 这种场景下，就要使用到单向 channel：\n\u0026lt;name\u0026gt; \u0026lt;-chan \u0026lt;type\u0026gt; // 只接收通道 \u0026lt;name\u0026gt; chan\u0026lt;- \u0026lt;type\u0026gt; // 只发送通道 例如：\nvar ch1 \u0026lt;-chan int // 这里为了演示，没有给 channel 分配内存 \u0026lt;- ch1 // 正确 ch1 \u0026lt;- 1 // 错误，cannot send to receive-only channel ch1 利用单向 channel，实现一个简易的生产者消费者模型：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;sync\u0026#34; ) func producer(ch chan\u0026lt;- int) { defer wg.Done() // 生产 100 个数据 for i := 0; i \u0026lt; 100; i++ { ch \u0026lt;- i } close(ch) // 注意，这里需要主动关闭，否则消费者会一直等待，导致死锁 } func consumer(ch \u0026lt;-chan int, name string) { defer wg.Done() for v := range ch { fmt.Printf(\u0026#34;%s: recv data %d\\n\u0026#34;, name, v) } } var wg sync.WaitGroup func main() { ch := make(chan int, 5) // 1 个生产者，多个消费者 wg.Add(1) go producer(ch) for i := 0; i \u0026lt; 5; i++ { wg.Add(1) go consumer(ch, \u0026#34;consumer \u0026#34; + strconv.Itoa(i)) } wg.Wait() // 等待所有协程退出 } 不难看出：在函数传参时，双向 channel 可以直接赋值给单向 channel，但不能反向转换\nselect 多路“复用” 没有接触 select 前，如果想要从多个 channel 获取值，可能会这样写：\nfor { data0, ok0 := \u0026lt;- ch1 data1, ok1 := \u0026lt;- ch2 // ... } 但是这样的效率很低，如果任意一个操作阻塞，那么即使其它 channel 是可用的，也无法读取\nselect 就是用来解决这个问题的\nselect 是 Go 语言中用于处理并发通信的重要控制结构之一。select 语句用于在多个通道操作中选择一个可用的操作并执行它。它允许在不阻塞的情况下等待多个通道中的任何一个产生数据。\nselect { case v := \u0026lt;- ch0 // ch0 可以读取 // ... case ch1 \u0026lt;- 1 // ch1 可以写入 // ... case \u0026lt;- ch2\t// ch2 可以读取 // ... default // 默认分支，可以省略 } 结构与 switch 语句很像\nselect 语句的特点：\n可以同时处理一个或多个 channel 的操作 如果有多个分支满足，随机 选择一个分支执行 select 的执行流程如下：\n每一个 case 应该是通道的读/写操作 从上到下，从左到右执行所有的 case 从所有可用的 case 中 随机 选择一个 如果没有可用的 case，执行 default 分支 例如：\nvar channels = [3]chan int{ nil, make(chan int), nil, } var numbers = []int{1, 2, 3} func getNumber(i int) int { fmt.Printf(\u0026#34;numbers[%d]\\n\u0026#34;, i) return numbers[i] } func getChan(i int) chan int { fmt.Printf(\u0026#34;channels[%d]\\n\u0026#34;, i) return channels[i] } func test5() { select { case getChan(0) \u0026lt;- getNumber(0): fmt.Println(\u0026#34;The first candidate case is selected.\u0026#34;) case getChan(1) \u0026lt;- getNumber(1): fmt.Println(\u0026#34;The second candidate case is selected.\u0026#34;) case getChan(2) \u0026lt;- getNumber(2): fmt.Println(\u0026#34;The third candidate case is selected\u0026#34;) default: fmt.Println(\u0026#34;No candidate case is selected!\u0026#34;) } } 输出：\nchannels[1] numbers[1] channels[0] numbers[0] channels[2] numbers[2] No candidate case is selected! 元素值在经过通道传递时会被复制，那么这个复制是是浅拷贝还是深拷贝呢 发送操作包括了「复制元素值」和「放置副本到通道内部」这两个步骤。\n接收操作通常包含了「复制通道内的元素值」「放置副本到接收方」「删掉原值」三个步骤。\n那么这里的「复制」是浅拷贝还是深拷贝呢？\n做个实验看看：\n// 验证进入 channel 的过程是深拷贝还是浅拷贝 func test2() { a0 := []int{1, 2, 3, 4, 5} ch := make(chan []int, 1) ch \u0026lt;- a0 a0[0] = 6 a1 := \u0026lt;-ch fmt.Printf(\u0026#34;a1[0]: %v\\n\u0026#34;, a1[0]) // 如果为 6，说明为浅拷贝 } // 验证出 channel 的过程是深拷贝还是浅拷贝 func test3() { a0 := []int{1, 2, 3, 4, 5} ch := make(chan []int, 1) ch \u0026lt;- a0 a1 := \u0026lt;-ch a1[0] = 6 fmt.Printf(\u0026#34;a0[0]: %v\\n\u0026#34;, a0[0]) // 如果为 6，说明为浅拷贝 } 输出均为 6\n由于切片是引用类型，如果发生浅拷贝，那么对拷贝后的切片的修改会反映到原切片，这里输出为 6，正好符合浅拷贝\n这里插一嘴：Go 语言默认情况下，所有的拷贝都是浅拷贝\n浅拷贝相对深拷贝，拷贝时不需要重新分配大量内存，性能更好\n当然浅拷贝有些时候不是我们期望的，这个时候往往需要自己深拷贝\n","permalink":"https://blogs.skylee.top/posts/go/channel/basic/note/","tags":["Golang","并发编程"],"title":"Go: Channel"},{"categories":["Golang"],"content":"与传统的加锁控制并发的对比 在访问临界资源时，可以使用传统的加锁方式，但如果项目变得复杂，这种方式容易出现 bug，编码困难\nGo 使用 channel 配合 select 来做并发控制，使得编写并发代码变得很简单\n使用 channel 可以实现 goroutine 间通信，甚至可以将多个 channel 的数据汇总到一个 channel 中\n当然传统的加锁 + 共享内存也可以做到，但实现较复杂，我们不得不把精力放在如何编写「正确」的并发代码上，反而忽略了业务逻辑的处理\nGo 的并发原则非常优秀，目标就是简单：尽量使用 channel；把 goroutine 当作免费的资源，随便用。\nchannel 实现原理 下面简要分析一下 channel 实现原理，基于 Go 1.21.1\n数据结构 channel 的源码位于 /src/runtime/chan.go，结构定义如下：\ntype hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G\u0026#39;s status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex } 挑几个重要的说明一下：\nbuf：channel 的缓冲区（针对有缓冲的 channel，无缓冲的 channel 这个字段为 nil） closed：标记 channel 是否关闭 sendx、recvx：分别指向 buf 可发送位置、可接收位置 sendq、recvq：分别指向因发送阻塞的 goroutine 队列，以及因接收阻塞的 goroutine 队列 具体的，waitq 的定义如下：\ntype waitq struct { first *sudog last *sudog } waitq 实际上是一个双向链表，每一个元素都是 sudog 类型\nsudog 封装了 goroutine：\n// sudog represents a g in a wait list, such as for sending/receiving // on a channel. type sudog struct { g *g // goroutine next *sudog prev *sudog elem unsafe.Pointer // data element (may point to stack) // ... } 引用 什么是 channel 的一张图来加以说明：\n创建 channel 创建 channel，需要使用 make：\nch := make(chan struct{}, cap) 该方法底层会调用 makechan：\nfunc makechan(t *chantype, size int64) *hchan { var c *hchan // ... } 当我们分配不带缓冲区的 channel 时，makechan 仅会涉及一次内存分配\nc = (*hchan)(mallocgc(hchanSize+uintptr(size)*elem.size, nil, true)) 如果 channel 带缓冲区，那么就还要单独为 buf 分配一次内存：\n// 涉及两次内存分配 c = new(hchan) c.buf = newarray(elem, int(size)) // 为 buf 分配内存 接收数据 当我们尝试从 channel 接收数据时，根据是否带 ok ，底层分别调用 chanrecv1、chanrecv2 函数：\n// entry points for \u0026lt;- c from compiled code. // //go:nosplit func chanrecv1(c *hchan, elem unsafe.Pointer) { chanrecv(c, elem, true) } //go:nosplit func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) { _, received = chanrecv(c, elem, true) return } 其中：\nc 是指向 channel 的指针 elem 是指向待赋值元素的指针 如果成功从 channel 获取了值，那么 received 为 true\n无论哪个方法，都会调用 chanrecv 函数：\n// chanrecv receives on channel c and writes the received data to ep. // ep may be nil, in which case received data is ignored. // If block == false and no elements are available, returns (false, false). // Otherwise, if c is closed, zeros *ep and returns (true, false). // Otherwise, fills in *ep with an element and returns (true, true). // A non-nil ep must point to the heap or the caller\u0026#39;s stack. func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { // ... } 如果 channel 有可用数据： 如果 ep 为 nil，说明调用者忽略了读取的数据 返回 true，true 如果 channel 没有可用数据： 如果是非阻塞调用，返回 false，false 否则，阻塞当前 goroutine 如果 channel 已被关闭： 如果 ep 不为 nil，给 ep 赋上一个对应类型的 零值 返回 true，false 为什么有 block 参数？channel 还有非阻塞的吗？\n这种情况通常发生在使用 select 的情况下，比如：\nselect { case value := \u0026lt;-ch: // 从通道中成功接收到值 default: // 通道空，接收操作无法立即进行 } 在这种情况下，对于 ch 的读取操作，编译器就会转译成 chanrecv(c, \u0026amp;value, true)\n如何从 channel 读取数据？\n首先，怎么才能判断 channel 有数据呢？\n先来判断 sendq 是否为空\n如果不为空，只有两种情况：\nchannel 不带缓冲区 channel 带缓冲区，但是缓冲区满了 无论哪种情况，都说明 channel 有可用数据\n在 sendq 不为空的情况，实际上会调用 recv 函数：\nif sg := c.sendq.dequeue(); sg != nil { recv(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true, true } 那么 recv 做了什么事情呢？\n简单来说：\n如果是不带缓冲区的 channel：直接拷贝数据，从 sender goroutine -\u0026gt; receiver goroutine 如果是带缓冲区的 channel： 将 buf 队头的数据拷贝给 ep 将 sender 发送的数据追加到 buf 队尾 唤醒 sender func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { // ... // copy data from queue to receiver if ep != nil { typedmemmove(c.elemtype, ep, qp) } // ... } 如果 sendq 为空，就要看看 buf 是否为空：\n如果为空，说明 channel 没有可用数据，阻塞 如果不为空，说明 channel 有可用数据，根据 recvx 来确定读取位置，再读取数据 // buf 不为空 if c.qcount \u0026gt; 0 { // Receive directly from queue qp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) } if ep != nil { typedmemmove(c.elemtype, ep, qp) } typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- unlock(\u0026amp;c.lock) return true, true } 阻塞当前 goroutine，怎么实现的？\n直接上源码：\n// no sender available: block on this channel. gp := getg() // 获取当前 goroutine 的虚拟内存地址 mysg := acquireSudog() // 创建一个 sudog mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // 保存 ep 的地址到 mysg（sudog）中 mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil c.recvq.enqueue(mysg) // 进入到当前 channel 的 recvq 等待队列 // 阻塞当前 goroutine gp.parkingOnChan.Store(true) gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanReceive, traceBlockChanRecv, 2) 当前 goroutine 唤醒后，怎么读取 channel 的值呢？\n接着上源码：\n// someone woke us up（某个 goroutine 将我们唤醒） // 执行一些扫尾工作 if mysg != gp.waiting { throw(\u0026#34;G waiting list is corrupted\u0026#34;) } gp.waiting = nil gp.activeStackChans = false if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } success := mysg.success gp.param = nil mysg.c = nil releaseSudog(mysg) return true, success 似乎没有看到 recv 操作\n还记得前面提到的 保存 ep 的地址到 mysg（sudog）中 这个操作嘛？\n实际上，当一个 sender 发送数据时，如果 recvq 不为空，会将数据直接拷贝到 mysg.elem 中（这个过程下文会讲），也就相当于保存到了 ep 中\n发送数据 当我们尝试向一个 channel 发送数据，底层实际上调用了 chansend1 函数：\n// entry point for c \u0026lt;- x from compiled code. // //go:nosplit func chansend1(c *hchan, elem unsafe.Pointer) { chansend(c, elem, true, getcallerpc()) } chansend1 又会调用 chansend 函数（套娃了属于是）：\nfunc chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { // ... } 有了 chanrecv 的基础，理解 chansend 就比较简单了：\n如果 recvq 不为空，说明有 goroutine 正在等待，将数据直接拷贝到 recvq 的队头的 goroutine 如果 channel 不带缓冲区：将当前 goroutine 挂到 sendq，阻塞 goroutine 如果 channel 带缓冲区： 如果 buf 没满，那么将当前数据 append 到 buf 队尾 否则，将当前 goroutine 挂到 sendq，阻塞 goroutine 如果 channel 已经关闭，直接 panic 还是来看看源码：\n当 recvq 不为空，说明有 goroutine 正在等待，会调用 send 函数：\nif sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true } send 函数做了什么事？\nfunc send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { // ...省略 // 如果 sudog 的 elem 字段不为 nil // 将数据直接拷贝到 sudog 的 goroutine 的栈区 if sg.elem != nil { sendDirect(c.elemtype, sg, ep) sg.elem = nil } gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) sg.success = true if sg.releasetime != 0 { sg.releasetime = cputicks() } // 唤醒 goroutine goready(gp, skip+1) } 当 buf 足以存放新的数据，基于 sendx 来存放数据到 buf：\nif c.qcount \u0026lt; c.dataqsiz { // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled { racenotify(c, c.sendx, nil) } typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(\u0026amp;c.lock) return true } 如果阻塞发送数据，并且 recvq 为空，并且 buf 不够存放新的数据，阻塞当前 goroutine：\n// Block on the channel. Some receiver will complete our operation for us. gp := getg() // 获取指向当前 goroutine 的指针 mysg := acquireSudog() // 创建 sudog mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep // 保存 ep 的地址到 mysg（sudog）中 mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) // 将当前 goroutine 挂到 sendq 的队尾 // 阻塞操作 gp.parkingOnChan.Store(true) gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanSend, traceBlockChanSend, 2) // 防止 ep 被 GC KeepAlive(ep) 当前 goroutine 唤醒之后：\n// someone woke us up. // 两种情况： // 1. 接收数据 // 2. 关闭 channel if mysg != gp.waiting { throw(\u0026#34;G waiting list is corrupted\u0026#34;) } gp.waiting = nil gp.activeStackChans = false closed := !mysg.success gp.param = nil if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) // 检查当前 channel 是否被关闭 if closed { if c.closed == 0 { throw(\u0026#34;chansend: spurious wakeup\u0026#34;) } panic(plainError(\u0026#34;send on closed channel\u0026#34;)) // 如果被关闭，panic } return true 注意：当前 goroutine 唤醒之后，如果 channel 关闭了，会 panic 掉\n关闭 channel 关闭 channel 的过程就比较简单了：\n如果当前 channel 已经被关闭，或者为 nil，panic 给 closed 字段赋值为 1 唤醒所有的 reader（返回值为对应类型的零值） 唤醒所有的 sender（它们会 panic） 源码如下：\nfunc closechan(c *hchan) { if c == nil { panic(plainError(\u0026#34;close of nil channel\u0026#34;)) } lock(\u0026amp;c.lock) if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026#34;close of closed channel\u0026#34;)) } if raceenabled { callerpc := getcallerpc() racewritepc(c.raceaddr(), callerpc, abi.FuncPCABIInternal(closechan)) racerelease(c.raceaddr()) } c.closed = 1 var glist gList // release all readers for { sg := c.recvq.dequeue() if sg == nil { break } if sg.elem != nil { typedmemclr(c.elemtype, sg.elem) sg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } // release all writers (they will panic) for { sg := c.sendq.dequeue() if sg == nil { break } sg.elem = nil if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } unlock(\u0026amp;c.lock) // Ready all Gs now that we\u0026#39;ve dropped the channel lock. for !glist.empty() { gp := glist.pop() gp.schedlink = 0 goready(gp, 3) } } channel 进阶 发送与接收数据的本质 在 之前的文章 提到了一个问题：元素值在经过通道传递时会被复制，那么这个复制是是浅拷贝还是深拷贝呢\n那篇文章已经给出了解答：这个过程是 浅拷贝\n深入 channel 底层是这样回答的：\nRemember all transfer of value on the go channels happens with the copy of value.\nchannel 的发送和接收操作本质上都是浅拷贝，无论是从 sender 的栈到 chan buf，还是从 chan buf 到 receiver，或者是直接从 sender 到 receiver\nhappened before 给出维基百科的定义：\nIn computer science, the happened-before relation (denoted: → ) is a relation between the result of two events, such that if one event should happen before another event, the result must reflect that, even if those events are in reality executed out of order (usually to optimize program flow). This involves ordering events based on the potential causal relationship of pairs of events in a concurrent system, especially asynchronous distributed systems.\n翻译成中文就是：\n在计算机科学中，happened before（记作：→）是两个事件结果之间的一种关系，它指的是如果一个事件应该在另一个事件之前发生，那么结果必须反映这一点，即使这些事件在现实中是以不同顺序执行的（通常是为了优化程序流程）。这涉及到根据并发系统中事件对的潜在因果关系来对事件进行排序，特别是在异步分布式系统中。\n这点怎么在 Go 中体现呢？\n基于 happened before，我们可以使用 channel 实现 goroutine 之间的同步：\nvar ch chan struct{} var x int func init() { ch = make(chan struct{}) x = -1 } func write(val int) { time.Sleep(time.Second * 3) x = val } func read() int { return x } func main() { var wg sync.WaitGroup wg.Add(2) // 我们希望 read 在 write 完毕后执行 go func() { defer wg.Done() write(114514) ch \u0026lt;- struct{}{} }() go func() { defer wg.Done() \u0026lt;-ch fmt.Printf(\u0026#34;read(): %v\\n\u0026#34;, read()) }() wg.Wait() } 输出：\nSky_Lee@SkyLeeMacBook-Pro test % go build Sky_Lee@SkyLeeMacBook-Pro test % ./test read(): 114514 优雅关闭 channel 在分析 closechan 的源码时，提到了：关闭一个已经关闭的 channel，会 panic\n分析 chansend 的源码时，提到了：写入一个关闭的 channel，会 panic\n那么应该如何关闭 channel，避免 panic 呢？\n一种暴力的方法是使用 defer + recover：你 panic 呗，哥们直接给你 recover 掉\n但是这种方式不符合我们的「优雅」关闭\n这里给出关闭 channel 的原则：\ndon\u0026rsquo;t close a channel from the receiver side and don\u0026rsquo;t close a channel if the channel has multiple concurrent senders.\n也就是：\n不要在 receiver 关闭 channel 当有多个 sender 时，不要关闭 channel 实际上我们可以不用关闭 channel，当没有任何一个 goroutine 使用 channel 时，它会被 GC 掉\nchannel 的应用 停止信号 任务定时 控制并发数 停止信号\nfunc goroutineA(ch chan struct{}) { for range time.Tick(time.Second) { select { case \u0026lt;- ch: // 停止信号 fmt.Println(\u0026#34;goroutine A is stopped.\u0026#34;) return default: } fmt.Println(\u0026#34;goroutine A is running...\u0026#34;) } } func main() { ch := make(chan struct{}) go goroutineA(ch) time.Sleep(5 * time.Second) ch \u0026lt;- struct{}{} // 发送停止信号 time.Sleep(1 * time.Second) fmt.Println(\u0026#34;main goroutine is exiting...\u0026#34;) } 任务定时\nfunc main() { ch := time.Tick(time.Second) i := 0 for { \u0026lt;- ch // 1s 执行一次 fmt.Printf(\u0026#34;i = %v\\n\u0026#34;, i) i++ if i == 5 { break } } } 控制并发\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;time\u0026#34; ) var activeGoroutines int32 func init() { activeGoroutines = 0 } func work(id int) { atomic.AddInt32(\u0026amp;activeGoroutines, 1) defer atomic.AddInt32(\u0026amp;activeGoroutines, -1) // fmt.Printf(\u0026#34;goroutine-%d is working...\\n\u0026#34;, id) time.Sleep(time.Second * 1) } // 限流 func limitRate(id int, ch chan struct{}) { ch \u0026lt;- struct{}{} work(id) \u0026lt;-ch } func main() { limit := 10 ch := make(chan struct{}, limit) // 控制并发量为 10 for i := 0; i \u0026lt; 100; i++ { // 100 个 goroutine 并发执行 work go limitRate(i, ch) } // 每秒拉取一次活跃 goroutine 的数量 for range time.Tick(time.Second) { fmt.Printf(\u0026#34;Active goroutines: %d\\n\u0026#34;, atomic.LoadInt32(\u0026amp;activeGoroutines)) } } 参考资料 深度解密 Go 语言之 channel ","permalink":"https://blogs.skylee.top/posts/go/channel/core/note/","tags":["Golang","并发编程"],"title":"Go: Channel 底层机制"},{"categories":["Golang"],"content":"在 Go 服务中，每个传入的请求都在其自己的 goroutine 中处理。请求处理程序通常启动额外的 goroutine 来访问其他后端，如数据库和 RPC 服务。处理请求的 goroutine 通常需要访问特定于请求（request-specific context）的值，例如最终用户的身份、授权令牌和请求的截止日期（deadline）。\n当一个请求被取消或超时时，处理该请求的所有 goroutine 都应该快速退出（fail fast），这样系统就可以回收它们正在使用的任何资源。\nGo 1.7 引入一个 context 包，它使得跨 API 边界的请求范围元数据、取消信号和截止日期很容易传递给处理请求所涉及的所有 goroutine（显示传递）。\ntype Context interface { Deadline() (deadline time.Time, ok bool) Done() \u0026lt;-chan struct{} Err() error Value(key any) any } 集成 Context 到 API 中 一般来说，将 Context 集成到 API 中，需要将 API 的首参数作为 Context：\nfunc (d *Dialer) DialContext(ctx context.Context, network, address string) (Conn, error) 在此函数执行 Dial 操作的过程中，可以通过 ctx 来超时控制和取消\nContext.WithValue func WithValue(parent Context, key, val any) Context WithValue 返回父级的副本，其中与 key 关联的值为 val。\n仅将 Value 用于传输进程和 API 的请求范围数据，而 不是用于将可选参数传递给函数。\n提供的键必须是可比较的，并且 不应是字符串类型或任何其他内置类型，以避免使用上下文的包之间发生 冲突。\nWithValue 的用户应该 定义自己的键类型。\n为了避免在将值分配给一个 interface{} 类型时进行额外的内存分配（allocation），上下文键（context keys）常常采用具体类型 struct{}\ntype contextKey struct{} // 使用 struct{} 类型的键 func main() { ctx := context.Background() value := \u0026#34;some value\u0026#34; ctx = context.WithValue(ctx, contextKey{}, value) retrievedValue := ctx.Value(contextKey{}) fmt.Println(retrievedValue) } 在 Go 中，interface{} 是一个空接口，它可以容纳任何类型的值。当你将一个具体类型的值分配给 interface{} 类型时，Go 通常需要进行类型转换（type assertion），这可能会导致额外的内存分配和运行时开销。为了避免这些开销，上下文（context）包提供了一种 context.Context 类型，它使用了一种特殊的技巧，其中键（keys）通常采用具体类型 struct{} 而不是接口类型。\ncontext.WithValue 方法允许上下文携带请求范围的数据。这些数据必须是安全的，以便多个 goroutine 同时使用。\n这里的数据，更多是面向请求的元数据，不应该作为函数的可选参数来使用（比如 context 里面挂了一个 sql.Tx 对象，传递到 data 层使用），因为 元数据相对函数参数更加是隐含的 ，面向请求的。而参数是更加显示的。\n原理 context.WithValue 内部基于 valueCtx 实现:\nfunc WithValue(parent Context, key, val any) Context { if parent == nil { panic(\u0026#34;cannot create context from nil parent\u0026#34;) } if key == nil { panic(\u0026#34;nil key\u0026#34;) } if !reflectlite.TypeOf(key).Comparable() { panic(\u0026#34;key is not comparable\u0026#34;) } return \u0026amp;valueCtx{parent, key, val} } valueCtx 的相关定义如下：\n// A valueCtx carries a key-value pair. It implements Value for that key and // delegates all other calls to the embedded Context. type valueCtx struct { Context key, val any } // 递归的查找，从父节点寻找匹配的 key，直到 root context（Backgrond 和 TODO Value 函数会返回 func (c *valueCtx) Value(key any) any { if c.key == key { return c.val } return value(c.Context, key) } 小技巧：如果要传递的参数很多，可以传递一个 map，避免产生的查询链太长，影响效率\n安全 同一个 context 对象可以传递给在不同 goroutine 中运行的函数；上下文对于多个 goroutine 同时使用是安全的。\n对于值类型最容易犯错的地方，在于 context 的 value 应该是 不可修改（immutable） 的，每次重新赋值应该是新的 context，即:\n// pkg context func WithValue(parent Context, key, val any) Context 这种思想就是 写时拷贝 思想\n修改一个 Context Value 时，不能直接修改，先 deep-copy 一份，再生成一个新的 context 传给下游\n这样各自读取的副本都是自己的数据，写行为追加的数据，在 ctx2 中也能完整读取到，同时也不会污染 ctx1 中的数据。\nCancellation When a Context is canceled, all Contexts derived from it are also canceled\n当一个 context 被取消时，从它派生的所有 context 也将被取消。\nWithCancel(ctx) 参数 ctx 认为是 parent ctx，在内部会进行一个传播关系链的关联。\nDone() 返回 一个 chan，当我们取消某个 parent context, 实际上上会递归层层 cancel 掉自己的 child context 的 done chan，从而让整个调用链中所有监听 cancel 的 goroutine 退出。\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { gen := func(ctx context.Context) \u0026lt;-chan int { dst := make(chan int) go func() { n := 0 defer close(dst) for { select { // 防止 goroutine 泄漏 case \u0026lt;-ctx.Done(): fmt.Printf(\u0026#34;ctx.Err(): %v\\n\u0026#34;, ctx.Err().Error()) return case dst \u0026lt;- n: n++ } } }() return dst } ctx, cancel := context.WithCancel(context.Background()) dst := gen(ctx) defer cancel() for v := range dst { fmt.Printf(\u0026#34;v: %v\\n\u0026#34;, v) if v == 5 { break } } } TimeOut All blocking/long operations should be cancelable\n这点可以通过使用 context.WithDeadline 实现\n例如，将上面的代码加上超时时间\nfunc main() { gen := func(ctx context.Context) \u0026lt;-chan int { dst := make(chan int) go func() { n := 0 defer close(dst) for { select { // 防止 goroutine 泄漏 case \u0026lt;-ctx.Done(): fmt.Printf(\u0026#34;ctx.Err(): %v\\n\u0026#34;, ctx.Err().Error()) return case dst \u0026lt;- n: n++ } } }() return dst } ctx, cancel := context.WithDeadline(context.Background(), time.Now().Add(time.Millisecond)) dst := gen(ctx) defer cancel() for v := range dst { fmt.Printf(\u0026#34;v: %v\\n\u0026#34;, v) } } 使用原则 Do not store Contexts inside a struct type; instead, pass a Context explicitly to each function that needs it. Replace a Context using WithCancel, WithDeadline, WithTimeout, or WithValue Do not pass a nil Context , even if a function permits it. Pass a TODO context if you are unsure about which Context to use. TODO returns a non-nil, empty [Context]. Code should use context.TODO when it\u0026rsquo;s unclear which Context to use or it is not yet available (because the surrounding function has not yet been extended to accept a Context parameter).\nUse context values only for request-scoped data that transits processes and APIs, not for passing optional parameters to functions All blocking/long operations should be cancelable. Context.Value should inform, not control.（即不要使用 context.Value 来控制程序的流程） Try not to use context.Value. 示例 下面实现一个简易的 MySQL 客户端，使用 context 来实现查询的超时和取消操作。\n先来看看目录结构：\nSky_Lee@SkyLeeMacBook-Pro context_demo % tree . ├── context_demo ├── go.mod ├── main.go └── sql └── sql.go 2 directories, 4 files main.go 的内容如下：\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;context_demo/sql\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) var db *sql.DB func init() { var err error // 这里是一个演示，并不会真正连接数据库 db, err = sql.Open() if err != nil { log.Fatal(err) } } func main() { defer db.Close() http.HandleFunc(\u0026#34;/user\u0026#34;, handlerGetUser) log.Fatal(http.ListenAndServe(\u0026#34;:1145\u0026#34;, nil)) } // controller 层 func handlerGetUser(w http.ResponseWriter, r *http.Request) { ctx, cancel := context.WithTimeout(r.Context(), 3*time.Second) // 控制 queryUsers 的超时时间为 3s defer cancel() user, err := queryUsers(ctx) if err != nil { http.Error(w, \u0026#34;Failed to query users\u0026#34;, http.StatusInternalServerError) log.Printf(\u0026#34;Failed to query users, reason: %v\\n\u0026#34;, err.Error()) return } w.Write([]byte(user + \u0026#34;\\n\u0026#34;)) } // logic 层 func queryUsers(ctx context.Context) (string, error) { return SelectUser(ctx, db) } // dao 层 func SelectUser(ctx context.Context, db *sql.DB) (string, error) { query := \u0026#34;SELECT username FROM user LIMIT 1\u0026#34; // 示例查询语句 // 使用context控制超时时间 res, err := db.QueryWithContext(ctx, query) if err != nil { return \u0026#34;\u0026#34;, err } return res, nil } 为了方便，main.go 实现了 controller、logic、dao 层的内容\nmain.go 控制了单次查询的超时时间为 3s，并且通过 context.WithTimeout 控制了查询操作的超时时间\n再来看看 sql.go 文件：\npackage sql import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;time\u0026#34; ) var ( ErrTimeout = errors.New(\u0026#34;sql: timeout\u0026#34;) ) type DB struct { } // 使用 context 控制超时时间 // 返回 string 作为查询结果，仅仅是一个示例 func (db *DB) QueryWithContext(ctx context.Context, query string) (res string, err error) { // 先检查 context 是否超时 select { case \u0026lt;-ctx.Done(): return \u0026#34;\u0026#34;, ctx.Err() default: } ch := make(chan struct{}) go func() { defer close(ch) res, err = db.execute(query) }() // 异步检查查询执行完毕，或者 context 超时 select { // context 超时或者其它错误 case \u0026lt;-ctx.Done(): // 根据 query 取消查询 // 但取消的是查询本身，不是之前的 goroutine // 因此存在 goroutine 泄漏的风险 // 理想情况下，我们希望 cancelQuery 这类函数可以在数据库驱动层面上，使得 execute 返回，以避免 goroutine 泄漏 db.cancelQuery(query) switch ctx.Err() { case context.DeadlineExceeded: err = ErrTimeout default: err = ctx.Err() } res = \u0026#34;\u0026#34; // 查询执行完毕 case \u0026lt;-ch: } return } // 这里就涉及到了与 DB 交互了 // 返回类型简单设置为 string func (db *DB) execute(query string) (string, error) { // 模拟 RTT 为 10s time.Sleep(time.Second * 10) return \u0026#34;done\u0026#34;, nil } // 取消查询 func (db *DB) cancelQuery(query string) { // 一般来说，query 应包含一个 connectionID // 可以根据 connectionID 来关闭与 DB 的连接 // 从而取消查询 // do nothing // just a example } func Open() (*DB, error) { // do nothing // just a example return new(DB), nil } func (db *DB) Close() { // do nothing // just a example } sql.go 最重要的方法就是 QueryWithContext 了\nQueryWithContext 方法的实现，需要考虑 context 超时的情况：\n先检查 context 是否超时 异步执行查询 异步检查查询执行完毕，或者 context 超时 根据 context 超时或者其它错误，返回错误 查询执行完毕，返回结果 来看看执行结果吧：\n可以看到，即使 execute 模拟的查询时间为 10s，总查询时间也控制在了 3s\n注意：\n这个 demo 是存在 goroutine 泄漏的问题的：如果 execute 函数一直不返回，那么创建的 goroutine 永远不会被销毁\n实际的实现，避免 goroutine 泄漏，依赖 cancelQuery 方法，在数据库驱动层面让 execute 返回，从而避免 goroutine 泄漏\nContext 到底解决了什么？ Context should go away for Go 2 这篇文章的作者提到了：Go2.0 版本应该去掉 context，理由如下：\nContext is like a virus\n每个函数的第一个参数都是 context，即使你不使用它（传一个 context.ToDo），显得代码十分丑陋，可读性差\nContext is mostly an inefficient linked list\n并且，context.Value，内部实现实际上就是一个链表，查询的时间复杂度为 O(n)，如果调用链很长，那么效率也是很低的\n那么，Context 到底解决了什么问题？为什么要使用 Context？\nContext 提供了一种优雅的 cancellation，仅管它并不完美，但它确实很简洁地解决了问题。\n","permalink":"https://blogs.skylee.top/posts/go/context/basic/note/","tags":["Golang"],"title":"Go: Context"},{"categories":["Golang"],"content":"与数组最大的不同，切片支持动态扩容\n切片（Slice）是一个拥有相同类型元素的可变长度的序列。它是基于数组类型做的一层封装。它非常灵活，支持自动扩容。\n切片的内部结构包含地址、长度和容量。切片一般用于快速地操作一块数据集合。\n定义 语法 var name []type var sl []int 与数组的区别就是不需要指定元素的数量\n例子：\nvar sl0 []int // 定义 var sl1 = []int{} // 定义并初始化 var sl2 = []int{1, 2} var sl3 = []int{1, 2} fmt.Printf(\u0026#34;sl0: %v\\n\u0026#34;, sl0) fmt.Printf(\u0026#34;sl1: %v\\n\u0026#34;, sl1) fmt.Printf(\u0026#34;sl2: %v\\n\u0026#34;, sl2) fmt.Printf(\u0026#34;sl3: %v\\n\u0026#34;, sl3) fmt.Printf(\u0026#34;(sl0 == nil): %v\\n\u0026#34;, (sl0 == nil)) // true fmt.Printf(\u0026#34;(sl1 == nil): %v\\n\u0026#34;, (sl1 == nil)) // false // (sl2 == sl3) 错误，切片之间不能比较，只能和 nil 比较 注意：切片之间不能比较，只能和 nil 比较\n长度和容量 与数组不同，切片的长度和数量往往不一致，就像 std::vector 一样，这是切片的自动扩容机制决定的\n可以使用 cap() 函数获取切片的容量：\nvar sl = []int{1, 2} c := cap(sl) 切片表达式 切片表达式从字符串、数组、指向数组或切片的指针构造子字符串或切片。它有两种变体：\n指定 low 和 high 两个索引界限值的简单的形式，a[low:high] 除了 low 和 high 索引界限值外还指定容量的完整的形式，a[low,high,cap] 切片的 底层就是一个数组，所以我们可以基于数组通过切片表达式得到切片：\nvar arr = [5]int{1, 2, 3, 4, 5} sl := arr[1:3] // [low, high) fmt.Printf(\u0026#34;sl: %v\\n\u0026#34;, sl) // 2 3 fmt.Printf(\u0026#34;cap(sl): %v\\n\u0026#34;, cap(sl)) // 4 注意：不包含右边界\n为什么容量是 4？这个会在后面讲「切片的本质」时提到\n此外，还可以对一个切片再切片：\nvar arr = [5]int{1, 2, 3, 4, 5} sl := arr[1:3] // [low, high), [2 3] sl1 := sl[0:4] // 对 sl 切片，[2 3 4 5] fmt.Printf(\u0026#34;sl: %v\\n\u0026#34;, sl) fmt.Printf(\u0026#34;sl1: %v\\n\u0026#34;, sl1) fmt.Printf(\u0026#34;cap(sl): %v\\n\u0026#34;, cap(sl)) // 4 fmt.Printf(\u0026#34;cap(sl1): %v\\n\u0026#34;, cap(sl1)) // 4 注意，再次分片时：high 的上限是切片的容量，对于上面的例子，即 cap(sl) == 4\n完整的切片表达式 数组、指向数组的指针、切片支持完整切片表达式：\nvar arr = [4]int{1, 2, 3, 4} // 数组 sl := arr[0:4] // 切片 sl1 := arr[0:1:4] // [1], cap = 4 sl2 := sl[0:1:4] // [1], cap = 4 fmt.Printf(\u0026#34;sl1: %v\\n\u0026#34;, sl1) fmt.Printf(\u0026#34;cap(sl1): %v\\n\u0026#34;, cap(sl1)) fmt.Printf(\u0026#34;sl2: %v\\n\u0026#34;, sl2) fmt.Printf(\u0026#34;cap(sl2): %v\\n\u0026#34;, cap(sl2)) 而 string 不支持：\nvar s string = \u0026#34;123\u0026#34; sl3 := s[0:1] // 正确 sl4 := s[0:1:2] // 错误，字符串不支持完整切片表达式 完整切片表达式必须满足 0 \u0026lt;= low \u0026lt;= high \u0026lt;= max \u0026lt;= cap(a)，a 为底层数组或切片\n使用 make 函数构造切片 除了基于数组构造切片外，还可以使用 make 函数动态构造切片：\nmake([]T, size, cap) sl1 := make([]int, 4, 16) // size = 4，cap = 16 切片的本质 切片的本质实际上就是把底层数组做了一层封装，包含三个属性：\n底层数组的指针 size cap 例如，假设有一个数组 a := [8]int{0, 1, 2, 3, 4, 5, 6, 7}，切片 s1 := a[0:5]：\n切片 s2 := a[3:6]：\n这样应该可以理解为什么容量分别为 8、5 了\n判断切片是否为空 判断切片是否为空的唯一方式：len(s) == 0\n不能 使用 s == nil 判断：\nsl := []int{} fmt.Printf(\u0026#34;(sl == nil): %v\\n\u0026#34;, (sl == nil)) // false fmt.Printf(\u0026#34;(len(sl) == 0): %v\\n\u0026#34;, (len(sl) == 0)) // true // 但是 sl 实际上确实为空 此外，切片之间也 不能 相互比较：\n如果要比较两个切片，可以遍历两个切片来比较\n切片的拷贝 切片的拷贝可以理解为「浅拷贝」，也就是说：拷贝后的切片与源切片 共享同一个底层数组！\n这也许会产生意想不到的结果：\nsl0 := []int{1, 2, 3, 4} sl1 := sl0 // 引用赋值 fmt.Printf(\u0026#34;sl1[0]: %v\\n\u0026#34;, sl1[0]) // 1 sl0[0] = 666 fmt.Printf(\u0026#34;sl1[0]: %v\\n\u0026#34;, sl1[0]) // 666 因此，需要考虑业务需求，来选择是否需要深拷贝\n深拷贝切片需要用到 copy 函数：\ncopy(destSlice, srcSlice []T) 对于上面的示例，可以修改成：\nsl0 := []int{1, 2, 3, 4} sl1 := make([]int, 4) // size == cap == 4，可以访问 sl1[0] copy(sl1, sl0) fmt.Printf(\u0026#34;sl1[0]: %v\\n\u0026#34;, sl1[0]) // 1 sl0[0] = 666 fmt.Printf(\u0026#34;sl1[0]: %v\\n\u0026#34;, sl1[0]) // 1 注意，当目的切片没有足够空间（这里是 size，不是 cap）时，copy 不会拷贝后续的部分，并不会抛异常，这与 C++ 是不同的\nappend 方法 append 方法可以用于向切片添加元素：\nfunc append(slice []Type, elems ...Type) []Type 可以一次添加一个元素或多个元素，也可以直接添加一个切片：\nsl0 := []int{5, 6, 7, 8} // sl1 := []int{} // 没有必要初始化 var sl1 []int // 直接声明即可 sl1 = append(sl1, 1) sl1 = append(sl1, 2, 3, 4) sl1 = append(sl1, sl0...) // 注意 ... fmt.Printf(\u0026#34;sl1: %v\\n\u0026#34;, sl1) 为啥每次都要重新赋值，直接写 append(sl1, 1) 不行吗？\n主要原因还是因为切片的扩容机制决定的\nappend 时，若底层数组的 cap 不足以容纳新的元素时，切片会按照一定「策略」扩容，这会改变切片指向的底层数组\n由于切片的扩容机制，append 实际上每次返回的是一个新的切片（底层数组可能相同）\n因此，每次 append 都要用原变量来接受 append 的返回值\n为什么 nil 切片可以直接 append？\n因为 nil 切片可以通过 append 来扩容（申请一块内存），最终变成一个正常的 slice\n扩容策略 来看一下 Go 切片扩容的源码：\nnewcap := oldCap doublecap := newcap + newcap if newLen \u0026gt; doublecap { newcap = newLen } else { const threshold = 256 if oldCap \u0026lt; threshold { newcap = doublecap } else { // Check 0 \u0026lt; newcap to detect overflow // and prevent an infinite loop. for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; newLen { // Transition from growing 2x for small slices // to growing 1.25x for large slices. This formula // gives a smooth-ish transition between the two. newcap += (newcap + 3*threshold) / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap \u0026lt;= 0 { newcap = newLen } } } 先假设扩容后的 newcap 为 doublecap，即原 cap 的 2 倍 如果 append 后，切片的长度比 doublecap 还大，说明扩容 2 倍不能满足需求，直接使用 append 后的切片长度作为 newcap sl0 := []int{1, 2, 3} sl0 = append(sl0, 4, 5, 6, 7, 8, 9, 10) fmt.Printf(\u0026#34;len(sl0): %v\\n\u0026#34;, len(sl0)) // 10 fmt.Printf(\u0026#34;cap(sl0): %v\\n\u0026#34;, cap(sl0)) // 10 否则，说明扩容 2 倍能满足需求，但不能直接使用 doublecap，因为要减少无效内存占用（如果本身的容量已经很大，扩容 2 倍很有可能浪费内存），判断原容量与 threshold 的大小关系： 如果比 threshold 小，说明原来的 cap 还比较小，可以接受扩容 2 倍带来的潜在内存浪费 否则，计算 newcap，以平滑地过渡从 2 倍增长到 1.25 倍增长的情况。循环将 newcap 增加到一个新值，该值介于两者之间。 如果计算过程中，newcap \u0026lt;= 0，说明计算溢出了，为了安全起见，将 newcap 设置为新的长度（newLen）。 此外，切片扩容策略还有可能与切片元素类型有关，例如，int 和 string 类型的处理方式就不一样\n2024.3.13 补充：\n切片扩容还涉及到了 内存对齐，主要是为了保证 CPU 的访问性能\n// Specialize for common values of et.Size. // For 1 we don\u0026#39;t need any division/multiplication. // For goarch.PtrSize, compiler will optimize division/multiplication into a shift by a constant. // For powers of 2, use a variable shift. switch { case et.Size_ == 1: case et.Size_ == goarch.PtrSize: case isPowerOfTwo(et.Size_): default: } 因此，扩容后的实际容量还与与切片元素类型有关\n测试 示例 0\nfunc test0() { a1 := [7]int{1, 2, 3, 4, 5, 6, 7} fmt.Printf(\u0026#34;a1: %v (len: %d, cap: %d)\\n\u0026#34;, a1, len(a1), cap(a1)) s9 := a1[1:4] fmt.Printf(\u0026#34;s9: %v (len: %d, cap: %d)\\n\u0026#34;, s9, len(s9), cap(s9)) for i := 1; i \u0026lt;= 5; i++ { s9 = append(s9, i) fmt.Printf(\u0026#34;s9(%d): %v (len: %d, cap: %d)\\n\u0026#34;, i, s9, len(s9), cap(s9)) } fmt.Printf(\u0026#34;a1: %v (len: %d, cap: %d)\\n\u0026#34;, a1, len(a1), cap(a1)) // 想清楚这一行的输出 fmt.Println() } 输出：\na1: [1 2 3 4 5 6 7] (len: 7, cap: 7) s9: [2 3 4] (len: 3, cap: 6) s9(1): [2 3 4 1] (len: 4, cap: 6) # 在这里对 a1 修改 s9(2): [2 3 4 1 2] (len: 5, cap: 6) # 在这里对 a1 修改 s9(3): [2 3 4 1 2 3] (len: 6, cap: 6) # 在这里对 a1 修改 s9(4): [2 3 4 1 2 3 4] (len: 7, cap: 12) s9(5): [2 3 4 1 2 3 4 5] (len: 8, cap: 12) a1: [1 2 3 4 1 2 3] (len: 7, cap: 7) # s9 的底层数组就是 a1，对 s9 的修改会反映到 a1 上 因此，在必要的时候使用深拷贝\n示例 1\nfunc test2() { // 示例1。 s6 := make([]int, 0) fmt.Printf(\u0026#34;The capacity of s6: %d\\n\u0026#34;, cap(s6)) for i := 1; i \u0026lt;= 5; i++ { s6 = append(s6, i) fmt.Printf(\u0026#34;s6(%d): len: %d, cap: %d\\n\u0026#34;, i, len(s6), cap(s6)) } fmt.Println() // 示例2。 s7 := make([]int, 1024) fmt.Printf(\u0026#34;The capacity of s7: %d\\n\u0026#34;, cap(s7)) s7e1 := append(s7, make([]int, 200)...) fmt.Printf(\u0026#34;s7e1: len: %d, cap: %d\\n\u0026#34;, len(s7e1), cap(s7e1)) s7e2 := append(s7, make([]int, 400)...) fmt.Printf(\u0026#34;s7e2: len: %d, cap: %d\\n\u0026#34;, len(s7e2), cap(s7e2)) s7e3 := append(s7, make([]int, 600)...) fmt.Printf(\u0026#34;s7e3: len: %d, cap: %d\\n\u0026#34;, len(s7e3), cap(s7e3)) fmt.Println() // 示例3。 s8 := make([]int, 10) fmt.Printf(\u0026#34;The capacity of s8: %d\\n\u0026#34;, cap(s8)) s8a := append(s8, make([]int, 11)...) fmt.Printf(\u0026#34;s8a: len: %d, cap: %d\\n\u0026#34;, len(s8a), cap(s8a)) s8b := append(s8a, make([]int, 23)...) fmt.Printf(\u0026#34;s8b: len: %d, cap: %d\\n\u0026#34;, len(s8b), cap(s8b)) s8c := append(s8b, make([]int, 45)...) fmt.Printf(\u0026#34;s8c: len: %d, cap: %d\\n\u0026#34;, len(s8c), cap(s8c)) } 输出：\nThe capacity of s6: 0 s6(1): len: 1, cap: 1 s6(2): len: 2, cap: 2 s6(3): len: 3, cap: 4 s6(4): len: 4, cap: 4 s6(5): len: 5, cap: 8 The capacity of s7: 1024 s7e1: len: 1224, cap: 1536 # 理论计算结果为 1024 + (1024 + 256 * 3) / 4 = 1472，但却是 1024 的 1.5 倍，这也是内存对齐的结果 s7e2: len: 1424, cap: 1536 s7e3: len: 1624, cap: 2048 The capacity of s8: 10 s8a: len: 21, cap: 22 s8b: len: 44, cap: 44 s8c: len: 89, cap: 96 # 很好的体现了扩容策略中的「内存对齐」 删除切片的元素 Go 没有直接提供删除切片元素的方法，但是可以使用 append 来达到相同效果：\nsl0 := []int{1, 2, 3, 4, 5, 6} sl0 = append(sl0[0:1], sl0[2:]...) // 删除 idx = 1 的元素 fmt.Printf(\u0026#34;sl0: %v\\n\u0026#34;, sl0) // 1 3 4 5 6 传 slice 和 slice 指针有什么区别 Go 所有的函数传递，都是 值传递，没有引用传递\n因此，在 Go 中：\n传一个对象本身，在函数调用者看来，是不希望对象被函数改变的 传一个对象的指针，在函数调用者看来，是允许对象被函数改变的 对于 slice，也是适用这个规则，因为 slice 实际上是一个复合类型\n示例 0\nfunc myAppend(s []int) { s = append(s, 100) } func main() { s := []int{1, 1, 1} myAppend(s) fmt.Printf(\u0026#34;s: %v\\n\u0026#34;, s) } 输出：\ns: [1 1 1] 可以看到，myAppend 对 slice 的改变，并没有反映到原 slice\n正确的写法有两种：\n// 返回修改后的对象 func myAppend(s []int) []int { s = append(s, 100) return s } // 直接传递指针 func myAppendPtr(s *[]int) { *s = append(*s, 100) } func main() { s := []int{1, 1, 1} s = myAppend(s) fmt.Println(s) myAppendPtr(\u0026amp;s) fmt.Println(s) } 当然，你可能对接下来的示例有些疑惑：\n示例 1\n// 传值 func foo(s []int) { s[len(s) - 1] = 114514 } func main() { s := []int{1, 2, 3} foo(s) fmt.Printf(\u0026#34;s: %v\\n\u0026#34;, s) } 输出：\ns: [1 2 114514] 可以发现，即使传递的是 slice 副本，还是被修改了，似乎违背了 Go 只有值传递的事实，这是为什么？\n实际上，虽然传递的是 slice 的副本，但是 底层数组还是同一个，通过 [] 运算符修改，实际上操作的还是底层数组\n例如：\ntype Foo struct { A *int } func Bar(f Foo) { *f.A = 114514 } func main() { f := Foo{A: new(int)} *f.A = 1 Bar(f) // 传的是 f 的副本 fmt.Printf(\u0026#34;f.A: %v\\n\u0026#34;, *f.A) // 114514 } 这个示例的本质与示例 1 是一致的\n因此，当直接用切片作为函数参数时，可以改变切片的元素，不能改变切片本身\n如果想要在一个函数中修改 slice 本身，那么建议 传递 slice 的指针\n","permalink":"https://blogs.skylee.top/posts/go/slice/note/","tags":["Golang"],"title":"Go: Slice"},{"categories":["Golang"],"content":"介绍 Pool\u0026rsquo;s purpose is to cache allocated but unused items for later reuse, relieving pressure on the garbage collector. That is, it makes it easy to build efficient, thread-safe free lists. However, it is not suitable for all free lists.\nsync.Pool 是 sync 包的一个组件，用于 复用对象，避免频繁创建销毁对象带来的性能开销\nAPIs sync.Pool 一共有两个 API：\nGet Put Put 用于向 sync.Pool 存放一个对象\nGet 用于从 sync.Pool 取出一个对象\n如果 sync.Pool 为空，在 Get 的时候，会自动创建一个新的对象返回给调用者\n为了控制 sync.Pool 返回的新对象，可以在初始化 sync.Pool 时，指定 new 的方法\n使用基本步骤 下面是一个使用 sync.Pool 的基本步骤：\ntype Object struct { } var pool sync.Pool func init() { // 1. 在使用 pool 前，指定 new 的方式 pool.New = func() interface{} { return new(Object) } } func demo0() { // 2. 从 pool 中取出一个对象 // obj := new(Object) // 而不是调用 new 来创建 obj := pool.Get().(*Object) // 3. 使用 obj // ... // 4. 清空 obj // ... // 5. 将 obj 放回 pool，供下一次使用 pool.Put(obj) } 注意： sync.Pool 中存放的对象应该是 可重用的\n特点 sync.Pool 是 goroutine 安全 的，多个 goroutine 可以并发的使用 sync.Pool\nsync.Pool 无法保证对象不被清理，当触发 GC 时，会 清空 sync.Pool 内所有没有被引用的对象\nsync.Pool 的大小无法手动设置，大小取决于内存上限\nGet 方法取出来的对象和上次 Put 进去的对象实际上是同一个，Pool 没有做任何「清空」的处理。但我们不应当对此有任何假设，因为在实际的并发使用场景中，无法保证这种顺序，最好的做法是在 Put 前，将对象清空\nfmt 包是怎么使用 sync.Pool 的 直接看 fmt.Printf:\nfunc Printf(format string, a ...any) (n int, err error) { return Fprintf(os.Stdout, format, a...) } 内部调用了一个 Fprintf 方法，继续看看：\nfunc Fprintf(w io.Writer, format string, a ...any) (n int, err error) { p := newPrinter() p.doPrintf(format, a) n, err = w.Write(p.buf) p.free() return } 容易发现，首先先 new 了一个 Printer 对象，那么这个 new，是否每次都需要分配内存呢？来看看 newPrinter：\nfunc newPrinter() *pp { p := ppFree.Get().(*pp) // 从 pool 中取出一个 pp 对象 p.panicking = false p.erroring = false p.wrapErrs = false p.fmt.init(\u0026amp;p.buf) return p } 这个函数返回一个 p，而 p 的来源，就是 sync.Pool！\n再来看看 ppFree：\nvar ppFree = sync.Pool{ New: func() any { return new(pp) }, } 可以发现：ppFree 就是一个 sync.Pool，new 方法创建了一个 pp 对象\npp 对象包含了什么呢？\ntype buffer []byte // pp is used to store a printer\u0026#39;s state and is reused with sync.Pool to avoid allocations. type pp struct { buf buffer // arg holds the current item, as an interface{}. arg any // ...省略 } pp 对象主要包含了一个 buffer 对象，用做输出缓冲区\nfmt.Printf 可能在一个项目中会多次出现，如果每次调用 fmt.Printf 都要创建一个 pp 对象，那么内存分配总次数会大大增加\nfmt 包通过使用 sync.Pool，很大程度上减少了内存的分配次数！有效提升了性能\n我们再来看看 Fprintf 函数：\nfunc Fprintf(w io.Writer, format string, a ...any) (n int, err error) { p := newPrinter() // 内部使用 sync.Pool 来获取一个 pp 对象 p.doPrintf(format, a) n, err = w.Write(p.buf) p.free() // 内部将 pp 对象清空了，并将 pp 对象归还给 sync.Pool return } 重点关注 free 函数：\n// free saves used pp structs in ppFree; avoids an allocation per invocation. func (p *pp) free() { if cap(p.buf) \u0026gt; 64*1024 { p.buf = nil } else { p.buf = p.buf[:0] } if cap(p.wrappedErrs) \u0026gt; 8 { p.wrappedErrs = nil } p.arg = nil p.value = reflect.Value{} p.wrappedErrs = p.wrappedErrs[:0] ppFree.Put(p) // 将 pp 对象放回 sync.Pool } 可以看出，fmt 包对于 sync.Pool 的使用，是符合 之前介绍的步骤 的\n使用场景 对于很多需要重复分配、回收内存的地方，sync.Pool 是一个很好的选择。频繁地分配、回收内存会给 GC 带来一定的负担，而 sync.Pool 可以将暂时不用的对象 缓存 起来，待下次需要的时候直接使用，不用再次经过内存分配，复用 对象的 内存，减轻 GC 的压力，提升系统的性能。\n例如，标准库的 fmt 包，gin 框架对 context 的管理等等\nPool 里对象的生命周期受 GC 影响\n因此，sync.Pool 不适合用作连接池，因为连接池需要自己管理对象的生命周期\n示例 下面通过一个简单的示例，展示使用 sync.Pool 的基本方法，以及性能提升\nbuffer.go\npackage buffer type Buffer struct { data []byte } func NewBuffer(size int) *Buffer { return \u0026amp;Buffer{ data: make([]byte, 0, size), } } func (b *Buffer) Write(p []byte) error { b.data = append(b.data, p...) return nil // 这里忽略了错误处理 } func (b *Buffer) Read() (n []byte, err error) { res := make([]byte, len(b.data)) copy(res, b.data) return res, nil // 这里忽略了错误处理 } func (b *Buffer) Clear() { b.data = b.data[:0] } buffer_test.go\npackage buffer import ( \u0026#34;sync\u0026#34; \u0026#34;testing\u0026#34; ) func TestBuffer(t *testing.T) { b := NewBuffer(1024) if err := b.Write([]byte(\u0026#34;Hello, world!\u0026#34;)); err != nil { t.Errorf(\u0026#34;Error writing to buffer: %v\u0026#34;, err) } tmp, _ := b.Read() if string(tmp) != \u0026#34;Hello, world!\u0026#34; { t.Errorf(\u0026#34;Expected \u0026#39;Hello, world!\u0026#39;, got \u0026#39;%s\u0026#39;\u0026#34;, string(tmp)) } b.Write([]byte(\u0026#34;\\nGoodbye, world!\u0026#34;)) tmp, _ = b.Read() if string(tmp) != \u0026#34;Hello, world!\\nGoodbye, world!\u0026#34; { t.Errorf(\u0026#34;Expected \u0026#39;Hello, world!\\nGoodbye, world!\u0026#39;, got \u0026#39;%s\u0026#39;\u0026#34;, string(tmp)) } b.Clear() tmp, _ = b.Read() if string(tmp) != \u0026#34;\u0026#34; { t.Errorf(\u0026#34;Expected \u0026#39;\u0026#39;, got \u0026#39;%s\u0026#39;\u0026#34;, string(tmp)) } } func BenchmarkBuffer(b *testing.B) { op := func (buffer *Buffer) { buffer.Write([]byte(\u0026#34;Hello, world!\u0026#34;)) tmp, _ := buffer.Read() if string(tmp) != \u0026#34;Hello, world!\u0026#34; { b.Errorf(\u0026#34;Expected \u0026#39;Hello, world!\u0026#39;, got \u0026#39;%s\u0026#39;\u0026#34;, string(tmp)) } } for i := 0; i \u0026lt; b.N; i++ { buffer := NewBuffer(1024) op(buffer) } } func BenchmarkBufferWithPool(b *testing.B) { pool := sync.Pool{ New: func() any { return NewBuffer(1024) }, } op := func (buffer *Buffer) { buffer.Write([]byte(\u0026#34;Hello, world!\u0026#34;)) tmp, _ := buffer.Read() if string(tmp) != \u0026#34;Hello, world!\u0026#34; { b.Errorf(\u0026#34;Expected \u0026#39;Hello, world!\u0026#39;, got \u0026#39;%s\u0026#39;\u0026#34;, string(tmp)) } } for i := 0; i \u0026lt; b.N; i++ { buffer := pool.Get().(*Buffer) op(buffer) buffer.Clear() pool.Put(buffer) } } 基准测试结果如下：\nBenchmarkBuffer\nSky_Lee@SkyLeeMacBook-Pro test % go test -count=5 -benchmem -run=^$ -bench ^BenchmarkBuffer$ test/buffer goos: darwin goarch: amd64 pkg: test/buffer cpu: Intel(R) Core(TM) i7-7820HQ CPU @ 2.90GHz BenchmarkBuffer-8 4847030 247.0 ns/op 1040 B/op 2 allocs/op BenchmarkBuffer-8 4912086 240.2 ns/op 1040 B/op 2 allocs/op BenchmarkBuffer-8 4875544 245.5 ns/op 1040 B/op 2 allocs/op BenchmarkBuffer-8 4924796 246.3 ns/op 1040 B/op 2 allocs/op BenchmarkBuffer-8 4623854 244.6 ns/op 1040 B/op 2 allocs/op PASS ok test/buffer 7.365s BenchmarkBufferWithPool\nSky_Lee@SkyLeeMacBook-Pro test % go test -count=5 -benchmem -run=^$ -bench ^BenchmarkBufferWithPool$ test/buffer goos: darwin goarch: amd64 pkg: test/buffer cpu: Intel(R) Core(TM) i7-7820HQ CPU @ 2.90GHz BenchmarkBufferWithPool-8 19232821 53.97 ns/op 16 B/op 1 allocs/op BenchmarkBufferWithPool-8 21850822 54.03 ns/op 16 B/op 1 allocs/op BenchmarkBufferWithPool-8 20813865 53.75 ns/op 16 B/op 1 allocs/op BenchmarkBufferWithPool-8 20970561 55.24 ns/op 16 B/op 1 allocs/op BenchmarkBufferWithPool-8 19820340 54.02 ns/op 16 B/op 1 allocs/op PASS ok test/buffer 6.032s 首先，让我们看看不使用池的 BenchmarkBuffer：\n4847030 到 4924796 次迭代（不同的运行次数） 每次操作平均耗时大约 240 到 247 纳秒（ns/op） 每次操作分配 1040 字节（B/op） 每次操作进行 2 次内存分配（allocs/op） 现在，让我们看看使用了池的 BenchmarkBufferWithPool：\n19232821 到 21850822 次迭代（不同的运行次数） 每次操作平均耗时大约 53.75 到 55.24 纳秒（ns/op） 每次操作分配 16 字节（B/op） 每次操作进行 1 次内存分配（allocs/op） 从这些结果中，我们可以得出以下结论：\n使用内存池的版本 (BenchmarkBufferWithPool) 比不使用内存池的版本 (BenchmarkBuffer) 快得多。平均耗时从约 245 纳秒减少到约 54 纳秒，几乎是 4.5 倍的性能提升。 使用内存池的版本在内存分配上也更高效。每次操作的分配量从 1040 字节减少到 16 字节，减少了超过 98%。 使用内存池的版本的内存分配次数也减少了。每次操作的分配次数从 2 次减少到 1 次。 综上所述，使用内存池可以 显著提高性能和内存使用效率，这在需要 频繁分配和释放内存的场景 中非常有用，可以有效减少 GC 的压力，提升程序整体性能。\n源码分析 关于源码分析，可以看看这篇文章：深度解密 Go 语言之 sync.Pool ","permalink":"https://blogs.skylee.top/posts/go/sync.pool/note/","tags":["Golang","并发编程"],"title":"Go: sync.Pool"},{"categories":["Redis"],"content":" Redis 事务 Redis 事务是一个 原子操作，但不具备传统意义上的原子性\n基本使用 redis 的事务与传统关系性数据库的事务不同，没有 ACID 特性\n开启事务可以使用 MULTI 命令 回滚事务可以使用 DISCARD 命令 提交事务可以使用 EXEC 命令 为了防止多客户端的并发修改，可以在开启事务前使用 WATCH 命令监视要修改的键\nWATCH 命令用于监视一个或多个键，如果被监视的键在事务执行前被其他客户端修改，当前事务将被中止。\nWATCH 实际上是一种 乐观锁 机制\n局限性 不支持回滚 客户端与 Server 交互，是一条一条，而不是一批一批 Redis 事务支持回滚吗？ 若在事务队列中存在命令格式错误（类似于 java 编译性错误），则执行 EXEC 命令时，所有命令都不会执行 若在事务队列中存在其它错误（如网络错误、Redis 服务器本身出现问题、OOM），则执行 EXEC 命令时，所有命令都会执行，只不过部分成功，部分失败 因此，Redis 事务 不支持回滚，第一种情况只是看起来像回滚，但这实际上是代码编写的问题，当然可以“回滚”，不在本次问题的讨论范围\n为什么不支持回滚？\nRedis 官方 是这样说的：\nRedis does not support rollbacks of transactions since supporting rollbacks would have a significant impact on the simplicity and performance of Redis.\nRedis commands can fail only if called with a wrong syntax\n简单来说，就是 Redis 认为没有必要实现回滚，破坏了 Redis 的简单性，对性能也有显著影响\n此外，Redis 认为：Redis 的命令只会在出现语法错误的情况下，执行失败，这种错误是可以在开发的时候发现的，也没有必要实现回滚\n当然，错误的发生还有一种情况：事务执行过程中，Redis Server 宕机了，这种情况下，会出现事务执行了一半的情况吗？可以看看这个 讨论 Redis 本身就是用作缓存的，搞这么复杂干嘛\n如果你把 Redis 用作缓存，那么更新数据时，完全可以把缓存删掉（如果删除失败，可以引入补偿机制），不需要回滚的支持\nRedis 事务具有原子性嘛？ Either all of the commands or none are processed, so a Redis transaction is also atomic.\nRedis 官方认为：原子性指的是：一组命令，要么全部执行，要么全部不执行\n那么从这个角度来看，Redis 事务是 具有原子性的，在命令本身没有语法错误的情况下：\n执行 EXEC，命令 全部执行 执行 DISCARD，命令 全部不执行 维基百科认为 原子性 的定义是：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。\n从上面提到的 讨论 ，可以发现：一个事务执行过程中，如果 Redis 宕机，导致 AOF Log 事务部分不完整（被截断），那么 Redis 在重启时应用日志时会发现这个问题，并跳过整个事务执行部分\n如果我们认为事务执行过程中，唯一可能出现的错误就是 Redis 宕机（这符合 Redis 官方 的说法），那么，Redis 事务就满足维基百科定义的「执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态」\n当然，在实际使用过程中，可能会有其它因素（例如 OOM）导致一条命令执行失败，而不仅仅是宕机\n因此，这里给出的答案是：Redis 事务：\n具有 提交原子性：事务中的所有命令在一起提交，如果 EXEC 成功，则所有命令按顺序依次执行 执行 非原子性：事务中的每个命令独立执行，某个命令失败不会回滚之前已经成功执行的命令 总结 Redis 的事务是 原子操作，即在事务执行过程中，不会被其它事务干扰 Redis 事务不具备 CI 性质，这意味着多个事务之间是 没有隔离性 的，如果要保证多个事务之间的一致性和隔离性，需要利用 WATCH 乐观锁 Redis 事务成功提交后不支持回滚 如果事务执行过程中，Redis 宕机了，重启后会「回滚」到事务执行前的状态 Redis 事务满足 提交原子性，不满足执行原子性 Redis 性能优化 批量发送数据 传统的 C/S 交互方式，通常是：\nClient 发一条消息，等待 Server 接收到一条消息，处理 Server 将结果返回给客户端 在这种交互方式下，一条命令，在 Server 端就需要一次 read syscall 和一次 write syscall\n一条命令的情况还好，但是如果是很多命令，频繁的 syscall，会带来很大的开销\n可以想到的一种优化方式是 一次性发送一批数据\n在 Redis 中，经常使用 pipeline 来批量发送命令。在这种机制下，客户端发送完一条命令后，不是等待，而是接着发送第二条\n这样，在 Server 端，调用一次 read，就可以读取很多命令，调用一次 write，就可以发送很多响应，节省了很多系统调用的开销\nPipelining is not just a way to reduce the latency cost associated with the round trip time, it actually greatly improves the number of operations you can perform per second in a given Redis server. This is because without using pipelining, serving each command is very cheap from the point of view of accessing the data structures and producing the reply, but it is very costly from the point of view of doing the socket I/O. This involves calling the read() and write() syscall, that means going from user land to kernel land. The context switch is a huge speed penalty.\nWhen pipelining is used, many commands are usually read with a single read() system call, and multiple replies are delivered with a single write() system call.\nRedis 认为，因为高效的数据结构，以及内存操作，Redis Server 执行命令的过程开销实际上不是很大，真正昂贵的操作是频繁的 read/write syscall\ntransaction、pipeline、lua 三者区别 transaction 与 pipeline\n事务是原子操作，pipeline 是非原子操作。两个不同的事务不会同时运行，而 pipeline 可以同时 以交错方式 执行。 Redis 事务中每个命令都需要发送到服务端，而 Pipeline 只需要发送一次，请求次数更少。 这里做了一个小实验，验证 pipeline 是不是原子操作：\n先来看代码：\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) func main() { // InitRedis() var wg sync.WaitGroup wg.Add(100) for i := 0; i \u0026lt; 100; i++ { rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;127.0.0.1:6379\u0026#34;, Password: \u0026#34;\u0026#34;, DB: 0, }) pipe := rdb.Pipeline() // pipe := rdb.TxPipeline() key := \u0026#34;key\u0026#34; + strconv.Itoa(i) go func() { defer wg.Done() for i := 0; i \u0026lt; 1000; i++ { value := \u0026#34;value\u0026#34; + strconv.Itoa(i) pipe.Set(context.TODO(), key, value, -1) } _, err := pipe.Exec(context.TODO()) if err != nil { fmt.Println(err.Error()) } }() } wg.Wait() } 查看 aof 文件：\n*4 $3 set $5 key18 $6 value0 $7 keepttl ... 可以发现一个 set 命令占了 9 行\n假设 pipeline 是原子操作，那么 1000 个命令应该占用 9000 行，那么是不是这样呢？\n可以发现，从起始位置到结束位置之间的行数远大于 9000 行，说明 pipeline 不是原子操作\n那 Redis 事务是不是原子操作呢？这里使用 TxPipeLine API 来验证：\npipe := rdb.TxPipeline() 结果如下：\n可以发现，使用事务以后，开始到结束位置的行数确实为 9000 行，说明事务是 原子操作\nRedis 事务是怎么保证操作是原子的？\n当开启一个事务以后，接下来的所有操作都会被放到一个「独立」的队列中\n当调用 EXEC 命令时，Redis 会单线程地处理队列中所有的命令，只有当前事务的命令处理完毕以后，才会处理其它事务的命令\n而 PipeLine 实际上是一种 网络优化的手段，只是批量的发送数据到 Redis Server，没有隔离保障，其它客户端的数据可能与 pipeline 的数据混在一起\npipeline 与 lua 脚本\nUsing Redis scripting, available since Redis 2.6, a number of use cases for pipelining can be addressed more efficiently using scripts that perform a lot of the work needed at the server side. A big advantage of scripting is that it is able to both read and write data with minimal latency, making operations like read, compute, write very fast (pipelining can\u0026rsquo;t help in this scenario since the client needs the reply of the read command before it can call the write command).\n当涉及到需要快速执行并且依赖于前一个命令输出的场景时，Redis 脚本比管道更高效。例如，如果你需要读取一个键的值，基于这个值做一些计算，然后将结果写回到数据库，使用 Lua 脚本会是一个非常快速的解决方案。整个读取、计算和写入的过程在 Redis 服务器上作为一个单独、原子性的操作执行，客户端只需要发送脚本并接收最终结果。\n此外，lua 脚本在服务端执行，可以保证原子操作，这也是 pipeline 做不到的\n补充：如何在 Redis 中使用 lua 脚本？\n在 Redis 中使用 Lua 脚本的步骤相对简单。Redis 内置了对 Lua 脚本语言的支持，这使得你可以在 Redis 服务器上执行原子操作的复杂逻辑。Lua 脚本在 Redis 服务器端运行时，可以保证脚本内的多个命令作为原子操作执行，不会被其他客户端的命令中断。\n以下是在 Redis 中使用 Lua 脚本的具体步骤：\n编写 Lua 脚本\n首先你需要编写 Lua 脚本。一个简单的例子如下，这个脚本会检查一个键是否存在，如果不存在，则设置它的值：\nif redis.call(\u0026#34;EXISTS\u0026#34;, KEYS[1]) == 0 then redis.call(\u0026#34;SET\u0026#34;, KEYS[1], ARGV[1]) return \u0026#34;Key was set\u0026#34; else return \u0026#34;Key already exists\u0026#34; end 使用 EVAL 命令执行 Lua 脚本\n使用 EVAL 命令将 Lua 脚本发送至 Redis 服务器执行。EVAL 命令的第一个参数是 Lua 脚本的文本，第二个参数指定了脚本中的KEYS数组参数的个数，后续的参数是被脚本使用的 Redis 键和附加的参数。\n以前面的 Lua 脚本示例为例，你可以这样调用：\nEVAL \u0026#34;if redis.call(\u0026#39;EXISTS\u0026#39;, KEYS[1]) == 0 then redis.call(\u0026#39;SET\u0026#39;, KEYS[1], ARGV[1]) return \u0026#39;Key was set\u0026#39; else return \u0026#39;Key already exists\u0026#39; end\u0026#34; 1 your-key your-value 这里：\nyour-key 是 Lua 脚本中 KEYS[1] 的值。 your-value 是 Lua 脚本中 ARGV[1] 的值。 数字 1 指的是脚本中只访问一个键。 使用 EVALSHA 命令执行预加载的脚本\n如果你多次执行同一脚本，可以先使用 SCRIPT LOAD 命令加载脚本到 Redis 服务器并获取返回的 SHA1 校验和。之后可以通过 EVALSHA 命令和这个 SHA1 校验和来执行脚本，而不是每次都传输整个脚本代码：\nEVALSHA sha1-checksum 1 your-key your-value 总结 Redis 认为的原子性：一批操作，要么 执行，要么 不执行，且不会被其他客户端的命令中断 Redis 事务是一个原子操作，不支持回滚，不具备关系型数据库的原子性 pipeline（管道）更像是一种网络 IO 的优化手段，操作不具备原子性 lua 脚本结合了 Redis 事务与 pipeline 的优点，操作具备原子性，并且网络 IO 开销很低 bigkey 优化 bigkey 对 Redis 的影响主要在：\nRDB 持久化、AOF 重写：fork 时间会变长，阻塞 Redis 服务 网络 IO：假设一个 bigkey 的 value 大小为 1M，QPS 为 1k，那么 1s 产生的流量为 1G，直接把千兆网卡打满 删除操作：删除一个 bigkey，在内存还给 OS 后，OS 要对空闲内存管理，即将对应的页放到 free_list 中，如果页太多，这个过程也会比较耗费性能 如何优化？\n尽量不用 bigkey 如果要用，可以将其拆分成若干个子 key 删除时，不直接用 DEL，而是用 UnLink 异步删除，或者分批次删除 DEL 是一个阻塞操作，而 UnLink 只是将 key 从哈希表中移除（unlink），真正释放内存的操作是另外一个线程去 异步 地操作的，不会阻塞 Redis 的服务\nThis command is very similar to DEL: it removes the specified keys. Just like DEL a key is ignored if it does not exist. However the command performs the actual memory reclaiming in a different thread, so it is not blocking, while DEL is. This is where the command name comes from: the command just unlinks the keys from the keyspace. The actual removal will happen later asynchronously.\nhotkey 优化 热点 key 指的是 QPS 相对整个 keyset 来说较高的 key，如果不对热点 key 妥善处理，很有可能因为频繁访问将 Redis 服务干掉\n常见的优化方式：\n读写分离：可以部署多个从节点来分摊主节点的读压力 Redis Cluster：集群模式，保障可用性 多级缓存：应用程序可以实现自适应热点发现，将热点数据缓存到本地，避免访问 Redis 内存碎片问题 内存碎片这个问题并不陌生，在 OS 的内存管理就有遇到这个问题\n在 Redis 中也有内存碎片的问题，发生的原因主要有两种：\n申请内存，并不是按需申请\nTo store user keys, Redis allocates at most as much memory as the maxmemory setting enables (however there are small extra allocations possible).\n在申请 size 大小的内存时，通常还要额外申请一小部分内存，这不难理解：如果接下来要使用，就不需要在分配一次内存，直接用就可以（这个思想在 malloc 中也有体现）\n当然，如果接下来要申请的内存空间大于预分配的内存，那还是要申请的，这部分内存就成为了内存碎片（当然只要申请的内存小于等于某一块碎片，还是可以用的）\n释放内存，并不是立即释放\nRedis will not always free up (return) memory to the OS when keys are removed. This is not something special about Redis, but it is how most malloc() implementations work. For example, if you fill an instance with 5GB worth of data, and then remove the equivalent of 2GB of data, the Resident Set Size (also known as the RSS, which is the number of memory pages consumed by the process) will probably still be around 5GB, even if Redis will claim that the user memory is around 3GB. This happens because the underlying allocator can\u0026rsquo;t easily release the memory. For example, often most of the removed keys were allocated on the same pages as the other keys that still exist.\n这是因为删除的 key 所在的内存页，可能还有其它 key 的数据，那么这些页就不能归还给 OS\n如果这些页无法得到利用，就成为了内存碎片\n如何解决\n在 Redis 中，可以执行 info memory 查看内存使用情况\nroot@a6950a95dd7a:/data/appendonlydir# redis-cli -p 6379 127.0.0.1:6379\u0026gt; info memory # Memory used_memory:1546648 used_memory_human:1.47M used_memory_rss:4300800 used_memory_rss_human:4.10M ... mem_fragmentation_ratio:2.82 mem_fragmentation_bytes:2774824 mem_not_counted_for_evict:640 mem_replication_backlog:0 ... 其中 mem_fragmentation_ratio 的值可以反映内存碎片的情况\nmem_fragmentation_ratio = used_memory_rss (操作系统实际分配给 Redis 的物理内存空间大小)/ used_memory(Redis 内存分配器为了存储数据实际申请使用的内存空间大小) 因此，mem_fragmentation_ratio 越大，说明碎片率越高\n当 mem_fragmentation_ratio \u0026gt; 1.5 就可以考虑做碎片整理了\nmem_fragmentation_ratio 小于 1 是什么情况？\n这说明 Redis 实际使用的内存量大于 OS 分配给 Redis 的内存，通常来说是因为内存不足，发生了 swap\n启用碎片整理\n# # 1. This feature is disabled by default, and only works if you compiled Redis # to use the copy of Jemalloc we ship with the source code of Redis. # This is the default with Linux builds. # # 2. You never need to enable this feature if you don\u0026#39;t have fragmentation # issues. # # 3. Once you experience fragmentation, you can enable this feature when # needed with the command \u0026#34;CONFIG SET activedefrag yes\u0026#34;. # # The configuration parameters are able to fine tune the behavior of the # defragmentation process. If you are not sure about what they mean it is # a good idea to leave the defaults untouched. # Enabled active defragmentation # activedefrag no # Minimum amount of fragmentation waste to start active defrag # active-defrag-ignore-bytes 100mb # Minimum percentage of fragmentation to start active defrag # active-defrag-threshold-lower 10 # Maximum percentage of fragmentation at which we use maximum effort # active-defrag-threshold-upper 100 # Minimal effort for defrag in CPU percentage, to be used when the lower # threshold is reached # active-defrag-cycle-min 1 # Maximal effort for defrag in CPU percentage, to be used when the upper # threshold is reached # active-defrag-cycle-max 25 Redis 默认关闭了碎片整理，因为这个功能只有在使用 Jemalloc 分配内存才有效，并且碎片整理需要耗费一定性能\nRedis 生产问题 缓存雪崩、击穿、穿透与缓存一致性 可以查看 这篇文章 常见阻塞情况 RDB 持久化，AOF 重写时的 fork RDB 持久化，AOF 重写时的「写时拷贝」 AOF 的刷盘（fsync） bigkey 的修改 bigkey 的删除 慢查询 内存淘汰 Redis Server 压力大 主从复制：主进程需要将 RDB 快照发送给从节点，发送过程在其它客户端看来，Redis 服务是阻塞的 故障转移：在故障转移的过程中，写操作可能阻塞 ","permalink":"https://blogs.skylee.top/posts/redis/%E6%9D%82%E9%A1%B9/note/","tags":["Redis","Cache"],"title":"Redis 杂项"},{"categories":["Redis"],"content":" 为什么要使用 Redis 集群 前面的文章 提到了基于 哨兵集群 的「一主多从」模型\n这种模型实际上也是一种 Redis 集群，优点如下：\n实现数据的冗余备份 基于故障转移，可以保证一定程度的可用性 读写分离，提高整体读取性能 但是，这种模型存在一个严重的问题：只有一个主节点，所有的写请求都是直接发到主节点上，主节点压力很大，如果写请求 QPS 很高，很有可能导致主节点撑不住，然后挂掉\n针对这种情况，Redis 原生 支持了 Redis 集群模式（Redis Cluster），这种模式往往有多个主节点，并且每个主节点还有一些从节点来保证可用性\n下面的文章，会基于 Redis 官方文档 来简要介绍 Redis 集群\nRedis 集群实现了什么功能 High performance and linear scalability up to 1000 nodes. There are no proxies, asynchronous replication is used, and no merge operations are performed on values. Acceptable degree of write safety: the system tries (in a best-effort way) to retain all the writes originating from clients connected with the majority of the master nodes. Usually there are small windows where acknowledged writes can be lost. Windows to lose acknowledged writes are larger when clients are in a minority partition. Availability: Redis Cluster is able to survive partitions where the majority of the master nodes are reachable and there is at least one reachable replica for every master node that is no longer reachable. Moreover using replicas migration, masters no longer replicated by any replica will receive one from a master which is covered by multiple replicas. 简单来说，Redis Cluster 实现了：\n高性能：支持扩展至高 1k+ 节点 写入安全保障：在保证性能的同时，有一定的写入安全保障（可能会丢失一部分数据） 高可用：基于副本迁移，只要有足够的从节点，Redis 集群能保证服务的可用性 但是，在 Cluster 模式下，Redis 不支持 select 操作，这意味着只能有 DB0\n在 Redis Cluster 协议下，Server 与 Client 在 Redis 集群中的身份 To perform their tasks all the cluster nodes are connected using a TCP bus and a binary protocol, called the Redis Cluster Bus. Every node is connected to every other node in the cluster using the cluster bus. Nodes use a gossip protocol to propagate information about the cluster in order to discover new nodes, to send ping packets to make sure all the other nodes are working properly, and to send cluster messages needed to signal specific conditions. The cluster bus is also used in order to propagate Pub/Sub messages across the cluster and to orchestrate manual failovers when requested by users (manual failovers are failovers which are not initiated by the Redis Cluster failure detector, but by the system administrator directly)\nServer 与 Client 之间，通过基于 TCP 的 Redis Serialization Protocol 协议建立连接\n而所有的 Cluster 节点之间，通过 Cluster Bus 连接，节点之间使用 gossip 协议通信，目的是为了：\n发现新节点 检测彼此的健康状态（服务是否可用） 在节点间发送信号，提供所需的集群消息 每个节点都要与其它节点建立 TCP 连接，应该是 5 条边才对\n这里的图片，为了看起来不太乱，每个节点都少画了两条边\nThe client is in theory free to send requests to all the nodes in the cluster, getting redirected if needed, so the client is not required to hold the state of the cluster. However clients that are able to cache the map between keys and nodes can improve the performance in a sensible way.\n客户端向 Redis 集群发送读写请求时，只需要连接任意一个节点，如果要操作的数据不在这个节点上，请求会被重定向到目标节点上\n当然，为了避免重定向的性能开销，Redis 的客户端会在建立连接时，将数据与节点的映射关系缓存到本地\n如何减少数据丢失 Writes targeting the minority side of a partition have a larger window in which to get lost. For example, Redis Cluster loses a non-trivial number of writes on partitions where there is a minority of masters and at least one or more clients, since all the writes sent to the masters may potentially get lost if the masters are failed over in the majority side.\n一种情况就是在主节点向从节点同步增量数据时，主节点挂了，导致的数据丢失，比较随机，不太好避免 还有一种情况就是常说的「脑裂」现象，即由于网络问题，集群内其它节点误认为主节点挂了 于是执行故障转移，选出新的主节点，此时就有两个主节点了（当然，原先的主节点由于网络问题，无法感知）\n等到原先的主节点的网络恢复以后，集群会以为原先的主节点上线了，于是让原先的主节点降级为新的主节点的从节点\n那么数据丢失发生在哪里呢？\n集群误认为主节点宕机，这个过程是需要时间的\n对客户端来说，此时是不知道集群内部网络不好的，还是会正常发送写请求\n假设主节点收到了这些写请求，肯定要同步给从节点，但由于内部网络问题，无法完成同步\n如果内部网络迟迟不恢复，那么在完成同步之前，就选出了新的主节点，原来的主节点降级后，这部分增量数据就会丢失\n如何减少数据丢失？\nRedis 是这样做的：\nSpecifically, for a master to be failed over it must be unreachable by the majority of masters for at least NODE_TIMEOUT, so if the partition is fixed before that time, no writes are lost. When the partition lasts for more than NODE_TIMEOUT, all the writes performed in the minority side up to that point may be lost. However the minority side of a Redis Cluster will start refusing writes as soon as NODE_TIMEOUT time has elapsed without contact with the majority, so there is a maximum window after which the minority becomes no longer available.\n可以调整 NODE_TIMEOUT 参数\n假设集群检测到主节点疑似宕机的时间为 t0，主节点恢复时间为 t1\n如果 t1 在 t0 + NODE_TIMEOUT 前，那么不会有数据丢失，因为不会选举新的主节点 否则，根据与从节点的同步情况，会丢失未同步给所有从节点的增量数据 高可用 Redis Cluster 有 故障转移 的能力\n这个故障转移的过程与 基于哨兵的故障转移 过程很类似\n以 5 主 5 从（即每一个主节点都有一个从节点）为例，假设掉了两个节点，那么只有：\n掉了一个主节点 该主节点的从节点也挂了 这种情况，整个 Redis Cluster 服务才会不可用，概率为 (5*2)/(10*9) = 11%\n当然如果是 5 主 6 从（即有一个主节点具有两个从节点），情况就不一样了\nThanks to a Redis Cluster feature called replicas migration the Cluster availability is improved in many real world scenarios by the fact that replicas migrate to orphaned masters (masters no longer having replicas).\nSo at every successful failure event, the cluster may reconfigure the replicas layout in order to better resist the next failure.\nRedis 引入了 副本迁移 机制，在检测到 孤儿主节点 时会触发，将其它主节点的部分从节点迁移到孤儿主节点，进一步保证可用性\n高性能 Very high performance and scalability while preserving weak but reasonable forms of data safety and availability is the main goal of Redis Cluster.\nRedis 集群的主要目标是在保持数据安全性和可用性的同时，提供非常高的性能和可伸缩性。进一步的，以下几个因素保证了 Redis Cluster 的高性能：\n客户端缓存 key 与 node 的关系 异步复制 线性扩展能力（n 个主节点的集群的性能，可以看作单个主节点性能的 n 倍） 客户端缓存 key 与 node 的关系\n任意 Redis Cluster 节点收到 Client 的请求后：\n检查这个 key 的 value 是否在本节点上，如果在，那就由自己处理 否则，将客户端的请求正确重定向到目标节点 为了提高性能，客户端会在本地缓存 key 与 node 的映射关系（即一个哈希表），具体来说：\nkey：hash slots value：\u0026lt;node_ip, node_port\u0026gt; 例如，假设 clusterA 负责 hash slots 在 (114, 514) 的数据，那么客户端就会缓存像这样的内容：\nkey：114 ～ 514 value：\u0026lt;cluterA\u0026rsquo;s IP, cluterA\u0026rsquo;s Port\u0026gt; 那么，客户端在发起请求前，会先计算 key 对应的 hash slots，然后请求对应的节点，避免一次重定向\n当然，这个缓存可能会过期，此时就还是要依赖 cluster 的重定向了\nRedis 集群的基本组件 Key distribution model 实现数据的分片，依赖的就是 key 分发模型\nRedis 给整个集群分配了 16384 个哈希槽（hash slots），每个 cluster 节点都拥有一部分 hash slots\n确定 key 在哪个 hash slot，这个过程使用 CRC16 算法，具体来说：\nhash_slot = CRC16(key) % 16384 在客户端读写数据时：\n先计算出 hash slot 读取本地缓存，获取这个 hash slot 在哪个 cluster 节点上存储 请求对应的 cluster 节点 cluster 节点收到数据后：\n校验一下这个 key 是否在当前 cluster 节点 如果在，那么处理客户端的请求即可 如果不在，读取本地缓存，确定这个 hash slot 在哪个 cluster 节点上存储，并发送 MOVE 错误给客户端，以重定向到正确的 cluster 节点 Hash Tags There is an exception for the computation of the hash slot that is used in order to implement hash tags. Hash tags are a way to ensure that multiple keys are allocated in the same hash slot. This is used in order to implement multi-key operations in Redis Cluster.\n如果要存储一个 User 对象在 Redis 中，有可能像这样实现：\nuser:114514:name user:114514:age user:114514:sex ... 如果要获取一个 User 对象所有的成员的值，可能会这样做：\nMGET user:114514:name, user:114514:age, user:114514:sex 如果这些 key 不在同一个 cluster 中，那么操作就很麻烦了\n因此，为了 批量操作 key，就要使用 hash tags 了\nRedis Cluster 可以保证拥有相同的 hash tags 的 key，永远具有相同的 hash slots，这意味着会存储在相同的 cluster 节点中\n使用 hash tags 的语法如下：\n{\u0026lt;key_prefix\u0026gt;}\u0026lt;key_suffix\u0026gt; 这样，在计算 key 所属的 hash slots 时，就只会计算 {} 内部的部分，即 CRC16(key_prefix) % 16384\n对于上面的示例，可以这样设置 key：\n{user:114514}.name {user:114514}.age {user:114514}.sex Cluster 属性 redis-cli cluster nodes d1861060fe6a534d42d8a19aeb36600e18785e04 127.0.0.1:6379 myself - 0 1318428930 1 connected 0-1364 3886e65cc906bfd9b1f7e7bde468726a052d1dae 127.0.0.1:6380 master - 1318428930 1318428931 2 connected 1365-2729 d289c575dcbc4bdd2931585fd4339089e461a27d 127.0.0.1:6381 master - 1318428931 1318428931 3 connected 2730-4095 可以看到，一个 cluster node 具有以下基本属性：\n节点 id ip 和 port（或者 host） 身份：master or slave 该节点最后一次被 ping 的时间戳 该节点最后一次收到 pong 的时间戳 config 的版本号 负责的 hash slots Cluster bus Cluster bus protocol: a binary protocol composed of frames of different types and sizes. Every node is connected to every other node in the cluster using the cluster bus.\nCluster bus 基于 gossip 协议，集群中，每个节点的通信通过 Cluster bus 进行\nCluster bus 的作用 Nodes use a gossip protocol to propagate information about the cluster in order to discover new nodes, to send ping packets to make sure all the other nodes are working properly, and to send cluster messages needed to signal specific conditions. The cluster bus is also used in order to propagate Pub/Sub messages across the cluster and to orchestrate manual failovers when requested by users (manual failovers are failovers which are not initiated by the Redis Cluster failure detector, but by the system administrator directly).\n节点发现：集群节点使用集群总线来发现集群中的其他节点。 故障检测：集群总线用来交换定期的心跳消息，这些心跳包含故障检测消息，用于确定节点是否处于可达状态。 配置更新：当集群配置发生变化时（如节点加入或离开集群），通过集群总线传播这些变化。 投票和协商：在某些操作需要共识时（例如，当一个主节点无法达到时，选择一个副本来提升为新的主节点），集群总线用于投票和协商流程。 公共订阅/发布消息的传播：集群总线还用于在集群各节点中传播订阅/发布（pub/sub）消息。 总的来说，Cluster bus 就是提供一个 高效 的节点间交互方式\n开放的端口 Cluster bus 开放的端口默认为 RESP 开放的端口 + 10000\n举个例子，RESP，即 Cluster Node 与 Client 通信的端口默认为 6379，那么 Cluster bus 的端口就是 16379\n当然，也可以手动指定 cluster bus 的端口，修改 cluster-port 即可\ngossip 协议 Gossip 协议是一种在分布式系统中用于节点间通信的协议，它通过 流言蜚语（即 Gossiping）的方式来交换信息，从而实现信息的传播和一致性。\nGossip 协议与我们平常所说的流言传播相似，一个节点会 随机选择其他几个节点 分享信息，这些被选择的节点又会同样选择其他节点进行信息的传播，这样信息就 像病毒一样快速扩散开 来。\nGossip 的特点就是 去中心化，具有可伸缩性和鲁棒性\n为什么 Redis Cluster bus 要使用 gossip 协议呢？\n来看看 Redis 官方怎么说的：\nWhile Redis Cluster nodes form a full mesh, nodes use a gossip protocol and a configuration update mechanism in order to avoid exchanging too many messages between nodes during normal conditions, so the number of messages exchanged is not exponential.\n在一个大型集群中，如果节点需要以直接的方式交换状态信息或配置更新，那么每个节点都需要与其他每个节点进行通信。这意味着在 N 个节点的集群中，每个节点都需要发送 N-1 个消息来通知每个其他节点，总共就有 N*(N-1) 次通信。对于大型集群，这个数字会非常大，随着节点数目线性增加，需要的通信次数会以平方级别(N 的平方)增长\n如果使用 gossip 协议，每个节点发送数据，仅仅会随机选择一些节点发送，然后慢慢传播开来，就像流言（病毒）传播一样，这样，整个集群内的消息数量处于一个可控范围，不会造成太大的压力\n当然，gossip 协议存在 消息不及时 的缺点：消息在节点间传播是需要时间的，经过多个节点的转发，时效性不如直接发送好（不过大型集群，直接发送的压力太大，可能造成网络拥塞，时效性可能还不如 gossip）\n重定向与重新分片 MOVED Redirection 前面提到了，如果客户端请求的数据不在某个 cluster 节点上，cluster 会给客户端发送重定向，告诉客户端应该请求哪个节点\n事实上，重定向分为两种：\n临时重定向 永久重定向 这里先说说永久重定向\n永久重定向在 Redis Cluster 的语义为：该 key 的 hash slot 永久迁移到了另一个节点\n永久重定向会发生在两种情况：\n这个 hash slot 本来就不是由当前 cluster 节点负责 这个 hash slot 原本由当前 cluster 节点负责，但是由于某种原因，迁移到了另一个节点 cluster 节点发送永久重定向的过程：\n发送 MOVED message，包含了新的 cluster 的 ip 和 port 发送 cluster node 到 hash slots 的 映射关系（哈希表）给客户端 客户端会更新（或者存储）映射关系到本地，并重新发起请求到正确的 cluster node\nResharding 添加一个节点的步骤：\n添加一个新的节点：redis-cli --cluster add-node \u0026lt;new_host:new_port\u0026gt; \u0026lt;existing_host:existing_port\u0026gt; 分配散列插槽：redis-cli --cluster reshard \u0026lt;host:port\u0026gt; 其中，host:port 可以是集群中任意一个存在节点的 host:port reshard 的示例输出如下：\n$ redis-cli --cluster reshard 127.0.0.1:7000 \u0026gt;\u0026gt;\u0026gt; Performing Cluster Check (using node 127.0.0.1:7000) M: 50ae0f1a7e203a3fbb1a76a8af04b3f398e2f1e4 127.0.0.1:7000 slots:[0-5460] (5461 slots) master M: 2a0d96e5c41ac012b93c7c402769178d6ad8cfd3 127.0.0.1:7001 slots:[5461-10922] (5462 slots) master M: b59c1b9f4557d17a6e6c9c4e0fd607a72bda5e60 127.0.0.1:7002 slots:[10923-16383] (5461 slots) master How many slots do you want to move (from 1 to 16384)? 1000 What is the receiving node ID? Please type \u0026#34;help\u0026#34; if you need help. Node ID: 2a0d96e5c41ac012b93c7c402769178d6ad8cfd3 Please enter all the source node IDs. Type \u0026#39;all\u0026#39; to use all the nodes as source nodes for the hash slots. Type \u0026#39;done\u0026#39; once you entered all the nodes IDs and you want to start the rebalancing. Source node #1:50ae0f1a7e203a3fbb1a76a8af04b3f398e2f1e4 done Ready to move 1000 slots. Source node: 50ae0f1a7e203a3fbb1a76a8af04b3f398e2f1e4 Destination node: 2a0d96e5c41ac012b93c7c402769178d6ad8cfd3 Moving slot 1234 from 127.0.0.1:7000 to 127.0.0.1:7001: DONE ... ... Moving slot 1235 from 127.0.0.1:7000 to 127.0.0.1:7001: DONE ... ... (Multiple lines indicating progress of slot migrations) ... [OK] Done 可以看出，reshard 需要指定源 cluster node 和目标 cluster node，以及要迁移的 hash slot 的数量\n哈希槽的重分配过程如下：\n原节点设置为 MIGRATING 状态 新节点设置为 IMPORTING 状态 开始迁移哈希槽 迁移过程中，如果有新的客户端请求： 如果 key 存在，那么 在原节点执行 否则，原节点会将请求 重定向 到新的节点，在新的节点执行 迁移完成，新节点发送一个 OK 给原节点 原节点收到 OK 后，将迁移哈希槽的数据从本地 dataset 中删除 ASK Redirection 在 MOVED 永久重定向提了以下临时重定向\n临时重定向，主要在迁移哈希槽过程中使用\n迁移过程中，如果客户端执行的 key 在源节点不存在，那么就需要 临时重定向 到新节点来执行（没有必要在源节点执行，反正最终都要迁移到新节点）\n源 Cluster 节点在收到一个 key 后，检查对应的 hash slot，如果这个 hash slot 不是由当前节点负责，那么直接发送 MOVED，否则：\n如果这个 hash slot 正在迁移，那么检查这个 key 是否是新 key： 如果是新 key，那么 临时重定向 到新的节点 否则正常返回 否则正常返回 临时重定向，就是发送一个 ASK message 给客户端，将请求临时重定向到新的节点\n为什么不直接使用 MOVED 重定向？\nWhy can\u0026rsquo;t we simply use MOVED redirection? Because while MOVED means that we think the hash slot is permanently served by a different node and the next queries should be tried against the specified node. ASK means to send only the next query to the specified node.\n在迁移哈希槽的过程中，源节点负责的 hash slots 暂时还没有发生改变，如果发送 MOVED message，那么客户端更新映射缓存，后续请求旧 key 就会跑到新的节点，但是 迁移可能还没有完成，如果还要正确执行：\n再重定向到源节点 或者等待迁移完毕 无论哪种方式，效率都很低\n客户端连接和重定向处理 前面已经基本将客户端连接和重定向处理到过程介绍了，这里再总结一下：\n客户端首次执行命令，由于映射缓存为空，往往会收到 MOVED 重定向，这时，可以将映射关系缓存到本地 以后执行命令，先计算 key 的 hash slot，然后查缓存获取目标 cluster node，最后执行请求 但缓存可能部分过期（集群由于某些原因发生了 Reshard），那么请求可能有三种情况： 当前请求的 key 对应的 hash slot 没有迁移，正常执行 当前请求的 key 对应的 hash slot 正在迁移： 如果是新 key，那么收到 ASK 重定向 否则正常执行 当前请求的 key 对应的 hash slot 迁移完毕：收到 MOVED 重定向，更新缓存，重新执行请求 Redis Cluster 如何处理读写请求的分发 Normally replica nodes will redirect clients to the authoritative master for the hash slot involved in a given command\n写请求分发到主节点：\n在 Redis Cluster 中，所有的写操作（如 SET, DEL, HSET 等）必须 由负责该键哈希槽的 主节点 处理。如果客户端尝试向从节点或错误的主节点发送写请求，它将收到一个 MOVED 重定向错误，指示正确的主节点地址。客户端需要重定向请求到指定的主节点。\n读请求分发到从节点：\n与写请求一样，读请求（如 GET, HGET 等）默认情况下也是发送到主节点\n假设主节点为 A，A 的从节点为 B，假设 key0 对应的 hash slot 由 A 管理\n如果客户端向 B 发送一个读请求，B 会回应一个 MOVED 重定向错误，将客户端重定向到 A\n但在 Redis Cluster 中可以设置从节点进行读取操作，以此来分担主节点的读取压力。\nhowever clients can use replicas in order to scale reads using the READONLY command.\nREADONLY tells a Redis Cluster replica node that the client is willing to read possibly stale data and is not interested in running write queries.\n要实现这一点，客户端可以使用 READONLY 命令通知从节点接受读请求。一旦进入只读模式，客户端就可以向该从节点发送读请求，读取存储在该节点的数据。\n当客户端给从节点发送了 READONLY 命令后，从节点之后再收到读请求，只要这个 key 对应的 hash slot 由自己的 master 负责，那么就不会重定向客户端的请求，而是自己处理，降低 master 的压力\n注意：Redis Cluster 没有提供读请求的负载均衡（即将读请求均分到不同的从节点执行）\n这意味着：如果要实现读请求的负载均衡，需要由客户端自己实现\n实现方式也很简单：\n客户端可以在建立连接时，获取整个集群的状态（master 有哪些，其下的 slave 有哪些） 然后可以实现不同的负载均衡策略，如轮询、分片等等，将请求路由到对应的从节点 当然，在从节点执行读请求，会遇到数据不一致的问题，这个就要看你的业务是否有强一致性需求了\n错误忍受（Fault Tolerance） 心跳包（Heartbeat Packet） 为了检测整个集群的状态，节点间会定期地互相发送心跳包（Heartbeat Packet）\nRedis 集群的节点通过发送和接收 ping 和 pong 包来不断交换信息，这两种包的结构相同，携带重要的配置信息，唯一的区别在于消息类型字段。我们将 ping 和 pong 包的组合称之为心跳包。\n心跳包包含以下内容：\n类型：是 ping 还是 pong Node ID RESP 的 ip + port Cluster bus 的 ip + port 版本号 当前节点的身份信息 包含的哈希槽 如果是从节点，还会记录主节点的 Node ID 发送节点视角下的集群状态信息（包括 down 和 ok） 简单来说，就是包含了节点本身的元数据，以及发送节点视角下的集群状态信息\n这里详细说一下 发送节点视角下的集群状态信息，对于后面理解 Redis Cluster 的错误检测会有帮助：\n当一个节点发出心跳包时，它会在包里面包含它所观察到的集群状态的信息。这有助于其他节点获得关于集群健康状况的信息，比如：\n下线状态（down）：如果发送心跳包的节点观察到集群或者特定的节点出现了问题（比如无法达到或不再发送心跳信号），它会在心跳包中报告该节点或集群处于 down 状态。 正常状态（ok）：相反，如果发送心跳包的节点认为集群状态良好，所有节点都是活跃的并且响应心跳信号，那么它会报告集群状态是 ok 的。 这样的设计能让集群中的其他节点根据接收到的心跳信息来更新自己的状态视图，从而使整个集群能够对节点失效做出快速响应，并相应地进行故障转移或重组。\n发送机制\n每个节点每秒会 随机 ping 一些节点，这样每个节点发送的 ping 包数（及接收到的 pong 包数）是一个恒定的量，与集群中的节点数量无关。 每个节点都要确保 ping 那些超过一半的超时时间(NODE_TIMEOUT)还没有发送 ping 或接收 pong 的其他节点。在 NODE_TIMEOUT 时间到期之前，节点还会尝试重新连接 TCP 链路，以确保节点不因为当前 TCP 连接的问题而被认为是无法到达的。 举个例子：\n在一个有 100 个节点的集群中，如果节点超时时间设置为 60 秒，每个节点会尝试在 30s 内发送 99 个 ping 包，那么每秒就要发送 3.3 个 ping 包，整个集群 100 个节点，那么每秒就会产生 330 个 ping 包\n虽然看起来很多，但是这些包都是均匀的分配到每一个节点，不会造成太大的负担\n错误检测 Redis 集群使用故障检测来识别当一个主节点或副本节点无法被集群大多数节点访问时的情形，并进行相应处理，例如提升一个副本成为新的主节点。如果无法执行故障转移（没有可用从节点），集群将进入错误状态，停止接收客户端查询。\n节点会保存与其他节点相关的一系列标志。用于故障检测的有两个标志，称为 PFAIL（可能的故障）和 FAIL（确认故障）。PFAIL 是一个未被确认的故障类型，而 FAIL 意味着节点发生故障，并且这种状况已经被集群中大多数主节点在固定时间内确认。\n当节点超过 NODE_TIMEOUT 时间无法访问时，其他节点会给该节点标记 PFAIL。 无论是主节点还是副本节点，都可以为其它类型的节点标记 PFAIL。Redis 集群中节点不可达的概念是指我们发送了 ping，但在 NODE_TIMEOUT 时间内还未收到回复。\n一个节点单独的 PFAIL 标志只是该节点关于其他节点的本地信息，不足以触发故障转移。要视为节点已经 down 掉，需要将 PFAIL 状态升级为 FAIL。\n那么如何将 PFAIL 状态升级为 FAIL？\n升级需要满足三个条件：\n某节点（称为 A）将另一个节点（称为 B）标记为 PFAIL。 通过心跳检测机制，A 获取到了其它主节点对于 B 是否为 down 的意见 大多数的 有效（在 NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT 内报告 PFAIL 或者 FAIL 状态） 意见都认为这个节点为 down 那么：\n节点 A 将 B 标记为 FAIL 给其它节点发送 FAIL message FAIL message 会强制其它节点将 B 标记为 FAIL\n从节点选举和升级过程 当一个主节点被标记为 FAIL，并且有可用的从节点，那么故障转移可以进行\n故障转移的过程是这样的：\n首先，每个从节点都是新的主节点的候选者\n从节点会递增 currentEpoch（投票轮数）\n每个从节点会通过 cluster bus 发送 FAILOVER_AUTH_REQUEST 包，请求其它主节点给自己投票\n每个主节点在 NODE_TIMEOUT * 2 时间内，只有一次投票机会（防止多个从节点当选），当主节点收到 FAILOVER_AUTH_REQUEST 包后，如果有投票机会，就会给这个从节点投票（发送 FAILOVER_AUTH_ACK）\n从节点会 抛弃 不属于当前 currentEpoch 的投票\n当一个从节点收到大多数主节点的投票后，它就成为了新的主节点\n如果超过 NODE_TIMEOUT * 2 时间，还没投票完毕，本轮投票失败，等待 NODE_TIMEOUT * 4 开启新的一轮投票\n一旦选举出新的主节点，其他节点会被通知这一变更，确保整个集群中所有节点的配置信息保持一致。\n主节点重新加入集群的过程 A master node will change its configuration to replicate (be a replica of) the node that stole its last hash slot.\n当一个主节点因为某些原因（例如网络分区或者服务崩溃）丢失了其所有的哈希槽，并且这些哈希槽被集群中的其他主节点接管之后，该节点在重新加入集群时将不再担任之前的主节点角色。\n相反，它将更改其配置，成为接管了它 最后一个哈希槽的那个节点的副本\nIn general it may happen that A rejoins after a lot of time, in the meantime it may happen that hash slots originally served by A are served by multiple nodes, for example hash slot 1 may be served by B, and hash slot 2 by C.\n副本迁移（Replica migration） Intro 在 高可用 这里简单提了一下副本迁移的好处\n假设这种情况：\nmaster A 挂了，故障转移，slave A 成为新的主节点 slave A 也挂了，没有副本可用，整个集群服务不可用 为了进一步保证可用性，引入了「副本迁移」机制\n如果一个主节点没有任何子节点，我们将其成为孤儿主节点\n当集群中出现孤儿主节点，副本迁移机制就起作用了，它会将某一个主节点冗余的丛节点迁移到孤儿主节点，使其成为孤儿主节点的从节点：\nmaster A 挂了，故障转移，slave A 成为新的主节点 由于 slave A 为孤儿主节点，执行副本迁移，将 slaveC1 迁移，作为 slave A 的从节点 slave A 也挂了，故障转移，slave C1 成为新的主节点，集群仍然可用 Algorithm 副本迁移的基本算法如下：\n当一个从节点检测到集群中出现至少一个孤儿主节点，开始尝试副本迁移 发现孤儿主节点的从节点可能有很多，但采取行动的从节点只有一部分 采取行动的从节点是：从节点数量最多的主节点中，不处于 FAIL 状态且节点 ID 最小的从节点。 举个例子：\n如果上面的节点都不是 FAIL 状态，那么会执行迁移过程的从节点为 slave2\n可以通过配置 cluster-migration-barrier 来限制每个主节点最少具有的不处于 FAIL 状态的从节点数量\n参考资料 Redis cluster specification READONLY ","permalink":"https://blogs.skylee.top/posts/redis/%E9%AB%98%E5%8F%AF%E7%94%A8/%E9%9B%86%E7%BE%A4/note/","tags":["Redis","高可用","Cluster"],"title":"Redis 集群"},{"categories":["Redis"],"content":"Cache-Aside 应用代码需要首先在缓存中查找数据，如果未找到，再从数据库中加载数据，并将其添加到缓存中。\n当应用程序需要修改 DB 的数据，只需要先更新 DB，再删除 Cache\n优点是只缓存需要的数据，减少了缓存资源浪费。\n缺点是可能会有缓存穿透问题，导致无效的数据库访问增多。\n这种模式在业务上经常使用\nRead-Through 缓存与数据存储配合紧密，应用代码在读取数据时，如果缓存中不存在，缓存会自动负责从数据库加载数据并返回给客户端。\n优点是对应用透明，使得应用代码更为简洁。\n缺点是可能在初次访问数据时有延迟。\nWrite-Through 应用在更新数据时，会 同时写入缓存和数据库（分布式事务），确保缓存数据的一致性。\n优点是数据最新，缓存始终反映了数据库的当前状态。\n缺点是可能会增加写入的延迟。\nWrite-Back 应用写操作 只更新缓存，不立即写入数据库；数据会在特定条件下（如定时或批量）异步写入数据库。\n优点是能极大提高写入性能。\n缺点是存在数据丢失的风险，如果缓存在数据同步到数据库前出现故障。\n如何更新数据？ 强一致性 如果要强一致性保障，那就只能用 Write-Through + 分布式事务\n纯粹的 Write-Through 是无法保障强一致性，只能最终一致\n要想保证强一致性，就必须将 DB 的更新操作和 Cache 的更新操作整合成一个分布式事务\n最终一致性 如果不要求强一致性，可以使用 先更新 DB，再删除缓存 的方案，并辅以 补偿机制\n这种思想很简单：要更新数据，那就更新数据库呗，由于数据库的数据已经发生了变化，那么缓存的数据肯定就没有意义了，删除掉即可\n下一次读取数据，在客户端看来就是 cache miss，重建缓存即可\n但是存在一个问题：如果缓存删除失败，那么后续的请求就会读取到缓存的过期数据，就产生了不一致\n删除失败，重试不就好了，一般实现是将删除操作扔到消息队列，由消费者异步删除缓存：\n如果删除成功，提交 offset 否则不提交 offset，下一次消费的消息还是这一条，就实现了重试 这种方法保证的是最终一致性，在 DB 更新完成到成功删除缓存期间，是有数据不一致的情况的（但时间通常很短）\n题外话 不应该在如何实现缓存与 DB 的一致性这条道路上走火入魔\n想想自己为什么使用缓存？\n使用缓存的目的通常应该是：缓存 不经常 修改的数据，减少对 DB 的请求，降低 DB 压力，保证系统的可用性\n当然，也有 Write-Back 这种模式，缓存经常修改的数据，避免频繁写 DB\n具体可以看看这篇知乎上的高赞回答：Redis 双写一致性，先改数据库再改缓存可以吗？有什么问题？ 要不要用缓存？应该用哪种模式？是需要结合实际需求来确定的，数据一致性重要嘛？当然重要，但是你要考虑自己的业务到底有没有这么高的一致性需求，如果真的有很高的一致性要求，并且还经常修改数据，个人认为这种场景还不如直接用 DB\n","permalink":"https://blogs.skylee.top/posts/redis/%E7%BC%93%E5%AD%98/%E7%BC%93%E5%AD%98%E7%9A%84%E5%87%A0%E7%A7%8D%E6%93%8D%E4%BD%9C%E6%A8%A1%E5%BC%8F/note/","tags":["Redis","Cache"],"title":"缓存的几种操作模式"},{"categories":["Redis"],"content":"缓存雪崩 缓存雪崩指的是大量 key 几乎同时过期，导致大量请求直接打到 DB，导致 DB 服务不可用，进而导致其它服务也不可用，就像雪崩一样\n应对方式 给 Key 指定过期时间时，加上一个随机数\n例如，对于 comment:likeset: 这一类 key 来说，假设预期过期时间为 120s，那么在设置 key 的 TTL 时，就可以在 120s 的基础上 +- 一个随机数，使得过期时间有略微偏差，不至于同时过期\n缓存击穿 缓存击穿指的是热点 key（即访问很多的 key）过期，导致大量请求直接打到 DB，导致 DB 服务不可用，进而导致其它服务也不可用\n缓存击穿是缓存雪崩的 子集\n应对方式 除了缓存雪崩的应对方式外，对于缓存击穿，还可以使用「互斥锁」来应对\n具体来说，读取数据时，先判断缓存是否过期，如果过期：\n加锁 再次判断缓存是否过期： 如果过期，读 DB，重建缓存 否则，读缓存 解锁 这样，就只有一个线程真正执行了读 DB 的操作\n例如，在 Go 的并发编程中，常常使用 singleflight 来防止缓存击穿\n缓存穿透 缓存穿透指的是请求 不存在的数据，由于数据不存在，必然 cache miss，导致请求直接打到 DB\n如果有恶意请求，随机生成一些 query condition，那么大量的请求打到 DB，导致 DB 服务不可用，进而导致其它服务也不可用\n应对方式 通常有两种方式防止缓存穿透：\n缓存空对象 布隆过滤 缓存空对象的思想是：如果访问了一个不存在的数据，那么将这个查询条件缓存起来，下一次请求就先判断数据是否存在，存在才访问 Cache、DB\n这种方式无法解决恶意请求的问题，因为查询条件是随机的\n布隆过滤器可以将存在的对象「放到」过滤器中，查询时，先根据布隆过滤器判断数据是否存在，存在才访问 Cache、DB\n那么，如果将一个对象放到布隆过滤器中？\n事实上：布隆过滤器使用了 位图 + hash function：\n先使用「多个」不同的哈希函数计算对象的哈希值 计算存储到位图的位置：hash_val % sizeof(bitmap) 在判断数据是否存在时：\n使用「多个」不同的哈希函数计算对象的哈希值 计算存储到位图的位置：hash_val % sizeof(bitmap) 如果每一个位置都在位图中存在，那么认为这个对象存在，否则认为不存在 图片来自小林 Coding 布隆过滤存在 误判 的可能，因为存在哈希冲突，不同对象可以有相同的哈希值\n举个例子：\n对象 A 的哈希值为 1、2、3 对象 B 的哈希值为 2、3、4 对象 C 的哈希值为 1、2、4 假设 A、B 在 DB 中存在，C 不存在，且位图的大小为 4\n那么根据哈希值，可以得到位图为 1、2、3、4\n由于 C 的哈希值为 1、2、4，每一位都在位图中存在，因此，布隆过滤器认为 C 存在，这就产生了误判\n为了降低误判几率，可以增加哈希函数的数量，以及位图的大小\n","permalink":"https://blogs.skylee.top/posts/redis/%E7%BC%93%E5%AD%98/%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F/note/","tags":["Redis","Cache"],"title":"缓存雪崩、缓存击穿、缓存穿透"},{"categories":["Redis"],"content":"介绍 Redis 主从复制（同步）是指将一个 Redis 服务器的数据复制（同步）到多个 Redis 服务器的过程。这样，当主服务器出现故障时，其他从服务器可以接管其工作，从而保证服务的稳定性。\n第一次同步的过程 从节点第一次同步主节点的数据的过程如下：\n从节点向主节点发起同步请求(PSYNC)，并携带主服务器的 server_id（这里为？）、offset（这里为 -1） 主节点收到请求后，发送自己的 server_id 和 offset 给从节点 从节点记录下主节点的 server_id 和同步的 offset 主节点 fork 一个子进程后台生成 RDB 文件，发送给从节点 从节点收到 RDB 文件后，开始同步主节点数据 这个过程存在一个问题：在生成 RDB 期间，如果有新的写命令，是不会包含在 RDB 中的，那么从节点如何获取这些增量数据呢？\n为了解决这个问题：\n主进程会在生成 RDB 期间，将 增量写命令 写到 replication buffer 中 从节点同步完 RDB 以后，会给主节点发送同步完成的消息 主节点收到同步完成的消息以后，会将 replication buffer 的命令发给从节点 从节点重放 replication buffer 的命令，第一次同步完成 多个从节点同时发起 SYNC 请求，Redis 如何处理？\nSYNC 请求的到达肯定有个先后顺序\n主节点收到第一个 SYNC 请求，会执行上述步骤\n接下来的 SYNC 请求，会被缓存到一个队列中，当 RDB 生成完了以后，发给剩余的从节点\n这里引用小林的 一篇文章 的图片来展现整个过程：\n第一次同步结束后的过程 第一次同步结束后，从节点与主节点会建立 TCP 长连接，用于 同步实时写命令\n当主节点执行了一个写命令后：\n主节点将该命令添加到 repl_backlog_buffer 中； 主节点通过已经建立的 TCP 长连接，将这个写命令发送给所有的从节点； 从节点接收到这个命令后，会将其放入自己的本地队列中，然后按序执行这个命令来更新自己的数据集； 主节点在发送命令后不会等待从节点的响应，它会继续处理自己的操作请求。 这种同步的方式叫做「基于长连接的命令传播」\n推 VS. 拉 在 Redis 的主从复制机制中，主要采取的是 从节点主动拉取数据 的方式。\n虽然主节点在一定程度上向从节点推送数据（例如实时写命令），但整个同步过程始终是从节点发起并控制的。\n因此，Redis 的复制机制可以概括为“从节点拉取数据，而主节点推送增量更新”的混合模式，但主体过程是由从节点拉取数据驱动的。\n增量同步 如果从节点到主节点的网络发生了 延迟，那么从节点无法及时获取到主节点最新的数据\n当网络恢复以后，从节点的同步过程如下：\n从节点向主节点发起同步请求(PSYNC)，并携带主服务器的 server_id（这里为第一次同步获得的 server_id）、offset（这里为同步的进度）\n主节点收到同步请求，如果：\nserver_id 与自己的 id 一致 offset 对应后续的数据仍存在 repl_backlog_buffer 中 就会执行 增量同步，不会生成新的 RDB 文件，而是直接将 repl_backlog_buffer 中要同步的数据发给从节点\nrepl_backlog_buffer 是一个 环形 的缓冲区，一个主节点只有一个 repl_backlog_buffer，默认大小为 1M，如果写满了，就会从头开始写数据（即覆盖原数据）\n如果覆盖了从节点要同步的数据，就只能走全量同步了\n因此，如果从节点同步速度比较慢，可以适当增加 repl_backlog_buffer 的大小，防止全量同步\n举个例子，如果主节点每秒产生 1M 数据，从节点同步 1M 数据需要 5s 时间，那么 repl_backlog_buffer 至少要大于 5M 才能避免全量同步\n为了应对突发情况，可以设置为此基础上的 2 倍\n如何判断一个节点是否正常工作 主从节点互相基于「心跳检测」机制来判断对方是否下线\n如果半数以上的节点 ping 一个节点都没有 \u0026ldquo;pong\u0026rdquo; 回应，redis 集群会认为这个节点客观下线，会断开与这个节点的连接\n而主节点与从节点的 ping 的时间间隔也有所不同：\n主节点默认每隔 10s（repl-ping-slave-period） 会向从节点发送 ping，判断从节点的健康情况 从节点默认每隔 1s 向主节点发送 replconf ack{offset} 命令，上报自己的 offset 情况，可以： 检测主节点是否掉线 自己的同步进度是否落后 如何应对主从不一致 首先需要保证主从之间的网络情况比较好，可以将主从节点部署在同一个机房中，走内网通信\n此外，可以监控从节点的同步情况，如果落后比较多，那么就不在这个从节点上读取数据，防止读到过期数据\n数据丢失 主节点宕机\n考虑这种情况：\n主节点正在发送增量数据给从节点 在发送过程中，主节点宕机 Redis 哨兵发现主节点宕机，于是执行故障切换，选出新的主节点 原来的主节点上线，降级为从节点，向新的主节点同步数据 这个过程发生了 数据丢失\n从节点还没有收到增量数据，主节点就挂了，那么整个集群就会丢失这一部分增量数据\n主节点伪宕机\n这种情况与上面的有所不同，主节点并不是真的宕机，而是由于网络延迟，被哨兵误认为宕机了\n主节点正在发送增量数据给从节点 由于网络延迟，从节点未能成功收到增量数据 由于长时间的网络延迟，哨兵集群认为主节点客观下线，于是执行故障切换，选出新的主节点 然后，网络恢复了，整个集群就出现了两个主节点，这就是常说的「脑裂」现象\n由于已经选出了新的主节点，原来的主节点会被降级成从节点，向新的主节点同步数据，于是增量数据就丢失了\n如何 减小 脑裂带来的数据丢失\nRedis 中有两个参数：\nmin-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会 禁止写数据。 min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果主从同步的延迟超过 x 秒，主节点会 禁止写数据。 当发生网络延迟，主节点会 误认为从节点下线了（ping 不通），会禁止写数据\n此外，由于网络延迟，主从复制的延迟也会增加，当延迟超过 min-slaves-max-lag，也会禁止写数据\n合理配置这两个参数，可以有效 减少 丢失的数据\n这里一直在强调是 减少 丢失数据，而不是 避免 丢失数据\n因为检测到从节点下线，或者主从延迟超过 min-slaves-max-lag，这个过程是需要时间的\n如果这段时间内，主节点写入了新的数据，并且网络迟迟不恢复，那哨兵还是进行故障转移，这些数据还是会丢失\n如何实现主节点宕机后的故障转移 故障转移的过程是由哨兵集群来实现的，具体内容可以查看 这篇文章 ","permalink":"https://blogs.skylee.top/posts/redis/%E9%AB%98%E5%8F%AF%E7%94%A8/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%90%8C%E6%AD%A5/note/","tags":["Redis","高可用"],"title":"Redis 主从复制（同步）"},{"categories":["Redis"],"content":"为什么要有哨兵 Redis 哨兵主要负责：\n集群健康状态监测 集群故障转移 通知 可以说，Redis 的高可用离不开哨兵\n如何判断主节点故障 主观下线\n前面提到，哨兵具有集群健康状态监测的功能\n每个哨兵会每隔 1s 给每个节点发送 Ping 命令，如果某个节点在规定时间内（down-after-milliseconds 参数决定）没有响应，那么这个节点会被认为下线了\n这个就叫做主观下线：哨兵节点 主观认为 节点下线\n客观下线\n单个哨兵认为节点下线，这个结果的可信度较低，不能就直接认为某个节点下线了\n当某一个哨兵认为某个节点下线，会给其它哨兵发一条消息，看看其他哨兵的意见：\n如果超过 quonum 个哨兵认为该节点下线，那么哨兵集群就认为这个节点 客观下线 quonum 的值建议设为 n / 2 + 1，n 为哨兵的总个数\n故障转移 由哪个哨兵进行故障转移 如果故障的是主节点，那么就要进行故障转移\n那么该由哪个哨兵进行故障转移呢？\n如果一个哨兵「主动」发现节点下线，那这个哨兵就是一个「候选者」\n可能有多个哨兵都「主动」发现节点下线了，那么就会有多个候选者\n每个候选者会给哨兵集群中的每一个哨兵发一个 leader 请求，表明自己想成为 leader，负责这一次故障转移：\n每个哨兵仅有一票 候选者会自己给自己投一票 其他哨兵，只要收到一个 leader 请求，就给这个候选者投票 如果一个候选者的得票数 超过半数（n / 2 + 1），并且 大于等于哨兵配置文件中的 quorum 值，那么认为这个候选者就是 leader\n经过上面哨兵内部的选举过程，成功选出了一个 leader，接下来，就由这个 leader 来负责故障转移的过程\n过程 选择新的主节点\n首先要排除网络不佳的节点\n前面提到了一个 down-after-milliseconds 参数，如果一个节点在 down-after-milliseconds 内没有响应哨兵的请求，那么会被认为主观下线\n主观下线的次数会由哨兵集群维护，如果一个节点主观下线的次数超过了 10 次，那么认为这个节点的网络不佳\n排出了网络不佳的节点后，在剩余从节点中选择出新的主节点，选择的依据如下：\n节点优先级（slave-priority，由用户配置）：slave-priority 越低，优先级越高 同步偏移 offset：如果优先级相同，那么比较 offset，同步的数据越多，优先级越高（丢失的数据越少） server_id：如果节点优先级和 offset 都相同，server_id 越小，优先级越高 选择出新的主节点后，哨兵会给这个从节点发送一个 SLAVEOF no one 命令，让其独立成一个主节点\n发送完 SLAVEOF no one 命令后，哨兵会监控这个节点的状态（每秒一次，INFO 命令），直到这个节点的状态为 master\n将从节点指向新的主节点\n当从节点成功升级为新的主节点后，哨兵会给其它从节点发送 SLAVEOF \u0026lt;new_ip\u0026gt; \u0026lt;new_port\u0026gt;，以让其它从节点修改自己的 master 为新的主节点\n通知客户端主节点发生更改\n现在，故障转移基本完成，只需要通知客户端最新的主节点是谁就可以了\n怎么通知？\n客户端与哨兵之间通过「发布-订阅」机制来实现通信\n客户端会订阅 +switch-master 频道，当故障转移完毕后，哨兵会向这个频道发送新的主节点的 IP 和 Port，这样客户端就能知道新的主节点的信息了\n监视原来的主节点的上线情况\n哨兵还需要监视原来的主节点是否上线，如果上线，需要发送一个 SLAVEOF \u0026lt;new_ip\u0026gt; \u0026lt;new_port\u0026gt;，将其降级为从节点\n为什么要部署多个哨兵节点，一个不行吗？ 前面提到：单纯一个哨兵主观认为一个节点下线，这个是不可信的\n因为完全有可能是哨兵自己出现了问题，而非主节点出问题\n因此，为了保证可靠性，部署的是多个哨兵节点组成集群（至少大于等于 3 个）\n此外，只部署一个哨兵节点，如果哨兵都挂了，整个集群都没有“领导者”了，成了一盘散沙，故障转移无法实现\n哨兵集群的组成 哨兵间如何通信\n在哨兵的配置文件中，并没有配置其它哨兵的 IP 和 Port，那哨兵是如何发现彼此的呢？\n还是依靠「发布-订阅」机制\n在 Redis 的集群中，会有一个频道 __sentinel__:hello，每个哨兵都会订阅这个频道：\n当一个哨兵上线，会向这个频道发送自己的 IP 和 Port 其它哨兵订阅这个频道，就可以读取到这个哨兵的 IP 和 Port，进而建立通信 图片来自小林 Coding 哨兵如何获取其它子节点的信息\n哨兵会定期向主节点发送 INFO 信息，获取子节点列表，这样，哨兵就可以知道其它子节点的 IP 和 Port 了\n图片来自小林 Coding ","permalink":"https://blogs.skylee.top/posts/redis/%E9%AB%98%E5%8F%AF%E7%94%A8/%E5%93%A8%E5%85%B5/note/","tags":["Redis","高可用"],"title":"Redis 哨兵"},{"categories":["Redis"],"content":"底层数据结构 2024.06.08 更新\n补充一张思维导图：\nSDS（简单动态字符串） 传统的 C 字符串存在以下问题：\n获取 strlen 效率低 无法存储文本以外的数据（二进制不安全） 不支持动态扩容，append 字符串效率低 Redis 为了解决 C 字符串的这些问题，对其进行了进一步封装：\n// __attribute__ ((__packed__)) 的意思是让编译器不要进行内存对齐 // 进一步节省内存空间 struct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; /* used */ uint8_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; 可以看出，SDS 有以下基本字段：\nlen：字符串的长度 alloc：字符串已分配内存的大小 flags：字符串类型 buf：实际存储的数据 动态扩容\nSDS 支持动态扩容，当 buf 的空间无法满足存储需求，会自动扩容，这点与 C++ 的 std::vector 是一致的\n动态扩容的策略如下：\nhisds hi_sdsMakeRoomFor(hisds s, size_t addlen) { ... ... // s目前的剩余空间已足够，无需扩展，直接返回 if (avail \u0026gt;= addlen) return s; //获取目前s的长度 len = hi_sdslen(s); sh = (char *)s - hi_sdsHdrSize(oldtype); //扩展之后 s 至少需要的长度 newlen = (len + addlen); //根据新长度，为s分配新空间所需要的大小 if (newlen \u0026lt; HI_SDS_MAX_PREALLOC) //新长度\u0026lt;HI_SDS_MAX_PREALLOC 则分配所需空间*2的空间 newlen *= 2; else //否则，分配长度为目前长度+1MB newlen += HI_SDS_MAX_PREALLOC; ... } 如果扩容后的大小小于 HI_SDS_MAX_PREALLOC，那么分配当前空间的 2 倍 如果扩容后的大小大于 HI_SDS_MAX_PREALLOC，那么分配当前空间 + 1M 类型\n为了进一步减少内存占用，Redis 设计的 SDS 有五种类型：sdshdr5、sdshdr8、sdshdr16、sdshdr32、sdshdr64\n这五种类型的区别仅仅是 len、alloc 的类型不同\nsdshdr 后面的数字决定了 len、alloc 的占用的字节数，这样可以保证单个 SDS 的头不会浪费太多内存\nList（链表） Redis 实现的链表是双向链表，没有什么特别的\nList 的缺点：\n无法利用 CPU Cache 存在内存浪费：每个 Node 都要保存 prev 和 next 指针 ZipList（压缩列表） 为了解决 List 的内存占用问题，Redis 实现了 ZipList\nZipList 会分配一块连续的内存空间用于存储数据，可以充分利用 CPU Cache\n图片来自小林 Coding 每个 Node 保存三个数据：\nprev_len：到前一个节点头部的距离，提供从后到前的遍历能力 encoding：编码方式、数据长度 data：存放的数据 ZipList 在首尾插入、删除数据的时间复杂度为 O(1)，效率高，但是要获取中间节点的值，就只能和链表一样遍历了\n连锁更新\n压缩列表节点的 prevlen 属性会根据前一个节点的长度进行不同的空间大小分配：\n如果前一个节点的长度小于 254 字节，那么 prevlen 属性需要用 1 字节的空间来保存这个长度值； 如果前一个节点的长度大于等于 254 字节，那么 prevlen 属性需要用 5 字节的空间来保存这个长度值； 目的是为了节省内存占用\n但基于这种机制，ZipList 存在连锁更新的问题：来看一下这个场景：\n图片来自小林 Coding 如果要在头部插入一个数据，且占用的内存超过 254 字节：\ne1 节点的 prev_len 无法保存头部数据的长度，只能扩容到 5 字节 由于 e1 节点的 prev_len 扩容，e1 节点整体占用的字节数也超过了 254 字节 e2 节点的 prev_len 无法保存头部数据的长度，只能扩容到 5 字节 \u0026hellip; 这样，后续节点都需要扩容，直到可以保存下前一个节点的长度\n如果扩容的节点数较多，那么性能就会很差\n因此，ZipList 适用于数据量较少，这样即使发生连锁更新，也可以接受\nQuickList QuickList 与普通的 List 的区别在于：\n普通的 List，节点保存的就是实际的 value QuickList，节点保存的是一个 ZipList 因此，在插入新的数据时，不一定要创建新的 Node，如果 ZipList 可以存下，那么就直接放在 ZipList 中\nQuickList 相较于普通的 List，优点如下：\n插入新的数据时，平均效率较高 读取数据时，局部可以使用到 CPU Cache ListPack 为了解决 ZipList 的连锁更新问题，Redis 5.0 引入了新的数据结构 ListPack\n图片来自小林 Coding ListPack 继承了 ZipList 的优秀设计，例如也采取连续分配内存的方式、节点采用不同的编码方式以节省内存\n但与 ZipList 不同的是：节点 不需要记录 prev_len 了，这样插入数据时，也不会出现连锁更新的问题\n不记录 prev_len，如何倒序遍历？\nListPack 可以根据一个 Node 的首地址，利用位运算，逐个向左解析，得到上一个节点的起始位置：\n/* Decode the backlen and returns it. If the encoding looks invalid (more than * 5 bytes are used), UINT64_MAX is returned to report the problem. */ uint64_t lpDecodeBacklen(unsigned char *p) { uint64_t val = 0; uint64_t shift = 0; do { val |= (uint64_t)(p[0] \u0026amp; 127) \u0026lt;\u0026lt; shift; if (!(p[0] \u0026amp; 128)) break; shift += 7; p--; if (shift \u0026gt; 28) return UINT64_MAX; } while(1); return val; } Dict Redis 实现了哈希表数据结构\nDict 整体有两个 Hash Table，第一个用于存储实际数据，第二个用于 rehash\nRedis 实现的哈希表使用链地址扫描法来解决哈希冲突\n图片来自小林 Coding rehash\n整个哈希表通过 负载因子 来衡量冲突的程度，负载因子越大，冲突越严重\n如果一直这样下去，那么哈希表的查找效率就会退化为 O(n)\n因此，Redis 默认在负载因子：\n大于 1：如果此时没有进行 bgwriteaof 以及 bgsave，执行 rehash 大于 5：强制 rehash rehash 的过程如下：\n给哈希表 2 分配空间 将哈希表 1 的数据「迁移」到哈希表 2（迁移包括了整理数据的过程，降低负载因子） 释放哈希表 1 的空间 将哈希表 2 设置为哈希表 1 渐进式 rehash\n由于数据量较大，rehash 的时间可能很长，为了避免直接 rehash 带来的性能影响，Redis 引入了渐进式 rehash，过程如下：\n给哈希表 2 分配空间 当客户端执行操作时，将哈希表 1 对应索引处所有的 key-value 迁移到哈希表 2 rehash 时，新增的 kv 都存在哈希表 2 随着客户端执行的操作越多，哈希表 1 的数据会逐渐迁移到哈希表 2\nInt Set 如果哈希表存储的全是整数，并且数据量不大，用哈希表就显得有点重了\nInt Set，即整数集合，用于存储一系列整数\nInt Set 的定义如下：\ntypedef struct intset { uint32_t encoding; // 编码方式 uint32_t length; // 数组长度 int8_t contents[]; // 数组，注意，元素的实际类型并不一定是 int8_t ，取决于 encoding } intset; contents 的数据是 有序 的，在查找数据时，可以使用 二分\n为了节省内存，Int Set 的元素默认采取 int8 类型\n如果新插入的元素无法用 8 个字节存储，Int Set 会进行类型升级\n例如，假设此时 contents 的元素为 {11, 45}，那么 int8_t 的编码就能满足了\n如果此时插入一个新的元素，假设为 114514:\n114514 的类型为 int32_t，大于 int8_t 提升 contents 的编码类型 为 int32_t（扩容实现） 倒序 遍历 contents，将原有数据向后移动 将 114514 插入到 contents 尾部 contents 仅支持升级不支持降级\n由于 intset 采取连续空间存储数据，并且要求有序，因此，仅适用于 数据量较小 的情况（插入 O(n)，查询 O(log(n))）\nSkip List Skip List，跳表，是 ZSET 的底层数据结构，支持在 O(log(n)) 内查找目标元素，并支持范围查询\nSkip List 的结构如下：\n图片来自小林 Coding 可以看到，Skip List 实际上就是一个 多层、 有序 链表\n头节点保存了每一层的第一个 next 节点 子节点保存了： prev 指针 不同 level 的 next 指针 value 来看看子节点的结构：\ntypedef struct zskiplistNode { //Zset 对象的元素值 sds ele; //元素权重值 double score; //prev 指针 struct zskiplistNode *backward; //节点的 level 数组，保存每层上的前向指针和跨度 struct zskiplistLevel { struct zskiplistNode *forward; unsigned long span; } level[]; } zskiplistNode; 查询过程\n一开始从最高的节点开始查询 将当前节点的 score 与目标 score 比较： 如果当前节点的 score 大于目标 score，那么继续向右遍历 如果当前节点的 score 小于目标 score，那么跳到 level - 1 层的 pre 节点， 如果当前节点的 score 等于目标 score，那么向右遍历直到 score 大于目标 score 举个例子：\n图片来自小林 coding 假设要寻找的节点为 member: abcd, score:4:\n从 L2 开始，遍历到节点 abc, 3 由于节点 abc, 3 的权重小于 4，因此向右遍历 向右遍历，发现为 NULL，于是跳到 level - 1 的下一个节点，即 abcde, 4 由于节点 abcde, 4 的权重等于 4，判断 member 是否一致 不一致，于是向左遍历到 abcd, 4，发现与目标节点一致，结束 可以看到整体思想也是基于二分\n层数设置\n跳表的查询效率与层数密切相关\n如果层数太小，退化成链表，还是得一个一个遍历 如果层数太多，会占用不必要的空间，向下查询次数也会变多，效率也不高 建议 level[n - 1]的节点数 : level[n]的节点数 = 2 : 1，可以获得不错的查询效率\nRedis 是如何维护比例为 2:1 的？\nRedis 在创建节点时，并没有刻意的去保证比例为 2:1\n在确定一个节点对应的 level 时，采取随机的策略，生成一个随机数：\n如果随机数大于 0.25，那么 level++ 否则 level 就确定下来 可以发现，越大的 level，对应越低的概率：\n100% 的节点的 level 为 0 50% 的节点的 level 为 1 25% 的节点的 level 为 2 12.5% 的节点的 level 为 3 \u0026hellip; 这样就近似保证了 level[n - 1]的节点数 : level[n]的节点数 = 2 : 1\n在 Redis7.0 中，level 的上限确定为 32\n为什么不用 Red-Blank Tree\n看看作者的解释：\nThere are a few reasons:\nThey are not very memory intensive. It\u0026rsquo;s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees.\nA sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees.\nThey are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code.\n具体来说：\n相较于 AVL 树、红黑树，SkipList 在存储相同数据的情况下，占用的内存更少 相较于 AVL 树、红黑树，SkipList 在范围查询时，数据的访问更加连续，可以更好的利用缓存，性能不比 AVL 树、红黑树、B 树差 相较于 AVL 树、红黑树，SkipList 的维护性更好，实现简单，插入删除数据时，不需要太多操作 数据结构（APIs） 在将数据结构前，先来了解一个概念\nRedis 无论什么数据结构，都被封装成了一个 Redis Object\n每个 Object 都有自己的 编码方式，决定了底层使用什么样的数据结构\nString String 可以说是 Redis 使用最多的数据结构了\nString 的编码方式如下：\n当 String 存储的是整数数据，采用 INT 编码方式，直接存储一个整数，支持 Incr 操作 当 String 存储的是字符串： 如果整个 Object 占的空间小于 64 字节（即 str 部分为 64 - 4 - 4 - 8 = 44 字节）时，采取 EMBSTR 编码方式 否则，采用 RAW 编码方式 EMBSTR 编码方式与 RAW 编码方式都是基于 SDS 的，区别在于：EMBSTR 的数据部分与头部部分为一个连续的空间，只需要一次内存分配；而 RAW 需要两次分配\n应用场景 String 的应用场景很多，这里就说一下 Session 存储\n如果直接将 Session 存放在单个服务端，那么用户请求到另一个的服务端，就无法验证 Session 的正确性\n可以以 uid 为 key，session 为 val，存在 Redis 中，这样服务端直接请求 Redis 即可\nList 实现原理 List 的编码方式：\n3.2 版本前： 如果存储的数据节点小于 512 个并且占用空间小于 64K，采取 ZipList 编码 否则，采取 List 编码 3.2 版本后：采取 QuickList 编码 应用场景 List 可以高效的在首尾增加和删除数据，适合用作 消息队列\n但是 List：\n不支持消费者组 不支持消息重放 没有维护消费的 offset Stream 为了解决 List 的缺陷，Redis 5.0 引入了 Stream\n实现原理 Stream 底层基于 Radix Tree 和 ListPack 实现\n每一个 Stream Entry 包含了三个部分：\n全局唯一 id：如果没有指定消息 id，Redis 会自己生成一个分布式 id key value 生产一条消息后，如果可以将新的消息放在 ListPack 中（即 ListPack 有空闲位置），就直接存放，否则，创建一个新的 ListPack，并将 ListPack 的 id 存在 Radix Tree 中\nRadix Tree 的键是 Stream Entry 的 ID，值是对应的 Listpack，Radix Tree 可以根据提供的消息 id，快速定位到 ListPack，进而定位到 Stream Entry 的位置\nAPI XADD：插入消息，保证有序，可以自动生成全局唯一 ID； XLEN ：查询消息长度； XREAD：用于读取消息，可以按 ID 读取数据； XDEL ： 根据消息 ID 删除消息； DEL ：删除整个 Stream； XRANGE ：读取区间消息 XREADGROUP：按消费组形式读取消息； XPENDING 和 XACK： XPENDING 命令可以用来查询每个消费组内所有消费者「已读取、但尚未确认」的消息； XACK 命令用于向消息队列确认消息处理已完成 概念抽象 Stream Entry：类似于 Message Stream Key：即 Stream 的 key，类似于 Partition 消费者组 与 Kafka 不同，Redis 的消费者组消费的是同一个 key，即同一个 Partition\n也就是说，多个消费者消费同一个 Partition，这在 Kafka 是不允许的（存在重复消费的问题）\n但 Redis 保证：一条 message，只能由 Consumer Group 中的一个 consumer 消费，没有重复消费的问题\nConsumer groups in Redis streams may resemble in some way Kafka (TM) partitioning-based consumer groups, however note that Redis streams are, in practical terms, very different. The partitions are only logical and the messages are just put into a single Redis key, so the way the different clients are served is based on who is ready to process new messages, and not from which partition clients are reading. For instance, if the consumer C3 at some point fails permanently, Redis will continue to serve C1 and C2 all the new messages arriving, as if now there are only two logical partitions.\n简单来说，Redis 流中的消费者组消费消息的方式与 Kafka 不同。在 Kafka 中，消息被存储在不同的分区中，消费者组中的每个消费者负责消费特定分区的消息。而在 Redis 流中，消息被存储在一个键中，消费者组中的消费者 通过竞争 获取消息的消费权。如果一个消费者失败，其他消费者将继续消费新消息，而不会受到分区分配的影响。\n消息持久化 Stream 的持久化依赖于 RDB 和 AOF\n在业务中使用 Stream，需要格外注意 AOF 的刷盘策略\n如果要保证消息 尽量 不丢，就要让 AOF 刷盘策略严格一些\nRedis 可以通过配置文件或运行时命令来控制 AOF（Append Only File）文件的 fsync 参数。\n在 Redis 的配置文件（redis.conf）中，可以使用 appendfsync 配置项来设置 AOF 文件的 fsync 策略。该配置项有以下三个可选值：\nalways：每次写入 AOF 文件时都会执行 fsync 操作，将数据写入磁盘。这种策略可以确保数据的持久性，但可能会降低性能。 everysec：每秒钟执行一次 fsync 操作，将数据写入磁盘。这种策略可以确保数据的持久性，同时避免频繁的磁盘操作对性能的影响。 no：不执行 fsync 操作，将数据写入内存，而不是磁盘。这种策略可以提高性能，但可能会导致数据丢失。 消息有序性保障 来看一下 Redis 官方的解释：\nWe could say that schematically the following is true:\nIf you use 1 stream -\u0026gt; 1 consumer, you are processing messages in order. If you use N streams with N consumers, so that only a given consumer hits a subset of the N streams, you can scale the above model of 1 stream -\u0026gt; 1 consumer. If you use 1 stream -\u0026gt; N consumers, you are load balancing to N consumers, however in that case, messages about the same logical item may be consumed out of order, because a given consumer may process message 3 faster than another consumer is processing message 4. 也就是说，要想保证消息消费顺序与 producer 生产顺序一致，那么 一个 Stream 就只能对应一个 Consumer\n与 Kafka 的对比 Redis Stream 与 Kafka 等专业消息队列，最主要的差距就在于：\n消息可靠性（保证不丢） 消息是否可堆积 对于 Redis Stream 来说，无法保证 消息的可靠性：\nAOF fsync 过程宕机 主从复制是异步的，存在丢失数据的风险 并且，由于 Redis 将数据存储在内存，决定了无法堆积太多消息\n详细可以看看 Redis 官方文档 Hash 实现原理 编码规则：\n如果存储的数据节点小于 512 个并且占用空间小于 64K，采取 ZipList 编码 否则，采取 Dict 编码 在 Redis7.0 版本，废弃了 ZipList，采用 ListPack\n应用场景 可以使用 hash 表存储对象，例如，要存储 user 对象，可以以 user_id 为 key，序列化以后的 user 为 value\nSet 实现原理 编码方式：\n如果元素数量小于 512 个，并且全是整数，使用 IntSet 编码 否则，使用 Dict 编码 应用场景 点赞场景，例如：\n记录一个文章有哪些用户点过赞，可以以 post_id 为 key，user_id 为 member 记录一个用户给哪些评论点过赞，可以以 user_id 为 key，comment_id 为 member 共同关注，例如：\n计算用户共同关注的公众号，可以以 公众号 id 为 key，user_id 为 member，然后求交集 Sorted Set 实现原理 编码方式：\n如果元素个数小于 128 个，并且占用空间小于 64 字节时，使用 ZipList 编码 否则，使用 Dict + SkipList 编码 这里使用哈希表的目的是为了保存：member 到 score 的映射关系，可以快速获取一个 member 的 score，即 ZSCORE API\n应用场景 Sorted Set 主要的应用场景就是排序了，例如排行榜功能\nBitMap 实现原理 BitMap 使用 SDS 来存储数据，具体来说，是利用 SDS 的 value 属性\nBitMap 提供了以下 API：\n# 设置值 SETBIT key offset value # 获取值 GETBIT key offset # 获取指定范围内值为 1 的个数 # start 和 end 以字节为单位 BITCOUNT key start end # result 计算的结果，会存储在该key中 # key1 … keyn 参与运算的key，可以有多个，空格分割，not运算只能一个key # 当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0。返回值是保存到 destkey 的字符串的长度（以字节byte为单位），和输入 key 中最长的字符串长度相等。 BITOP [operations] [result] [key1] [keyn…] # 返回指定key中第一次出现指定value(0/1)的位置 BITPOS [key] [value] 应用场景 BitMap 非常适合用于统计二值的场景（即 true or false），例如：\n用户签到表：以 user_id 为 key，day 为 offset，365 天的签到表，仅会占用约 60 字节的空间 用户在线表：以 user:online 为 key，user_id 为 offset，如果 user_id 从 1 开始，5000w 用户仅会使用 6M 空间 HyperLogLog HyperLogLog 是一种高效的数据结构，用于 估计 集合的大小，即估算集合中元素的数量。HyperLogLog 的优点是 空间复杂度低，时间复杂度为 O(1)，并且可以处理非常大的集合。\nHyperLogLog 的误差通常为 0.81%\nHyperLogLog 的原理是利用概率算法，通过比较概率分布来估算集合的大小。具体来说，HyperLogLog 会维护一个长度为 m 的窗口，用于存储输入序列中的元素。对于每个元素，HyperLogLog 会计算其与前 m 个元素的相似度，并根据相似度计算其概率。然后，HyperLogLog 会根据概率计算该元素在集合中的大小，并更新窗口。最后，HyperLogLog 会根据窗口中的元素数量估算集合的大小。\nAPIs PFADD：将一个或多个元素添加到 HyperLogLog 中。 PFADD myhll element1 element2 ... PFCOUNT：获取 HyperLogLog 中的基数估计值。 PFCOUNT myhll PFMERGE：合并多个 HyperLogLog 结构为一个，用于估计多个集合的联合基数。 PFMERGE dest-hll source-hll1 source-hll2 ... 应用场景 HyperLogLog 适合大量 计数，且 对数据精度要求不高 的场景，例如：网页的 UV 统计、用户活跃度\nGeospatial Redis 中的 Geospatial 数据类型是一种用于存储地理空间信息的数据结构，允许存储地理位置（经度和纬度）以及与这些位置相关的其他数据（例如，商家名称、城市名称等）。\nAPIs GEOADD：将一个或多个地理位置成员添加到指定的有序集合中。\nGEOADD locations 13.361389 38.115556 \u0026#34;Palermo\u0026#34; 15.087269 37.502669 \u0026#34;Catania\u0026#34; GEODIST：计算两个地理位置之间的距离。\nGEODIST locations \u0026#34;Palermo\u0026#34; \u0026#34;Catania\u0026#34; km GEOPOS：获取一个或多个地理位置成员的经度和纬度。\nGEOPOS locations \u0026#34;Palermo\u0026#34; \u0026#34;Catania\u0026#34; GEORADIUS：根据指定的地理位置和半径，返回范围内的成员。\nGEORADIUS locations 15 37 200 km GEORADIUSBYMEMBER：根据指定的地理位置成员和半径，返回范围内的成员。\nGEORADIUSBYMEMBER locations \u0026#34;Palermo\u0026#34; 200 km 实现原理 Geo 实际上是对 Sorted Set 的进一步封装，以：\nplace_name 为 key 经度与纬度键值对 为 value 基于此，就不难理解 Geo 的 APIs 的实现了\n应用场景 Geo 的应用场景有：\n位置服务：Geo 可以用来提供位置服务，如查找附近的人、查找附近的店铺等。 推荐系统：Geo 可以用来进行推荐系统，如根据用户的位置信息推荐附近的店铺、景点等。 ","permalink":"https://blogs.skylee.top/posts/redis/%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/note/","tags":["Redis"],"title":"Redis 常见数据结构与实现原理"},{"categories":["Redis"],"content":" 2024.06.08 更新\n补充了思维导图：\nAOF 日志 介绍 AOF（append-only file）是 Redis 持久化数据的一种策略\n由于 Redis 的数据都存在内存，如果不做持久化，重启后数据就丢失了\nAOF 的机制是：在执行完每一条写命令后，顺便将这条命令追加在 AOF 文件中\n例如：\n192.168.124.114:6379\u0026gt; set k1 v1 OK 192.168.124.114:6379\u0026gt; exit [root@localhost redis-6.2.13]# cat appendonly.aof *3 # 接下来的命令有三个字符串 $3 # 接下来的字符串有三个字节 set # set 命令 $2 k1 $2 v1 要配置和启用 AOF 持久化，可以在 Redis 配置文件中设置以下配置项：\nappendonly yes：启用 AOF 持久化。\nappendfilename：指定 AOF 文件的名称，默认为 appendonly.aof。\nappendfsync：设置 AOF 同步策略\n当 Redis 重启或者断电恢复时，就可以使用 AOF 文件做重放，以达到恢复数据的目的\n持久化策略 来看看 AOF 持久化的原理图：\nAOF 有三种持久化策略：\nalways：每执行一次写命令，就执行一次 fsync，效率最低，安全性最高 everysec：执行一次写命令，将命令记录到 AOF 缓冲区，每个 1s 将缓冲区的数据刷新（fsync）到磁盘，效率中等，安全性较高 no：执行一次写命令，将命令记录到 AOF 缓冲区，由 OS 决定将缓冲区的数据刷新到磁盘（只 write，不 fsync），效率高，安全性低 AOF 重写 来看这一个例子：\n192.168.124.114:6379\u0026gt; set k1 v1 OK 192.168.124.114:6379\u0026gt; set k1 v2 OK 192.168.124.114:6379\u0026gt; del k1 (integer) 1 192.168.124.114:6379\u0026gt; exit [root@localhost redis-6.2.13]# cat appendonly.aof *2 $6 SELECT $1 0 *3 $3 set $2 k1 $2 v1 *3 $3 set $2 k1 $2 v2 *2 $3 del $2 k1 事实上，对 k1 的修改完全没有必要全部记录下来，但是 AOF 还是将多余的操作记录下来了，这将浪费空间\n而 AOF 重写就是用来解决这个问题的\nAOF 重写的过程：\n新建一个 AOF 文件，记做 tmp 遍历 Redis 中 所有的键值对，生成对应的执行命令（例如，有一个 string 类型的 k1，value 为 v1，就会在 tmp 中记下 SET k1 v1） 用 tmp 替换原来旧的 AOF 文件，这个过程是 原子的 这个重写过程存在问题：\n如果在主进程执行，那么在重写过程中，主线程就无法接受新的写请求，会导致 tmp 与 Redis 的数据不一致\n因此，为了保证在 AOF 的重写的过程中，主线程仍然可以接受新的写请求，重写操作实际上是由子 进程 完成的\n这里给出 AOF 重写的完整过程：\n新建一个 AOF 文件，记做 tmp 新建一个 AOF 重写缓冲区 fork 一个子进程 子进程遍历 Redis 中所有的键值对，生成对应的执行命令 重写过程中，如果有新的写入请求，主进程需要： 写入到 data 区 写入到 aof_buffer 写入到 aof_rewrite_buffer 当子进程的重写操作执行完毕，发送一个信号给主进程 主进程收到信号后，调用信号处理函数 信号处理函数会做以下事情： 将 aof_rewrite_buffer 的数据追加到 tmp 中 用 tmp 替换原来旧的 AOF 文件 AOF 重写（rewrite） 是一个有歧义的名字，该功能是通过读取 数据库（dataset）中 的键值对来实现的，程序无须对现有 AOF 文件进行任何读入、分析或者写入操作。\nRedis 利用子进程 与 aof_rewrite_buffer 来实现重写时仍能接受新的写请求\n为什么要 fork 一个子进程，子线程不行吗？\n使用子进程的目的主要还是避免加锁带来的性能开销\nfork 子进程会用到 写时拷贝，即使在重写过程中，主进程由于接受用户写请求而修改了部分数据，也不会影响到子进程的内存空间\n只要修改的数据不多（即读多写少），那么子进程带来的性能开销不会很大，比线程加锁的性能开销更低\nRDB 快照 介绍 Redis RDB（Redis Database Dump）是 Redis 数据持久化的一种方式，它用于将 Redis 数据快照保存到磁盘上，以便在服务器重启时可以重新加载数据。RDB 是一种 紧凑且高性能 的持久化方式，通常用于灾难恢复和 备份 。\n持久化策略 # Unless specified otherwise, by default Redis will save the DB: # * After 3600 seconds (an hour) if at least 1 key changed # * After 300 seconds (5 minutes) if at least 100 keys changed # * After 60 seconds if at least 10000 keys changed # You can set these explicitly by uncommenting the three following lines. # save \u0026#34;\u0026#34; 禁用 RDB save 3600 1 save 300 100 save 60 10000 以 save 3600 1 为例：如果一个小时内，Redis 执行了 1 条写命令，就触发 RDB 持久化\nRDB 持久化也是 fork 一个子进程来处理，但相较于 AOF 来说，RDB 更紧凑，存放的是二进制数据，占用空间小，恢复速度更快\n也可以手动执行 BGSAVE 来生成一个 RDB 快照\nRDB 与 AOF 混合持久化 如果单独使用 RDB，性能较好，恢复较快，但备份的粒度太粗，丢的数据可能较多 如果单独使用 AOF，性能还行，备份粒度细，丢的数据少，但恢复速度慢 在 Redis4.0 引入了「混合持久化」，可以将 RDB 与 AOF 混合在一起使用\n混合持久化，其实与 AOF 的重写很类似，就是把「重写」换成了 RDB 而已，重写过程生成的 tmp 实际上是 RDB 快照\n也就是说，AOF 文件的 前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。\n这样，既能保证恢复的速率，也可以保证数据的可靠性\n开启混合持久化，只需要修改 aof-use-rdb-preamble 为 yes 即可\n# When rewriting the AOF file, Redis is able to use an RDB preamble in the # AOF file for faster rewrites and recoveries. When this option is turned # on the rewritten AOF file is composed of two different stanzas: # # [RDB file][AOF tail] # # When loading Redis recognizes that the AOF file starts with the \u0026#34;REDIS\u0026#34; # string and loads the prefixed RDB file, and continues loading the AOF # tail. aof-use-rdb-preamble yes 注意：RDB 与 AOF 混合持久化这种方式，是建立在 AOF 持久化基础上的，本质上还是 AOF 持久化\n可以看看这个 issue 其它 AOF VS. RDB 来看看 AOF 的优缺点：\n优点：\n可以控制刷盘策略 备份粒度细，丢失数据少 如果 AOF 文件损坏，相较于 RDB 更容易恢复 可读性较好，可以人为修改（撤销）错误命令 缺点：\n文件占用空间相对较大，重启恢复慢 性能占用平均而言比 RDB 高（需要将数据转换成一条条 Redis Command） 再来看看 RDB 的优缺点：\n采用二进制编码，占用空间小，重启恢复快 性能占用平均而言比 AOF 低 缺点：\n备份粒度粗，如果仅使用 RDB，可能丢失较多数据（取决于设置的备份间隔） 如果 RDB 文件损坏，几乎无法恢复 fork 的频率比 AOF 高（AOF 只有重写时才会 fork，而 RDB fork 的频率取决于设置的备份间隔） fork 期间，主进程是无法接受客户端的所有请求的，如果 dataset 较大，fork 需要更长的时间\nRedis 重启后是怎么恢复数据的？ 分三个情况讨论：\n单独启用 RDB 单独启用 AOF 同时启用 RDB 和 AOF 单独启用 RDB，或者单独启用 AOF，主进程会先读取 RDB 或者 AOF 文件到内存，做数据恢复，然后才能接受客户端的请求\n而如果同时启用了 RDB 和 AOF，那么 Redis 会 优先使用 AOF 文件做恢复，因为 AOF 的备份粒度较细，更能保证数据的可靠性（以牺牲恢复速度为代价）\nIn the case both AOF and RDB persistence are enabled and Redis restarts the AOF file will be used to reconstruct the original dataset since it is guaranteed to be the most complete.\n既然都优先使用 AOF 了，为什么还要使用 RDB？\nRedis 官方给出的解释：\nThere are many users using AOF alone, but we discourage it since to have an RDB snapshot from time to time is a great idea for doing database backups, for faster restarts, and in the event of bugs in the AOF engine.\n简单来说，RDB 更适合做数据备份，可以作为一种兜底方案，如果在 AOF 引擎中出现错误，可能会导致数据丢失。定期创建 RDB 快照可以在这种情况下提供一种恢复数据的方法。通过比较 RDB 快照和当前数据，可以找出丢失的数据并从快照中恢复。\nRedis7.0 后的 AOF 从 Redis 7.0.0 版本开始，Redis 使用 多部分 AOF 机制（multi part AOF mechanism）。原来的单 AOF 文件被拆分为 基础文件（最多一个）和 增量文件（可能多个）。基础文件代表在 AOF 被重写时的数据初始快照（RDB 或 AOF 格式），增量文件包含自上次基础 AOF 文件创建以来的增量更改。所有这些文件都被放在一个单独的目录中，并由一个清单文件（manifest file）跟踪。\nAOF 重写时，Redis 父进程打开一个新的增量 AOF 文件以继续写入。子进程执行重写逻辑并生成一个新的基础 AOF。Redis 将使用一个临时清单文件来跟踪新生成的基础文件和增量文件。当它们准备好时，Redis 将执行一个原子替换操作，使这个临时清单文件生效。\nAOF 文件截断或者损坏了，怎么办？ 如果 AOF 截断（truncated）了，在重启 Redis 会看到如下提示：\n* Reading RDB preamble from AOF file... * Reading the remaining AOF tail... # !!! Warning: short read while loading the AOF file !!! # !!! Truncating the AOF at offset 439 !!! # AOF loaded anyway because aof-load-truncated is enabled 默认情况下，Redis 会继续运行，忽略 掉截断的部分，保证可用性\n当然我们也可以通过修改 aof-load-truncated 来控制这个行为（yes or no）\n通过提示信息定位到截断的部分，可以人工修改一下\n如果 AOF 损坏了，在重启 Redis 会看到如下提示：\n* Reading the remaining AOF tail... # Bad file format reading the append only file: make a backup of your AOF file, then use ./redis-check-aof --fix \u0026lt;filename\u0026gt; 这种情况就需要使用 redis-check-aof 来修复了\nredis-check-aof 工具会提示问题出现的位置，我们可以人工修改\nThe best thing to do is to run the redis-check-aof utility, initially without the \u0026ndash;fix option, then understand the problem, jump to the given offset in the file, and see if it is possible to manually repair the file: The AOF uses the same format of the Redis protocol and is quite simple to fix manually. Otherwise it is possible to let the utility fix the file for us, but in that case all the AOF portion from the invalid part to the end of the file may be discarded, leading to a massive amount of data loss if the corruption happened to be in the initial part of the file.\n如何在 Redis 从 RDB 切换到 AOF？ 如果在仅启用 RDB 快照的情况下，想要启用 AOF，千万不要直接修改配置文件，然后重启，会造成数据丢失\n准备工作\n备份 RDB 文件 在运行的 Redis Server 上迁移\n启用 AOF： redis-cli config set appendonly yes 关闭 RDB（可选） 更新 Redis 的配置文件，确保启用了 AOF（否则重启使用旧的配置文件，可能造成数据丢失） 如果要重启 Server，需要等到 AOF 持久化完毕以后才能重启（You can do that by watching INFO persistence, waiting for aof_rewrite_in_progress and aof_rewrite_scheduled to be 0, and validating that aof_last_bgrewrite_status is ok.）\nRedis 大 key 对持久化有啥影响？ 大 Key 指的是占用空间大的 K-V 键值对\n如果启用了 AOF，并且持久化策略为 Always，那么每次对大 Key 写操作，fsync 的时间会很长，阻塞其它客户端的请求\n在创建 RDB 快照，或者 AOF 重写时，需要 fork 子进程，由于大 Key 的存在，页表也会更大，fork 的时间会更长，阻塞其它客户端的请求\n此外，在子进程创建 RDB 快照，重写 AOF 时，如果修改了大 Key，主进程会发生 写时拷贝，这个过程的时间可能较长，阻塞其它客户端的请求\n除了影响持久化以外，还有以下影响：\n打满网络 IO：例如一个 1M 的大 key，如果 QPS 为 1k，那么每秒产生 1G 流量，网络 IO 压力大 删除操作：如果直接 DEL 一个大 Key，可能阻塞较长时间，影响其它客户端的请求 参考资料 Redis 官方文档 AOF 持久化如何实现的 RDB 快照如何实现的 ","permalink":"https://blogs.skylee.top/posts/redis/%E6%8C%81%E4%B9%85%E5%8C%96/note/","tags":["Redis","Persistence"],"title":"Redis 持久化机制"},{"categories":["Redis"],"content":" 2024.06.08 更新\n补充了思维导图：\n过期删除策略 Redis 实际上使用了一个哈希表来记录每个 key 的过期时间\n在给 Redis 的 key 设置 TTL，实际上就是新增（修改）哈希表中的数据\n在获取 key 的 value 时，首先要先判断 key 是否过期，这点可以通过读取哈希表的数据实现\n那什么时候删除过期的 key 呢？\n容易想到三种方案：\n直接删除：可以给每个 key 设置一个回调，过期自动删除（成本高） 懒删除：key 过期了不删除，而是等到访问时，判断过期再删除，存在数据永远删不掉的可能，造成内存浪费 周期删除：开一个后台线程，定期扫描过期的 key，删除 Redis 采取了 懒删除 + 周期删除 结合的策略\n懒删除的过程与上文描述一致，而周期删除用于弥补懒删除无法完整删除数据的问题\nRedis 实现的周期删除有两种模式：\nSLOW 模式 FAST 模式 SLOW 模式规则：\n执行频率受 server.hz 影响，默认为 10，即每秒执行 10 次，每个执行周期 100ms 执行清理耗时不超过一次执行周期的 25% 抽取 20 个 key 判断是否过期 如果没达到时间上限（25ms） 并且过期 key 比例大于 10%，再进行一次抽样，否则结束 FAST 模式规则（过期 key 比例小于 10%不执行）：\n执行频率受 beforesleep（调用频率影响，但两次 FAST 模式间隔不低于 2ms 执行清理耗时不超过 1ms 抽取 20 个 key 判断是否过期 如果没达到时间上限（1ms） 并且过期 key 比例大于 10%，再进行一次抽样，否则结束 周期删除的时机：\nRedis 会设置一个定时任务，模式为 SLOW 每次事件循环前（epoll_wait 前）执行 beforeSleep 函数，内部会执行过期 key 清理，模式为 FAST（避免因清理时间过长，导致主线程阻塞） 内存淘汰策略 我们可以给 Redis Server 设置一个可使用内存的上限（通过 maxmemory ）\n32 位 OS，默认上限为 3G 64 位 OS，默认没有上限 超出内存使用上限，Redis 就会采取内存淘汰策略，来淘汰掉一些 key，回收内存\n内存淘汰策略大致分为两类：\n不淘汰（noeviction）：不淘汰任何 key，但是 内存满时不允许写入新数据，默认就是这种策略。 淘汰策略：内存满后，采用内存淘汰策略，来淘汰掉一些 key，回收内存 而淘汰的范围进一步分为两类：\n在设置了 TTL 的 key 淘汰，以 volatile 为前缀 在所有 key 中淘汰，以 all 为前缀 淘汰策略又进一步分为三类：\n随机淘汰 LRU LFU 随机淘汰这种方式不推荐使用\nLRU 和 LFU 都依赖于 RedisObject 的 lru 属性\nRedis 实现的 LRU 与常规的 LRU 不太一样：\n不维护 LRU 链表（节省内存空间） 在淘汰的每一轮，随机选 5 个（默认值），然后淘汰最久没有访问的 但 LRU 存在一个问题：如果一个 key 是个热点 key，但仅仅是最近还没有访问，LRU 算法有可能将这个热点 key 淘汰了\n于是 4.0 版本引入了 LFU 算法\n不同于 LRU 算法，LFU 是淘汰使用频率最低的 key\n使用 LFU 算法，lru 字段记录的格式如下：\n图片来自小林 Coding 高 16 位记录 key 上一次访问的时间戳 低 8 位记录 key 的「逻辑访问次数」 LFU 的访问次数之所以叫做逻辑访问次数，是因为并不是每次 key 被访问都计数，而是通过运算：\n生成 0~1 之间的随机数 R 计算 1 / (旧次数 * lfu_log_factor+1)，记录为 P，lfu_log_factor 默认为 10 如果 R \u0026lt; P，则计数器 + 1，且最大不超过 255 访问次数会随时间衰减，距离上一次访问时间每隔 lfu_decay_time 分钟（默认 1），计数器 - 1 可以看出，随着实际访问次数的增加，逻辑访问次数也会增加，但增加的幅度会越来越小（P 越来越小，R \u0026lt; P 的概率越来越小），最多为 255\n","permalink":"https://blogs.skylee.top/posts/redis/%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E4%B8%8E%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0/note/","tags":["Redis"],"title":"Redis 过期删除与内存淘汰策略"},{"categories":["MySQL"],"content":" 冷热分离 冷数据与热数据 冷数据指不经常访问，但是需要长期保存的数据\n可以按照两个常见维度来区分冷数据与热数据：\n时间维度：按照时间来区分冷热数据，例如，对于订单系统，1 年前的订单可以认为是冷数据 访问频率维度：按照内容的访问频率来区分冷热数据，例如，对于帖子系统，可以认为很长一段时间内，浏览量很低的文章是冷数据 当然这仅仅按照这两个维度有些时候不太适用，还是要根据业务实际情况来决定\n为什么要冷热分离 如果将冷热数据均存放在一个 DB 中，随着数据的增多，检索速率会下降，用户体验差\n如果引入冷热分离，将冷数据和热数据分开存储，能保证大部分用户的使用体验\n冷热分离的优缺点 优点：\n热数据的查询性能提高，大部分用户的使用体验较好 降低存储成本：热数据可以存储在 SSD 中，冷数据可以存储在 HDD 中 缺点：\n不稳定因素增加 统计整体数据时，速度较慢 如何实现 业务层\n业务层代码可以写一个后台任务，定期判断存在的冷数据，将其迁移到冷库，并从热库删除\n监听 binlog\n可以通过监听 binlog 来分析存在的冷数据，将其迁移到冷库，并从热库删除\n深度分页 概念 来看一下这个 SQL 语句：\nSELECT * FROM user LIMIT 1000000, 10 这种查询就叫做深度分页\n由于这类查询在分页的时候无法利用索引，要想获取第 1000000 ～ 1000010 条数据，就需要先 获取前 1000010 数据，然后再跳过前 1000000 条记录，再返回第 1000000 ～ 1000010 条数据\n可以看到效率很低，那么在无法避免深度分页的情况下，应该如何优化？\n如何优化 优化方式很简单：延迟关联\n使用子查询先获取第 1000000 ～ 1000010 条数据的 id 然后再使用内连接 即：\nselect * from user, (select id from user limit 1000000, 10) tmp where user.id = tmp.id; 优化的点在于：\n深度分页时，获取的只是 id，而不是所有数据 使用到了「覆盖索引」，避免回表查询（这也是 select id 的原因所在） 当然这里用到了子查询，需要创建一张临时表，也会产生一定的性能损失\n普通索引和唯一索引，应该如何选择？ 查询过程 普通索引，查到满足条件的第一个记录后，继续查找下一个记录，直到第一个不满足条件的记录 唯一索引，由于索引唯一性，查到第一个满足条件的记录后，停止检索 实际上，二者的性能差距是很小的，因为 InnoDB 读取数据的基本单位是「页」\n也就是说，虽然普通索引看起来查找记录的次数较多，但是由于一次性读取了一整张页（包括许多记录），性能不会比唯一索引差\n更新过程 在讨论更新过程的性能差异，需要引入一个概念：change buffer\n当需要更新一个 数据页：\n如果数据页在内存中就直接更新 如果不在内存中，需要将该数据页从磁盘读取到内存，然后在内存中更新（回写操作可以在后台执行） 可以发现，如果数据页不在磁盘中，就会涉及到 随机读 IO，这对性能的影响是很大的\nchange buffer 那么，引入 change buffer，在使用 普通索引 的条件下，就可以解决这个问题\n来看看引入 change buffer 的更新数据页的操作：\n如果数据页在内存中就直接更新 如果不在内存中，在不影响数据一致性的前提下，InnoDB 会将这些更新操作 缓存在 change buffer 中。 也就是说，使用 change buffer，避免了大量随机读 IO，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。\nchange buffer 是 可以持久化 的数据。在内存中有拷贝，也会被写入到磁盘上\nchange buffer 用的是 buffer pool 里的内存，change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。\n使用 change buffer，什么时候更新磁盘的数据呢？ 我们将 change buffer 中的操作应用到原数据页上，得到最新结果的过程，称为 merge\nmerge 的时机：\n后台线程定期 merge 数据库正常关闭，merge 访问该数据页的时候，会触发 merge change buffer 一开始是在内存的，如果掉电，会不会导致 change buffer 丢失？ 不会，分两种情况讨论：\nchange buffer 的操作会被记录到 redo log 里\n如果 redo log 已经落盘，那么重启可以根据 redo log 来恢复 change buffer\n如果 redo log 还没来得及落盘，重启后，会因为 redo log 没有落盘而回滚事务，丢失了 change buffer 也没关系\nchange buffer 的操作会被记录到 redo log 里，有了 redo log，为什么还要将 change buffer 持久化到磁盘？ 其实我也不太清楚，但感觉有点类似 Linux 的 Swap 机制？\n如果执行更新操作以后，一直没有 merge，那么 change buffer 的剩余空间会越来越少\n于是可以将 change buffer 持久化到磁盘，为新的更新操作流出空间，就好像 Linux 的 Swap 一样\n为什么唯一索引的更新不能使用 change buffer？ 唯一索引在更新数据时，需要 先判断更新的数据会不会与现有数据冲突，如果待更新数据页不在内存，会 不可避免的读取磁盘到内存 来判断\n既然数据都已经读到内存了，也就没有使用 change buffer 的必要了\n什么时候用 change buffer？ 在一个数据页做 merge 之前，change buffer 记录的变更越多，收益就越大。\n对于 写多读少 的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。\n反过来，假设一个业务的更新模式是 写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。\n这样 随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。\n因此是否使用 change buffer，还需要结合业务需求\n与 redo log 对比 你可能会问：redo log 的 WAL 机制，不也是减少随机 IO 吗，还要 change buffer 干嘛\n实际上，使用 redo log，可以将事务执行期间涉及到的 随机写 IO 转换成 顺序写 IO，真正的随机 IO 被推迟到后台线程进行\n但使用 redo log 不能减少随机读 IO：增删改一个数据还是要先从磁盘读取到内存\n而对于非唯一索引，在有 change buffer 的前提下，增删改就可以不从磁盘读取到内存，而是直接记录到 change buffer 中，减少的是 随机读 IO\n用一句话简单总结：redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。\n总结 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。\nchange buffer 因为减少了随机磁盘读取，所以对更新性能的提升很明显。\n由于唯一索引无法使用到 change buffer 来优化，因此 唯一索引的更新性能可能会不如普通索引\n如果业务允许的情况下，尽量使用普通索引，以提高读性能\n当然，使用了 change buffer，在某些场景下，不但不会提升性能，还会导致性能的下降，此时应该禁用 change buffer\n如何给字符串建索引 直接建立索引\n如果单个字符串的长度不是很长，直接建立索引，方便\n前缀索引\n如果要考虑索引占用的空间，可以考虑建立前缀索引\n在建立前缀索引时，要考虑 区分度，可以给定一个可以接受的区分度，然后确定索引字符串的长度\n-- 看看有多少个不一样的字段 select count(distinct email) as L from SUser; -- 看看 L4、L5 与 L 的差距，选择一个合适的 select count(distinct left(email,4)) as L4, count(distinct left(email,5)) as L5, count(distinct left(email,6)) as L6, count(distinct left(email,7)) as L7, from SUser; 但是建立前缀索引，就无法用到覆盖索引的优化了，必然回表查询\n哈希\n可以给表加一个字段，存储字符串的哈希值\n在存储数据时，记录下这个字符串的哈希值 在取出数据时，根据哈希值来取，然后 在判断字段是否完全一致 这种方式适合前缀区分度较低的场景，但是会引入一个额外字段，占用一定的存储空间\n倒序存储\n例如身份证，前缀的区分度低，就可以将 ID 倒序存储，然后建立前缀索引\nredo log 设置得太小会怎么样 这里引用 12 | 为什么我的 MySQL 会“抖”一下？ 的一条评论：\nredo log 是关系型数据库的核心啊，保证了 ACID 里的 D。所以 redo log 是牵一发而动全身的操作\n按照老师说的当内存数据页跟磁盘数据页不一致的时候，把内存页称为\u0026rsquo;脏页\u0026rsquo;。如果 redo log 设置得太小，redo log 写满.那么会涉及到哪些操作呢，我认为是以下几点:\n把相对应的数据页中的脏页持久化到磁盘，checkpoint 往前推 由于 redo log 还记录了 undo 的变化，undo log buffer 也要持久化进 undo log 当 innodb_flush_log_at_trx_commit 设置为非 1，还要把内存里的 redo log 持久化到磁盘上 redo log 还记录了 change buffer 的改变，那么还要把 change buffer purge 到 idb 以及 merge change buffer.merge 生成的数据页也是脏页，也要持久化到磁盘 上述 4 种操作，都是占用系统 I/O，影响 DML，如果操作频繁，会导致\u0026rsquo;抖\u0026rsquo;得向现在我们过冬一样。但是对于 select 操作来说，查询时间相对会更快。因为系统脏页变少了，不用去淘汰脏页，直接复用干净页即可。还有就是对于宕机恢复，速度也更快，因为 checkpoint 很接近 LSN，恢复的数据页相对较少\n所以要控制刷脏的频率，频率快了，影响 DML I/O，频率慢了，会导致读操作耗时长。\n因此，如果 redo log 设置太小，redo 很容易写满，会导致频繁刷盘，系统锁死，触发 checkpoint 推进，导致写操作卡住。由于主机 IO 能力很强，checkpoint 推进会很快完成，卡住的写操作又很快可以执行。循环往复，现象就是 写操作每隔一小段时间执行就会变慢几秒。\n合适的 redo log 大小非常重要\n此外，还有一个 innodb_io_capacity 选项，这个选项是告诉 InnoDB ：你的磁盘的 IO 能力，建议设置为磁盘的 IOPS\nInnoDB 会根据 innodb_io_capacity 来控制刷盘的速率，如果设置得太小，会导致写入速率很慢，表现就是：IO 没满，但是脏页很多，来不及写入\norder by 是怎么排序的？ 现在有一张 user 表：\ncreate table user ( id int auto_increment primary key, name varchar(10) not null, sex char null, phoneNum varchar(20) not null, email varchar(128) null, age int null, ); 且有主键索引、idx_sex\n注意，这里的 idx_sex 只是为了实验而创建\n由于 sex 的区分度太低，在实际应用不要根据 sex 创建索引\n执行以下 sql 语句：\nexplain select name, sex, age from user where sex = \u0026#39;F\u0026#39; order by age; 输出:\n+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+---------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+---------------------------------------+ | 1 | SIMPLE | user | NULL | ref | idx_sex | idx_sex | 5 | const | 6 | 100.00 | Using index condition; Using filesort | +----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+---------------------------------------+ 在 Extra 字段，提示 Using filesort，这是什么意思呢？\n全字段排序 在对没有建立索引的字段排序时，就会出现 Using filesort\nMySQL 会为每一个线程分配一个缓冲区用于存放待排序的数据，成为 sort_buffer\n对于上面的 SQL 语句，具体来说，MySQL 是这样做的：\n初始化 sort_buffer 根据 where 条件，利用 idx_sex，定位到第一个满足 sex = \u0026lsquo;F\u0026rsquo; 的节点，得到主键 id 根据主键 id，利用主键索引，取出 name、sex、age，将这个节点的行数据存放到 sort_buffer 中 定位到下一个节点，得到主键 id 重复 3、4，直到遍历完所有节点 对 sort_buffer 中的数据根据 age 字段来做 快速排序 返回数据给客户端 sort_buffer 是有大小限制的（sort_buffer_size），如果待排序的数据在 sort_buffer 无法完全存下，MySQL 会：\n将剩余数据存放到 磁盘 上的若干临时文件中 分别对这些文件的内容做 归并排序 递归地合并文件内容，直到合并成一个有序大文件 合并 sort_buffer 和文件内容，返回给客户端 rowid 排序 可以发现：如果待排序的数据很大，会使用到 磁盘 辅助排序，性能很差\n为了减少存储在 sort_buffer 的数据量，可以使用 rowid 排序\n与全字段排序不同，rowid 排序仅会在 sort_buffer 存储：主键 id、排序字段\n对于之前的 SQL 语句，使用 rowid 排序的过程如下：\n初始化 sort_buffer 根据 where 条件，利用 idx_sex，定位到第一个满足 sex = \u0026lsquo;F\u0026rsquo; 的节点，得到主键 id 根据主键 id，利用主键索引，取出 id、age，将这个节点的行数据存放到 sort_buffer 中 定位到下一个节点，得到主键 id 重复 3、4，直到遍历完所有节点 对 sort_buffer 的数据进行快速排序 遍历 sort_buffer 的数据，根据主键 id，回表查询 name、sex 字段 返回结果给客户端 虽然 rowid 排序在相同的 sort_buffer 下，可以在对更多行排序的情况下，不创建临时文件，降低磁盘 IO\n但是与全字段排序相比，rowid 多了一次回表查询\n因此，MySQL 不会优先选择 rowid 排序\n当单行数据大小超过 max_length_for_sort_data ，MySQL 才会使用 rowid 排序\n避免在 sort_buffer 排序 为什么要在 sort_buffer 排序呢？是因为数据是无序的\n这看起来是一句废话，但也表明了：如果数据本身就是有序的，那自然就不需要在 sort_buffer 排序\n我们可以为 sex、age 字段建立索引\ncreate index idx_sex_age on user(sex, age); 创建索引后，order by 的执行流程如下：\n执行器发现有 idx_sex_age 联合索引，于是判断不需要使用 filesort 利用 idx_sex_age 定位到第一个 sex = \u0026lsquo;F\u0026rsquo; 的节点，得到主键 id 根据主键 id 回表查询 name 由于 idx_sex_age 能保证在 sex 相同的情况下，age 有序，因此无需排序，重复 2、3 即可 返回结果给客户端 再来看看 explain 的结果：\nmysql\u0026gt; explain select name, sex, age from user where sex = \u0026#39;F\u0026#39; order by age; +----+-------------+-------+------------+------+---------------------+-------------+---------+-------+------+----------+-----------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------------+-------------+---------+-------+------+----------+-----------------------+ | 1 | SIMPLE | user | NULL | ref | idx_sex,idx_sex_age | idx_sex_age | 5 | const | 6 | 100.00 | Using index condition | +----+-------------+-------+------------+------+---------------------+-------------+---------+-------+------+----------+-----------------------+ 1 row in set, 1 warning (0.00 sec) 可以看到：没有 Using filesort，只有 Using index condition 了\n当然，可以创建联合索引 idx_sex_age_name，走覆盖索引，避免一次回表查询\nmysql\u0026gt; explain select name, sex, age from user where sex = \u0026#39;F\u0026#39; order by age; +----+-------------+-------+------------+------+--------------------------------------+------------------+---------+-------+------+----------+--------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+--------------------------------------+------------------+---------+-------+------+----------+--------------------------+ | 1 | SIMPLE | user | NULL | ref | idx_sex,idx_sex_age,idx_sex_age_name | idx_sex_age_name | 5 | const | 6 | 100.00 | Using where; Using index | +----+-------------+-------+------------+------+--------------------------------------+------------------+---------+-------+------+----------+--------------------------+ 1 row in set, 1 warning (0.00 sec) 总结 建议在待排序字段建立索引，避免 filesort 如果无法避免使用到 filesort，那么可以： 适当扩大 sort_buffer 的大小，避免在文件排序 适当提高 max_length_for_sort_data 的大小，避免使用 rowid 排序，多一次回表查询 基于临时表的排序机制 引例\n现在有一张表：\ncreate table words ( id int auto_increment primary key, word varchar(64) null ); -- 只有主键索引 并向其插入了 10k 行数据\n有一个业务需求：随机获取 words 中的三个单词，返回给用户\n一个简单的实现方式如下：\nselect word from words order by rand() limit 3; 那么这句话实际上做了什么呢？\n内存临时表 执行 explain 查询计划：\nmysql\u0026gt; explain -\u0026gt; select word from test0.words order by rand() limit 3; +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------+ | 1 | SIMPLE | words | NULL | ALL | NULL | NULL | NULL | NULL | 9980 | 100.00 | Using temporary; Using filesort | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------+ 1 row in set, 1 warning (0.00 sec) 可以发现，这句话使用到了 临时表 辅助查询，并且 需要排序，具体执行过程如下：\n创建临时表（memory 存储引擎），包含两个字段：随机值，单词 遍历 words 表，计算出每个 word 的 rand 值，插入到临时表中 依据 随机值 对临时表进行排序（排序过程可以参考 order by 是怎么排序的 ） 取出临时表的前三行数据，返回 word 给客户端 上述过程一共会扫描 20003 行数据（10000 + 10000 + 3），效率低，并且，随着 words 表的行数增加，执行速度会更慢\n磁盘临时表 所有的临时表都是内存临时表吗？\n并不是，如果内存临时表存放不下待排序的数据，会将内存临时表转化为磁盘临时表\n这个大小是由 tmp_table_size 参数控制的，默认为 16M\n对磁盘临时表的排序，规则与 order by 是怎么排序的 一致\nTopk 问题（优先队列排序算法） 执行以下 SQL 语句：\nset tmp_table_size=1024; -- 将当前会话的内存临时表设置为 1024 字节 set sort_buffer_size=32768; set max_length_for_sort_data=16; /* 打开 optimizer_trace，只对本线程有效 */ SET optimizer_trace=\u0026#39;enabled=on\u0026#39;; /* 执行语句 */ select word from words order by rand() limit 3; /* 查看 OPTIMIZER_TRACE 输出 */ SELECT * FROM `information_schema`.`OPTIMIZER_TRACE` \\G; 正常来说，rand_val + word 一行要占 6 + 8 = 14 字节，那么 10000 行数据应该会占用 140000 字节的空间，超过了内存临时表的大小，应该会使用磁盘临时表，做归并排序\n事实真的如此吗？\nOPTIMIZER_TRACE 有关 filesort 的输出如下：\n并没有使用磁盘临时表\n事实上，排序方式的选择还 与 limit 参数有关\n当 limit 后的数据（对于上面的示例，就是 3 行 rand_val + word）如果可以存放在内存临时表，那么 MySQL 会将其优化 TopK 问题，体现在 OPTIMIZER_TRACE 的输出上，就是这两行：\n关于 TopK 问题，可以看看 这道 leetcode 题目 为什么要使用优先队列优化呢？\n因为 limit 后，只要元素的前 n 个有序即可，利用优先队列解决，空间复杂度低，减少排序所需空间，一定程度上避免使用磁盘辅助排序\n随机排序的实现方式 无论是使用内存临时表还是磁盘临时表，扫描的数据都太多了，性能不佳，如何优化？\n方案一\n-- 获取 min_id、max_id select max(id),min(id) into @M,@N from t ; -- 产生一个介于 min_id、max_id 之间的随机值 set @X= floor((@M-@N+1)*rand() + @N); -- 获取随机值对应的行数据 select * from t where id \u0026gt;= @X limit 3; 这个方案效率很高，只扫描了 3 行数据，但是存在「空洞问题」\n如果 id 不连续，例如 1、2、6、7、8，那么选到 6、7、8 的概率会高很多，即概率分布不均匀\n要想解决这个问题，可以：\n保证 id 连续自增 删除数据时，采用逻辑删除 方案二\n-- 获取行数 c select count(*) into @C from t; -- 生成介于 1～c 之间的随机数 set @Y1 = floor(@C * rand()); set @Y2 = floor(@C * rand()); set @Y3 = floor(@C * rand()); -- 获取对应行数据 select * from t limit @Y1，1; select * from t limit @Y2，1; select * from t limit @Y3，1; 总扫描行数为：C+(Y1+1)+(Y2+1)+(Y3+1)，可以利用「大表分页查询思想」进一步优化为：\n-- 获取行数 c select count(*) into @C from t; -- 生成介于 1～c 之间的随机数 set @Y1 = floor(@C * rand()); set @Y2 = floor(@C * rand()); set @Y3 = floor(@C * rand()); -- 获取对应行数据 select * from t limit @Y1，1; -- 应用层可以保存 id1 select * from t where id \u0026gt; id1 limit @Y2 - @Y1，1; -- 应用层可以保存 id2 select * from t where id \u0026gt; id2 limit @Y3 - @Y2，1; 总扫描行数为：C+Y3\n为什么 limit n order by \u0026lt;字段\u0026gt; 会有重复值？ 前面提到了：order by 的排序规则与 limit 的参数有关：\n如果 limit 后的数据 可以 存放在内存临时表，采用优先队列（堆）排序 如果 limit 后的数据 不可以 存放在内存临时表，采用归并排序 例如这个场景：\n第一次分页查询：order by rand() limit n, a，无法存放在内存临时表，采用归并排序 第二次分页查询：order by rand() limit n + a, a，可以存放在内存临时表（最后一页，数据较少），采用优先队列（堆）排序 就有可能出现两次分页查询包含重复值的情况，本质上还是因为 优先队列排序不是稳定的\n如果要避免这种情况，可以在排序依据加上唯一字段（例如主键 id）\n总结 在查询使用到临时表时，如果还要排序，并且排序字段没有建立索引，会触发临时表排序机制 内存临时表有大小限制，超过后，会换成磁盘临时表，采用归并排序 基于临时表排序时，如果有 limit，并且 limit 后的数据 可以 存放在内存临时表，采用优先队列（堆）排序 如果要避免 limit n order by \u0026lt;字段\u0026gt; 会有重复值 ，可以在排序依据加上唯一字段（例如主键 id） 参考资料 MySQL 实战 45 讲 | 如何正确展示随机消息 如何临时“提高” MySQL 的性能 如果连接数过多导致 MySQL 无法连接，可以关闭一些空闲连接 如果是连接请求 QPS 过高，可以临时关闭 MySQL 的连接鉴权 如果是单个 SQL 语句的 QPS 过高，可以重写该 SQL 语句为 select 1，将 QPS 降低为 0 详细内容参考 MySQL 有哪些“饮鸩止渴”提高性能的方法？ 误删数据如何及时补救？ 分情况讨论：\n误删了某一行的数据\n这种情况可以使用 FlashBack 来做数据恢复\nFlashBack 做数据恢复的原理是通过修改 binlog，然后拿到主库重放，要求 binlog 的行格式为 row\n对于 delete 语句，在 binlog event 的类型为 delete_rows event ，可以修改为 write_rows event\n注意：恢复数据时，应该先恢复到临时实例，确定没问题再同步到主库\n误删了一整张表\n删除了整张表，使用 binlog 来恢复数据就不可能了，因为仅仅记录了一个 drop\n这种情况，要求定期做全量备份，并且实时备份 binlog\n例如：\n最近的一个备份点为今天的 0:00 取出这个全量备份，创建一个临时库 取出今天 0:00 到现在的所有 binlog 备份 使用 binlog 做增量恢复（去除误删除的语句） 全量同步可能比较耗费时间，如果业务不允许这么长恢复时间，可以 搭建一个延迟复制的从库\n例如，一个从库与主库的延迟为 1h，那么在这 1h 内，只要发现了有误删除，就可以使用延迟复制的从库快速恢复数据：\n发现误删除，stop slave 取出最近 1h 内的 binlog，跳过误删除语句，做数据恢复 可以看出：恢复速度与主从延迟时间相关，延迟时间越长，恢复时间越长，但允许了更长的时间来发现误删除现象\n无论如何，这些都是补救措施，我们需要优先思考的是如何避免，而不是如何补救\n开发的 权限控制 确保 SQL 语句的 where 条件 SQL 审计 \u0026hellip; 修改生产的数据，或者添加索引优化，都要先写好四个脚本：备份脚本、执行脚本、验证脚本和回滚脚本。备份脚本是对需要变更的数据备份到一张表中，固定需要操作的数据行，以便误操作或业务要求进行回滚；执行脚本就是对数据变更的脚本，为防 Update 错数据，一般连备份表进行 Update 操作；验证脚本是验证数据变更或影响行数是否达到预期要求效果；回滚脚本就是将数据回滚到修改前的状态。\n虽说分四步骤写脚本可能会比较繁琐，但是这能够很大程度避免数据误操作。\n\u0026mdash; 来自 误删数据后除了跑路，还能怎么办？ 下的评论\nMySQL 发送查询结果的过程 MySQL 是 边读边发 的，这意味着，如果客户端不及时接收 MySQL Server 的数据，MySQL 就无法读取剩余数据并发送，导致整个事务时间变长\n有两个参数控制客户端接收数据的方式\nmysql_store_result 启用 mysql_store_result 参数，客户端会将查询结果 暂存到本地缓存 中，直到将所有查询结果接收完毕后，才会返回给应用程序\n这种方式可以防止 MySQL Server 因为 Client 无法及时接收数据导致事务「阻塞」\nmysql_use_result 如果单个查询结果集太大，客户端的内存可能不足以接收所有数据\n这种情况就只能启用 mysql_use_result 参数，客户端 逐行处理 查询结果\n但是如果单行结果的处理时间过长，会导致整个事务的时间变长，加锁时间变长，影响 MySQL Server 的并发能力\n总结 如果客户端的内存比较充足，尽量使用 mysql_store_result 选项，将查询结果缓存起来，减少 MySQL Server 等待 Client 接收数据的时间，进而减少单个事务的时间，提高 MySQL Server 的效率\n当然，如果单次返回的数据太多，那就只能用 mysql_use_result 了\n到底能不能使用 join？ 先创建两张表：\ncreate table t0 ( id int primary key, a int, b int, key idx_a(a) ); create table t1 like t0; 假设 t0 有 100 行数据，t1 有 1000 行数据\n被驱动表上，join \u0026hellip; on 字段有索引 例如：\nselect t0.* from t0 straight_join t1 on t0.a = t1.a; 执行过程如下：\n遍历 t0 的所有行数据 对 t0 的每一行数据，在 t1 根据 idx_a 去查询，看看有没有符合条件的行，如果有，扔到结果集 返回结果给客户端 总扫描行数近似为 100 + 100 * log(100)，整个过程的时间复杂度为 100 * log(1000)\n如果不使用 join，而是：\n应用程序执行 select * from t0; 对于每一行数据，执行 select * from t1 where t1.a = ?; 总扫描行数与时间复杂度可以认为不变，但是客户端与 MySQL Server 需要 101 次交互，效率较低\n因此，这种情况下，使用 join 比不使用 join 会更好\n被驱动表上，join \u0026hellip; on 字段无索引 例如：\nselect t0.* from t0 straight_join t1 on t0.a = t1.b; 执行过程如下：\n遍历 t0 的所有行数据 对 t0 的每一行数据，在 t1 全表查询，看看有没有符合条件的行，如果有，扔到结果集 返回结果给客户端 总扫描行数近似为 100 + 100 * 1000，时间复杂度为 100 * 1000（平方级）\n可以发现，在 join \u0026hellip; on 字段无索引的情况下，执行效率极低\n为了提高速度，MySQL 引入了 join_buffer，用于暂存驱动表的数据\n执行过程如下：\n将 t0 所有行数据存到 join_buffer 扫描 t1 的所有行，逐行与 join_buffer 的数据做对比，看看有没有符合条件的行，如果有，扔到结果集 返回结果给客户端 虽然总扫描行数与时间复杂度没有变化，但是在内存中比对的速度会更快\n如果 join_buffer 存不下 t0 的数据，会发生什么？\n与之前的处理方式不同，如果存不下，不会借用磁盘，而是将 t0 的数据分成多个段，存放到 join_buffer\n执行过程如下：\n将 t0 第 1 ～ n 行存到 join_buffer 扫描 t1 的所有行，逐行与 join_buffer 的数据做对比，看看有没有符合条件的行，如果有，扔到结果集 清空 join_buffer 将 t0 第 n+1 ～ m 行存到 join_buffer 扫描 t1 的所有行，逐行与 join_buffer 的数据做对比，看看有没有符合条件的行，如果有，扔到结果集 \u0026hellip; 返回结果给客户端 可以发现，如果分段越多，全表扫描 t1 的次数会越多，效率就越低\n注意： join_buffer 是无序的，要想知道一个 t1.b 是否与 join_buffer 中的某一行的 b 相等，需要遍历 join_buffer 的所有数据\njoin_buffer 太小，会发生什么 根据上面的分析，如果 join_buffer 太小，会导致分段次数增加，进而导致 全表扫描被驱动表的次数增加，会造成一系列连锁反应\n如果 被驱动表 是一张 大表 ，并且存放的是 冷数据，会发生什么？\n根据上面的理论，如果 join_buffer 不足，会多次全表扫描被驱动表\n又因为被驱动表是一张大表，全表扫描的时间可能较长，会在 LRU 链表的 old 区域停留超过 1s，那么 LRU 的优化就不起作用了，导致大量冷数据进入 young 区，把原来的热点数据挤出来，进而导致 buffer pool 的命中率严重下降，磁盘 IO 迅速上升，给整个系统带来很大压力\n此外，由于系统的负载上升，所有 事务的执行都会变慢，进而导致：\nundo log 回收慢，导致 undo log 版本链过长，快照读的时间变长（这会影响许多查询） 需要更长的时间后，才能释放锁，系统的 并发能力下降 join 时，应该让小表驱动大表，还是大表驱动小表 经过上面的分析，可以总结出：\n被驱动表上，join \u0026hellip; on 字段有索引：应该让 小表驱动大表，降低时间复杂度 被驱动表上，join \u0026hellip; on 字段无索引：应该让 小表驱动大表，减少 join 时的分段次数，进而减少全表扫描的次数 小表指的就是行数少的表吗？\n这个说法并不绝对，例如：\nselect t0.*, t1.id from t0 straight_join t1 on t0.a = t1.a; 虽然 t0 只有 100 行，但是查询的是所有字段，而 t1 查询的仅仅是 id 字段，可能 1000 行 id 占的总空间 还没有 100 行 t0.* 多\n在这种情况下，可以认为 t1 是小表\n总结 到底能不能用 join？\n如果被驱动表上，join \u0026hellip; on 字段 有索引，那么就可以使用 join，效率更高\n相反的，如果没有索引，那么尽量不要使用 join，因为如果多次分段，代表着多次全表扫描，会引起一系列雪崩反应\n使用 join 时，应该让小表驱动大表，提高效率（当然 MySQL 会选择合适的表来做驱动表，如果发现不符合预期，可以使用 straight_join 来指定）\njoin 如何优化？ MRR 先来看看回表查询的一个问题：\n在回表查询时，是一行一行的回表，还是一批一批的回表？\n以 select * from user where age \u0026gt;= 18; 作为示例\n一行一行地回表，由于 id 不是连续的，每一次回表都是一次 随机 IO，效率低\n为了将随机 IO 近似 转化为顺序 IO，MySQL 引入了 MRR（Multi-Range Read）的概念\n使用 MRR 优化后，执行过程如下：\n开一个 read_rnd_buffer 将符合条件的主键 id 集合放到 read_rnd_buffer 中 对 read_rnd_buffer 的主键 id 排序 按照排序后的主键 id 一行一行的回表，得到结果集 如果 read_rnd_buffer 不足以存放下所有的主键 id，清空 read_rnd_buffer，重复 2、3、4 MRR 的适用场景是：主键 id 是 有序 的，且是 范围查询\n如果主键 id 是随机的，例如 UUID，那么即使对主键 id 排序，回表时也还是随机 IO，MRR 的优势就体现不出来了\n因此，为了利用 MRR 优化，建议将主键 id 设置为自增的\nBKA 前面提到：在 被驱动表上，join \u0026hellip; on 字段有索引 的情况下，会先扫描驱动表，然后 一行一行 的去被驱动表查\n如果一行行地查，就无法使用到 MRR 的优化\n于是引入 BKA（Batched Key Access），一次性查一批数据，就好像范围查询一样\n使用 BKA 优化后，执行过程如下：\nBKA 复用 join_buffer 将驱动表的数据放到 join_buffer 将 join_buffer 的数据作为查询条件，批量地在被驱动表查询（这个过程会使用到 MRR），得到结果集 如果 join_buffer 不够，重复 2、3 根据上述分析：使用 BKA 优化的前提，是启用了 MRR 优化\n因此，启用 BKA 的方法如下：\nset optimizer_switch=\u0026#39;mrr=on,mrr_cost_based=off,batched_key_access=on\u0026#39;; BNL 的优化 前面提到的 被驱动表上，join \u0026hellip; on 字段无索引 这种情况，使用的算法称作 BNL（Block Nested-Loop Join）\nBNL 算法的性能很不理想，如果多次分段，还有可能会引起一系列雪崩反应\n优化 BNL 的方式，就是 将 BNL 转化为 BKA，即在被驱动表的 join on 字段 加上索引\n但是如果被驱动表太大，并且这个 SQL 还是个低频 SQL，那么加索引的成本就太高了\nselect * from t1 join t2 on (t1.b=t2.b) where t2.b\u0026gt;=1 and t2.b\u0026lt;=2000; 可以创建临时表来解决这个问题：\n-- 创建临时表 create table tmp like t2; -- 建索引 create index idx_b on t2(b); -- 插入数据 insert into tmp (select * from t2 where t2.b\u0026gt;=1 and t2.b\u0026lt;=2000); -- 得到结果 select * from t1 join tmp on (t1.b=tmp.b); hash join 使用 BNL 算法时，在查找 join_buffer 中是否包含满足条件的行，这个操作是 O(n) 的时间复杂度\n这是因为 join_buffer 维护的是一个无序列表（可以看成数组），查找的过程是线性的\n这种查找很慢，因此在 MySQL 8.0.18 版本引入了 hash join，将 join_buffer 维护成一个哈希表，这样查找效率得到大幅提高，可以大幅减少扫描行数\nhash join 是默认启用的\n总结 MRR 将回表查询的随机 IO 近似转化为顺序 IO BKA 使用 join_buffer 缓存驱动表的数据，保证可以利用 MRR 优化 IO，建议启用 优化 BNL 算法的方式是将 BNL 转化为 BKA，即建立索引 如果建立索引的成本过高，可以通过创建临时表来优化 hash join 的原理是维护一个 join_buffer 哈希表，减少扫描行数 group by 优化 group by 的原理 以这个 SQL 语句说说 group by 的流程：\nmysql\u0026gt; explain -\u0026gt; select (id % 10) m, count(*) from user group by m; +----+-------------+-------+------------+-------+-----------------------------------------------------------------------------------+---------+---------+------+------+----------+------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+-----------------------------------------------------------------------------------+---------+---------+------+------+----------+------------------------------+ | 1 | SIMPLE | user | NULL | index | PRIMARY,idx_phone,idx_email,idx_name,idx_age,idx_sex,idx_sex_age,idx_sex_age_name | idx_age | 5 | NULL | 7 | 100.00 | Using index; Using temporary | +----+-------------+-------+------------+-------+-----------------------------------------------------------------------------------+---------+---------+------+------+----------+------------------------------+ 1 row in set, 1 warning (0.00 sec) 执行过程如下：\n创建内存临时表 tmp，包含两个字段：m, count 根据 idx_age 全表扫描，如果： tmp 表包含 m，那么 count + 1 否则，插入一条新的数据 优化 使用 group by，避免不了全表查询，那么，就只能 避免创建临时表 了\n如何避免？\n先要弄清楚为什么要创建临时表\n由于 id % 10 这个字段是 无序 的，要想统计不同的 id % 10 的 count，就只能创建一个临时表来暂存数据\n如果 id % 10 是有序的，那么就不用创建临时表，扫描一遍即可：\n因此，最直接的方法就是给 group by 的字段 创建索引\n如果创建索引的代价太大呢？\n可以适当调大 tmp_table_size，避免转换为磁盘临时表 如果数据量太大，使用 SQL_BIG_RESULT 提示 MySQL 直接使用磁盘临时表，避免多一次不必要的转换过程： select SQL_BIG_RESULT (id % 10) m, count(*) from user group by m; 有没有必要使用 Memory 引擎？ Memory 引擎最大的优势是读写速度快，但缺点也有几个：\n锁的粒度比较粗，只支持表锁 断电丢失数据 断电重启，主从数据不一致 因此，不建议在生产环境上使用 Memory 引擎，虽然读写速度快，但是并发能力弱，断电还会丢数据\n此外，InnoDB 有 buffer pool 的加持，缓存命中后，读写速度也是很不错的，并发能力也比较强\n但是有一个例外，就是在 BNL 的优化 提到的创建临时表\n在这种情况，如果数据量不大，就可以使用 Memory 引擎，快速得到结果，用完删掉即可：\n-- 创建临时表，并在字段 b 上建立 hash 索引 create table tmp(id int primary key, a int, b int, index (b))engine=memory; -- 插入数据 insert into tmp (select * from t2 where t2.b\u0026gt;=1 and t2.b\u0026lt;=2000); -- 得到结果 select * from t1 join tmp on (t1.b=tmp.b); -- 删除表 drop table tmp; 自增主键为什么不是连续的？ 插入数据时，出现冲突 例如：\n要想弄清楚原因，需要知道 MySQL 是如何申请一个 id 的\n以 insert into t0 values (null, 5, 4); 为例：\n加 AUTO_INC 锁 用户没有指定主键 id，因此，读取 内存 中，表 t0 的自增值 5 将自增值 + 1 释放 AUTO_INC 锁 插入数据，冲突 可以发现：自增值 + 1 这个操作是在插入数据 之前 的，这是导致主键不连续的第一个原因\n事务回滚 例如：\n为什么在插入冲突、事务回滚时，MySQL 不回滚自增 id 呢？ 主要还是 性能因素\n假设在插入冲突、事务回滚时，MySQL 回滚自增 id\n事务 A 申请了一个主键 id = 2 事务 B 申请了一个主键 id = 3 事务 B 提交 事务 A 插入冲突，或者回滚，MySQL 回滚自增 id = 2（这里的回滚，相当于 id = 2 可用） 那么下一个事务要申请新的主键 id 就有可能出现重复的问题，为了解决，有两种方案：\n方案一：在申请主键 id 时，从 1 开始申请，逐个判断有没有使用，如果没有，申请成功 方案二：AUTO_INC 锁的范围扩大到整个插入语句，即事务提交才释放 方案一的效率太低了，方案二的锁粒度大，并发能力太弱了\n因此，MySQL 选择不回滚自增 id 以保证性能\n批量申请 id 策略 在批量插入数据时，如果插入的行数比较多，例如 10w 行，那么就要申请 10w 次 AUTO_INC 锁，并发能力不强\nMySQL 在申请自增 id 采取批量申请的策略，即第一次申请 1 个，第二次申请 2 个，第三次申请 4 个\u0026hellip;\n如果申请的 id 没有用完，也不会回滚，原因同上\n自增 id 用完了，会发生什么？ 对于主键自增 id，如果用完了，再次申请，它的值不会发生改变，因此，插入时会报错（主键冲突）\n如果一张表没有主键，MySQL 会生成一个隐式的 row_id\n一个 row_id 占用 6 字节，如果用完了，再次申请，会 从头开始，也就是说，会产生 数据覆盖\n简单总结一下：\n主键自增 id 的策略保证的是数据的可靠性，而 row_id 的策略是保证的可用性\n生产上建议每张表都要有一个主键 id，保证数据可靠性\n","permalink":"https://blogs.skylee.top/posts/mysql/%E6%9D%82%E9%A1%B9/note/","tags":["MySQL"],"title":"MySQL 杂项"},{"categories":["MySQL"],"content":" 主从复制 常见主从模型 常见的主从模型有以下几种：\n一主一从 一主多从 互为主从 主从复制原理 简单来说，就是从库的 IO_Thread 将主库发来的 binlog 写到本地的 relay log，然后由 SQL_Thread 负责拉取 relay log 完成主从复制\nbinlog 是主库主动推送，还是从库主动拉取？\n一开始创建主备关系的时候，是由备库指定的。\n比如基于位点的主备关系，备库说“我要从 binlog 文件 A 的位置 P”开始同步， 主库就从这个指定的位置开始往后发。\n而主备复制关系搭建 完成以后，是 主库 来 决定“要发数据给备库” 的。\n所以主库有生成新的日志，就会发给备库。\n循环复制 生产上常常使用双主（互为主从）结构，这样主节点宕机了，还可以切换到从节点，保证可用性\n但双主结构存在一个 循环复制 的问题：\n假设一个业务代码向 A 库写入了一个数据，那么 A 库会产生相应的 binlog，并发送给 B 库\nB 库收到 A 库的 binlog，也会写入这个数据，并生成相应的 binlog（建议将 log_slave_updates 设置为 on，表示从库执行 relay log 以后，也会生成 binlog）\nB 库当然要把最新的 binlog 发给 A 库，但这个 binlog 记录的数据是 重复 的\n如果 A 库再执行一次这个 binlog，就相当于循环复制了\n如何解决这个问题？\n事实上，binlog 还 记录了生产该日志的 server id，如果收到的 binlog 的 server id 与自己是一样的，就不执行\n因此，要避免循环复制，需要：\n每个库的 server id 不同 从库重放 binlog 时，生成的新 binlog 的 server id 必须与原 binlog 一致 那么，保证了上面两点就能绝对避免循环复制吗？\n并不是\n场景一\ntrx1 是在 C 库执行的，server_id 为 C 库的 server_id\n然后，A、B 搭建成双主结构，由于 server_id 不是 A、B，因此循环复制\n解决方法就是：临时忽略 C 库的 server_id\n可以在 A 或者 B 上执行：\nstop slave； CHANGE MASTER TO IGNORE_SERVER_IDS=(server_id_of_C); start slave; 这样这个节点收到这个 binlog 就不会执行，也就避免了循环复制\n同步完毕以后再改回来\n场景二\n如果主库发送完 binlog 之后，修改了自己的 server id，等 binlog 传回来以后，就会认为自己的 server id 与 binlog 中的 server id 不一致，造成循环复制\n那为什么还要允许运行时修改 server id 呢？设置为只读不就解决了？\n逆向修改 server_id，反而可以解决死循环问题\n例如上面的示例：可以将 A 库的 server_id 临时修改成 C，不就避免了死循环吗\n主从延迟 从库要想同步主库的数据，肯定需要一定的时间，这段时间就叫做主从延迟时间\n主从延迟期间，如果有客户端读取从库的数据，可能会读取到旧数据，因此，需要尽可能缩短主从延迟\n可以通过在从库执行 show slave status 来查看 seconds_behind_master（SBM）的值\n出现场景 将整个主从复制分为三个时间点：\n主库完成一个事务，写入 binlog，这个时刻记做 T1 从库收到主库的 binlog，记做 T2 从库执行完这个事务，记做 T3 那么主从延迟的时间 T = (T2 - T1) + (T3 - T2)\nT2 - T1 比较好理解：就是 网络延迟时间，如果网络波动比较大，有可能造成主从延迟时间较长，一般来说，网络延迟时间是比较短的，也不太好优化\n所以，主从延迟的原因主要取决于：从库接收到 binlog 到处理完事务的时间差\nT3 - T2 又可以细分：\nIO_Thread 写入 relay log 的过程 SQL_Thread 执行 relay log 的过程 来讨论一下一些导致 T3 - T2 时间较长的场景：\n从库性能太差\n一般来说，为了保证业务的可用性，我们总是偏向于提高主库的配置，从库的配置可能较低\n例如：主库使用 SSD，而从库使用 HDD，写入速度太慢，IO 造成瓶颈\n解决方案：\n现在一般讲究一个「对称部署」，为了保证可用性，主库挂了，从库随时要来顶住，因此主库与从库配置相同\n从库压力太大\n我们遇到的大多场景都是读多写少\n如果做读写分离：主库 write only，从库 read only\n从库同步的过程中，如果读的 QPS 太高了，从库压力太大，CPU、IO 都被大量读请求占用，自然主从延迟就变高了\n解决方案：\n部署多个从节点，将读请求分发到不同的从节点上，降低单个从节点的压力（当然数据不一致的可能也相应增加）\n大事务\n例如一个事务在主库执行 10min，那么只有等到这个事务执行完毕以后，才会写入 binlog\n从库收到这个 binlog，也需要花较长时间才能执行完这个事务，这个期间，主从都是不同步的\n因此，总是听到：“不要一次性 delete 太多数据”\n解决方案也很简单：就是避免大事务的产生\n从库的并行复制能力\n这里的「复制」指的是 SQL_Thread 执行 relay log 的过程\n早期的 MySQL（5.6 版本之前）SQL_Thread 是单线程的\n如果 binlog 很多，单线程可能就处理不过来了，造成积压，主从延迟升高\n为什么从库会延迟好几个小时？ 如果从库的 复制能力太弱，就有可能出现这种延迟几个小时的情况\n为了提高复制能力，很容易想到多线程处理\nSQL_Thread 变成了一个 Coordinator 和若干个 Worker Thread\nCoordinator 不负责 relay log 的实际处理，而是负责将 relay log 分发到不同的 Worker\nWorker 就专注于执行事务就可以\n那么重点来了：Coordinator 分配 relay log 的原则是什么？\n不能产生更新覆盖，这就要求对同一行的更新事务，必须放在同一个 Worker 中 同一个事务不能拆开，只能由一个 Worker 处理 第一点的「更新覆盖」的场景：\n两个事务，都想修改第 row 行的数据\n事务 A 先执行，将这一行的数据修改为 data0 事务 B 后执行，将这一行的数据修改为 data1 在主库，更新顺序为 A-\u0026gt;B，那么在从库，这个顺序也应该是 A-\u0026gt;B，否则可能出现主从不一致问题\n5.6 版本 MySQL 解决方案 5.6 版本的 MySQL 引入了并行复制，即 SQL_Thread 变成了多线程，但是粒度比较粗，是 基于「库」的，不同库的事务肯定是可以并行执行的\n这个方式在每个库的负载都比较均衡的场景下比较有用，能起到不错效果\n但是，如果业务上就一个库，或者不同 DB 的热点不同，比如一个是业务逻辑库，一个是系统配置库，那就起不到并行的效果。\n5.7 版本 MySQL 解决方案 因此，5.7 版本的 MySQL 又额外引入了一种策略：LOGICAL_CLOCK，基本思想如下：\n如果主库的两个事务都处于 prepare 状态，那么这两个事务可以在从库上并行执行 如果主库的两个事务，一个处于 prepare，一个处于 commit，那么这两个事务可以在从库上并行执行 只要若干个事务同时处于 prepare 状态（包括 commit 状态），那么说明这些事务肯定是客户端发起了 commit 请求，自然可以并行执行\n这种方案比起 5.6 版本的方案，更具有适用性\n还可以调整 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数（这两个参数是用于控制 fsync 的），来延长 prepare 状态的时间，以此产生更多「同时」处于 prepare 状态的事务，进而提交从库的并发度\n但是这个方式还存在局限：\n假设一个主库，单线程插入了很多数据，过了 3 个小时后，我们要给这个主库搭建一个相同版本的备库。\n由于已经过了 3 个小时，此时同时处于 prepare 状态的事务可以说是很少的，绝大多数事务已经被提交了\n此时如果采用 LOGICAL_CLOCK 策略，从库的复制过程就退化成单线程了，效率不高\n5.7.22 版本 MySQL 解决方案 为了解决上面这个场景的问题，5.7.22 版本 MySQL 引入了 WRITESET 策略，基本思想如下：\n在主库，对于一个事务涉及到的 每一行，计算出这一行 库名 + 表名 + 索引名 + 值 的哈希值\n将这些哈希值组成一个 write_set 集合，写到 binlog 里面\n从库收到 binlog，coordinator 会看看两个事务的 write_set 有没有交集：\n如果没有交集，说明这两个事务涉及完全不同的行，可以并行操作 主从切换 来看看主从切换的大体步骤：\n从库循环检查自己的 SBM 是否小于某个值（比如 5s），如果小于，进入下一步 将主库设置为 Read Only 从库继续同步数据，知道 SBM 等于 0 将从库设置为可读写 将业务请求切换到从库 可以看到，在主从切换期间，是存在 服务不可用 的：主、从均处于 Read Only\n如果要确实要保证下游服务的可用性，并且可以接受短期的数据不一致，可以直接将从库设置为可读写，然后把请求路由到从库，从库后台同步主库的数据，最后完成切换（不推荐）\n可以看出 MySQL 的可用性，很大一部分因素在于主从延迟，如果一开始 SBM 就比较短，那么整个切换过程中，服务不可用的时间就很短了\n主库出问题，从库怎么办？ 主库出问题，从库怎么办？\n按照正常流程，应该要选举出一个从节点，然后走主从切换的流程\n但问题是：其它从节点应该怎么去同步新的主节点的数据呢？\n难点：从库去同步新的主库的数据，不是全量同步，而是增量同步，如何寻找这个同步的「起始点」？\n基于位点的主从切换\nCHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password MASTER_LOG_FILE=$master_log_name -- 指定主库同步的文件 MASTER_LOG_POS=$master_log_pos -- 指定同步的位置 问题就是如何确定同步的起始位置\n使用这种方法，只能获取一个 大概的 的位点：\n等待新的主库将 relay log 同步完成 获取新的主库的 file（show mater status） 取原主库的故障时刻 T 使用 mysqlbinlog 解析新主库 file 在 T 时刻的位点 mysqlbinlog File --stop-datetime=T --start-datetime=T 基于 GTID 的主从切换\n上面的方式太麻烦了，还容易出错\nMySQL 5.6 引入了 全局事务 ID（GTID），是一个事务的唯一标识\n在 GTID 模式下，从库获取新的主库的位点这件事情就在 MySQL 内部做好了：\nCHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password master_auto_position=1 -- 指定使用 GTID 确定位点 想要更深入的了解 GTID 的实现，可以看看这篇文章：27 | 主库出问题了，从库怎么办？ 读写分离 什么是读写分离 读写分离，指的是将 DB 的读写操作分配到不同的节点执行，例如：主库只进行写操作，从库只进行读操作\n采用读写分离，可以提高整个 DB 集群的读能力\n读写分离如何实现 在 MySQL 集群搭建完毕以后，应用层一般有两种方式实现读写分离：\n应用层代码手动实现 基于 Proxy（代理层）实现 应用层代码手动实现\n应用程序主动做负载均衡，这种模式下一般会把数据库的连接信息放在客户端的连接层。也就是说，由客户端来选择后端数据库进行查询。\n这种方式的性能较好，缺点就是需要对已有代码进行修改\n基于 Proxy（代理层）实现\n这个方式就是在 Client 到 Server 之间引入一个代理，自动分发客户端的请求：\nProxy 会负责请求的分发与负载均衡\n这种方式的优点就是不用修改已有代码，但是有一定的性能损耗\n问题 引入读写分离，虽然可以提高读写性能，但会带来 数据不一致 的问题：\n前面提到主从复制是需要时间的，当一个业务修改了表中的某一个数据，同步到其它从库需要时间\n如果刚刚修改完数据，就去从库访问的话，就有可能访问到过期数据\n如何解决？\n强制走主库\n一些对数据一致性要求很高的业务（如金融），可以采取这种方式，保证绝对的数据一致性\n这种方式比较简单，缺点就是：如果很多业务都有很高实时性要求，都走主库，会 对主库造成很大压力\nSleep\n既然同步需要时间，那我们可以「sleep」一下，等它同步完了再去访问不就好了吗\n例如，一个用户发布了一个商品，前端可以基于本地已有信息进行展示，就好像获取到了最新的数据一样\n用户待会刷新页面，其实已经过了一段时间，也就达到了 sleep 的目的，只要等待时间内同步完成，访问的就是最新的数据\n这种方式适用于对数据实时性要求低的业务，就算访问到过期数据也影响不大\n缺点就是：我们无法保证在规定时间内主从同步完毕，还是 有访问到过期数据的可能性\n判断主从无延迟方案\n这种方式比较折中\n在访问数据之前，判断主从是否同步完毕： 如果同步完毕，那么走从库，获取的是最新数据 如果没有，可以等待一段时间，重复第一个步骤 如果等待时间 超过一定阈值，那么根据实际需求，看看是走主库，获取最新数据；或者走从库，获取过期数据 这种方式看起来比较灵活，可以根据实际需求选择走主库还是走从库\n如何判断主从是否同步完毕？\n可以使用 show slave status 查看 SBM，如果为 0，说明 可能 同步完毕。但这个方式判断不太精确 可以对比位点确保主备无延迟 可以对比 GTID 集合确保主备无延迟 分库分表 什么是分库 分库，指的是将一个数据库的数据分散到不同的数据库上，以减轻单个 DB 的压力\n分库有两种方式：\n垂直分库 水平分库 垂直分库指的是：按照业务，将一个库的多个表分到不同的库上，不同业务使用不同的数据库\n例如，一个数据库内有：User 表、Post 表、Comment 表，如果要垂直分库，就可以将这三张表分到不同的库中\n水平分库指的是：同一个表，按照一定「分片规则」，分配到不同的库中\n例如，一张 Comment 表，做水平切分，id 为 1 ～ 10000 的存在第一个 DB，id 为 10001 ～ 20000 的存在第二个 DB\n什么是分表 分表，指的是将一张表的数据拆分成多张表，提高单表查询效率\n分表也是两种方式：\n垂直分表 水平分表 垂直分表指的是：将一张表按照不同的字段，拆分成多张表\n例如 User 表有 name、sex、addr、phone、email 字段，可以将 User 表拆成 user_base(id, name, sex) 和 user_extra(id, addr, phone, email)\n水平分表指的是：按照一定的「分片规则」，将一张表的若干行数据拆分成到不同的表，解决单表数据太多导致检索能力下降问题\n例如，对于 User 表，id 为 1 ～ 10000 的存在表 user_0，id 为 10001 ～ 20000 的存在表 user_2\n水平分表只能解决单表数据量太大的问题，为了提高整体的效率，一般与水平分库结合使用\n什么时候需要分库分表 单表数据量太大，检索能力下降 数据库存放的数据太多，备份时间很长 并发请求高，一个 DB 扛不住 常见分片算法 哈希分片\n根据指定 key（比如 id） 的哈希值，算出这个数据在哪个库（表）中\n哈希分片算法比较适合 随机 查询的场景，并能 一定程度避免单个库上的热点问题，不适合范围查询\n范围分片\n按照指定范围区间来分配数据，例如，将 id 为 1 ～ 10000 的存在第一个 DB，id 为 10001 ～ 20000 的存在第二个 DB\n范围分片算法比较适合 范围 查询的场景，但有可能 存在 单个库上的 热点问题\n分库分表带来的问题 引入分库分表，除了性能的提升，还会带来一定麻烦：\n无法使用 join 操作 事务问题：如果单个事务涉及到不同的 DB，就涉及到 分布式事务 了 ID：分库分表以后，数据存在不同的 DB，那么自增 ID 就无法满足需求了（可能重复），需要引入分布式 ID 聚合查询：对于 order by、group by 变得很复杂 维护成本：分库分表会带来额外的维护成本 无法使用 join 操作其实也不能算缺点\n很多大厂的资深 DBA 都是建议尽量不要使用 join 操作，效率不太理想\n虽然不能用 join，但是可以在业务层做数据的组装，也能达到类似效果\n分库分表后，数据如何迁移 第一种方案就是 停机迁移\n在低峰期，可以挂一个公告，表示需要停机维护，然后做数据迁移即可\n第二种方案是 双写，适用于无法停机，只能在线迁移的情况\n老库的增删改操作，新库也要同步进行（双写），如果新库没有这个数据，需要先插入 双写在新库只会写入新的数据，旧数据需要新库在后台同步 持续上面的操作，直到数据迁移完毕 双写方案比较麻烦，很可能出问题\n无论哪种方案，都要使用到 binlog\n","permalink":"https://blogs.skylee.top/posts/mysql/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E4%B8%8E%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/note/","tags":["MySQL","Cluster","高可用"],"title":"MySQL 主从复制、读写分离、分库分表"},{"categories":["MySQL"],"content":" 什么是 Buffer Pool 如果每次增删查改都要直接操作磁盘，会严重拖慢性能\n因此 InnoDB 引入了 Buffer Pool，用于提高 DB 的读写性能\n有了 Buffer Pool 以后：\n读取数据，可以看看要读取的数据页有没有在 buffer pool，如果在，直接读取就行，不用访问磁盘 写入数据，可以看看要写入的数据页有没有在 buffer pool，如果在，直接写入就行，并标记为「脏页」，不用访问磁盘 Buffer Pool 缓存什么 Buffer Pool 主要缓存以下内容：\n数据页 索引页 undo 页 插入缓存页 自适应哈希索引 锁信息 图片来自小林 coding 如何管理 Buffer Pool 为了管理众多的页面，Buffer Pool 为每一个缓存页都创建了一个 控制块\n控制块记录了一些元数据：\n数据页信息：包括：表空间 id、页码、指向数据页的指针 脏标记：记录了该页是否修改过 \u0026hellip; 分配给 Buffer Pool 的内存空间通常是 连续 的，且控制块在 Buffer Pool 的最前面：\nFree 链表 起初，分配给 Buffer Pool 的内存很多都是没有使用的，也就是说，存在很多 空闲页\n这也是为什么刚开始启动 MySQL 时，虚拟内存占用相对较高，但物理内存占用相对较低的原因\n如果想要缓存数据到 Buffer Pool，肯定要选择一个空闲的缓存页来存放\n我们不可能遍历每一块缓存页来判断是否空闲，效率太低了\nInnoDB 使用 Free 链表来管理空闲页（free page）\nFree 链表记录了空闲页对应控制块的指针，这样，需要分配缓存页时，就可以遍历 Free 链表来获取可以分配的空闲页\nFlush 链表 Buffer Pool 中的「脏页」是需要在「合适的时候」刷到磁盘的\n与空闲页的管理一样，也有一个 Flush 链表来管理等待刷到磁盘的脏页：\nLRU 链表 缓存肯定不能一直存放在内存中：如果有源源不断的数据被缓存，那么 Buffer Pool 的空闲页肯定会被用完\n因此，需要考虑在空闲页用完之前，淘汰一些使用频率较低的缓存数据\n可以想到使用 LRU 算法来实现内存淘汰：\n基本 LRU 算法的思想：\n使用一个链表管理所有正在使用的缓存页 当一个新的页面被缓存，将其插入到链表 头部 若要淘汰缓存页，优先淘汰链表尾部的 预读失效 MySQL 有一种 预读机制：基于 局部性原理，当前访问的页面的接下来几个页面很有可能也会被访问\n于是，在从磁盘读取数据时，除了读取目标页以外，可以 预读 接下来的若干页到 Buffer Pool 中\n这看起来很美好，但是考虑一个场景：预读进来的页一直没有被使用\n这种场景除了 浪费缓存空间 以外，还会 存在热点数据被淘汰 的可能：\n如果使用传统的 LRU 算法，就会把预读页放到 LRU 链表头部，而当 Buffer Pool 空间不够的时候，还需要把末尾的页淘汰掉。\n如果这些预读页如果一直不会被访问到，就会出现一个很奇怪的问题，不会被访问的预读页 却占用了 LRU 链表 前排 的位置，而末尾 淘汰的页，可能是 频繁访问 的页，这样就大大 降低了缓存命中率。\n为了避免这种情况，InnoDB 将 LRU 链表分为两个部分：\nyoung old old 区域占整个 LRU 链表长度的比例可以通过 innodb_old_blocks_pct 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37。\n分区后，目标页还是放在 young 的头部，而 预读页放在 old 头部\n当预读页被 第一次 访问时，就加入到 young 的头部\n在淘汰数据时，先淘汰的肯定是 old 区域的，只有 old 部分都淘汰完了才会淘汰 young 部分的，这样就避免了热点数据被淘汰的风险\n缓存污染 当一个 SQL 语句 扫描了大量数据，在 Buffer Pool 的空间比较有限的情况下，可能会 把热点数据替换掉，大大降低缓存命中率，产生大量磁盘 IO，这就是缓存污染问题\n为什么会这样？\n如果采用上面的 young-old 分区策略，当一个 SQL 语句 扫描了大量数据：\n扫描第一页，预读接下来 2、3、4\u0026hellip; 页 扫描第二页，由于第二页已经在 old 区，提升 到 young 区，预读接下来 5、6、7 \u0026hellip;页 扫描第三页，由于第三页已经在 old 区，提升 到 young 区，预读接下来 8、9、10 \u0026hellip;页 可以发现，在这种情景，预读页也跑到了 young 区，可能导致热点数据被挤到 old 区，甚至淘汰\n而这种全表扫描的 SQL，缓存页往往就访问那么一两次，为了这么一个 SQL 淘汰热点数据太不划算了\n究其原因，还是预读页提升门槛太低，仅仅读一次就可以提升到 young 区\nInnoDB 为了提高预读页进入 young 区的门槛，是这样做的：进入到 young 区域条件增加了一个停留在 old 区域的时间判断\n第一次访问，仍然在 old 区域，不提升 到 young 区 第二次访问，如果与第一次访问的时间间隔 低于某个阈值，也不提升到 young 区 全表扫描时，预读页「前后访问时间间隔」通常很短，使用这种策略，就可以避免「缓存污染」问题\n这个阈值可以通过 innodb_old_blocks_time 来修改，默认为 1000ms\n详细说说「淘汰」这一过程 在淘汰页面时，需要判断这个页是干净页还是脏页：\n如果是 clean page：直接淘汰即可，无需任何其它处理 如果是 dirty page：需要 回写磁盘 再淘汰 如果按照上面的逻辑，在 LRU 淘汰脏页需要涉及磁盘 IO\n为了尽可能少的涉及磁盘 IO，InnoDB 会优先选择淘汰干净页，迫不得已才会淘汰脏页\n最后再来理一下 LRU 的过程：\nFree 链表为空，说明没有空闲页，于是来 LRU 链表 第一次扫描：从尾部扫描 LRU 链表，如果该页面为 clean，淘汰，将其加到 Free 链表 如果第一次扫描后，剩余的空闲页还不够（第一次只扫描 100 个页），执行第二次扫描：深度扫描 LRU 链表，单页刷新（目的是尽可能快的获取足够的空闲页），淘汰，将其加到 Free 链表 可以看出：空白页的数目与脏页数目是影响 LRU 性能的关键因素\n脏页什么时候回写磁盘 从 LRU 淘汰机制可以看出：在空白页紧张的情况下，如果脏页之前没有及时回写，会影响 LRU 的性能\n那么脏页回写磁盘的时机是什么呢？\nredo log 满了：如果 redo log 满了（write pos 追上了 checkpoint），那么 MySQL 会停下来，执行脏页回收 LRU 淘汰过程：如果淘汰掉所有的 clean page 还不够，那么就要触发脏页回写了 MySQL 认为当前空闲：如果 MySQL 认为当前比较空闲，也会将触发脏页回写 MySQL 退出时：正常关闭 MySQL，肯定需要将脏页回写到磁盘，保证持久性（Buffer Pool 是处于用户空间的） 如果偶尔发现一些慢 SQL，可能就是因为脏页回写导致的「抖动」，此时可以调整 Buffer Pool 和 redo log 的大小\n补充：\n淘汰的是脏页，则此时脏页所对应的 redo log 的位置是随机的，当有多个不同的脏页需要刷，则对应的 redo log 可能在不同的位置，这样就需要把 redo log 的多个不同位置刷掉，这样对于 redo log 的处理不是就会很麻烦吗？（合并间隙，移动位置？） 另外，redo log 的优势在于将磁盘随机写转换成了顺序写，如果需要将 redo log 的不同部分刷掉（刷脏页），不是就在 redo log 里随机读写了么？\n其实淘汰的时候，刷脏页过程不用动 redo log 文件的。\n这个有个额外的保证，是 redo log 在「重放」的时候，如果一个数据页已经是刷过的，会识别出来并跳过。\n参考资料：12 | 为什么我的 MySQL 会“抖”一下？ Buffer Pool 的并发性能 问题来了，在实际开发中，肯定有多个线程会并发访问同一个 Buffer Pool，那怎么保证多个线程之间的同步呢？\nInnoDB 的实现很简单：给 Buffer Pool 实例加一个锁，线程要想访问 Buffer Pool，必须持有锁\n可以认为：对单个 Buffer Pool 的访问是 串行 的\n串行的效率不会低吗？\n并不会，Buffer Pool 的操作几乎是纯内存操作，IO 是很快的，这点就像 Redis 一样，核心处理部分采用单线程，效率也很高\n当然，实际开发中，如果内存充足，可以配置 多个 Buffer Pool 实例，提高并发性能\n每个 Buffer Pool 缓存的数据是一样的吗？\n不是，如果缓存一样的数据，虽然提高了并发性能，但是内存利用率不高\n每个 Buffer Pool 缓存的数据是不一致的：\n在缓存一个 page 的时候：使用 hash function 计算该页应该缓存到哪个 Buffer Pool 在读取一个 page 的时候：使用 hash function 计算该页在哪个 Buffer Pool 基于 Buffer Pool 的数据页访问过程 使用 hash function 可以确定一个 page 在哪个 Buffer Pool，但是如何快速确定这个 page 的存储地址呢？\nInnoDB 为每个 Buffer Pool 实例分配了一个 hash table，以 表空间号+页号 为 key，存储地址 为 value\n确定了 Buffer Pool 以后，就可以根据 hash table 获取 key 对应的 控制块的存储地址\n拿到控制块的存储地址，就可以得到这个 page 的地址了\n如果 cache miss，就要从磁盘读取这一页的数据到 Buffer Pool：\n看看有没有足够空白页，如果有，分配即可，如果没有： 基于 LRU 淘汰机制 ，淘汰掉不常使用的页面，腾出空白页 Buffer Pool 预热 关闭 MySQL 服务时，有时需要等待较长时间才能关闭，这是为啥？\n除了上文提到的 MySQL 会在退出时，将 Buffer Pool 的脏页刷盘以外，还有没有其它因素？\n有的\n在刚启动 MySQL 时，Buffer Pool 为空，执行 SQL 时，由于目标页没有在 Buffer Pool，于是需要 从磁盘读取目标页 到 Buffer Pool\nBuffer Pool 的数据从无到业务热点数据的这个过程称为 Buffer Pool 预热\n如果 Buffer Pool 比较大，并且业务热点数据也比较多，那么预热的过程可能很长，这也是 MySQL 刚启动时，性能没那么好的原因\n为了缩短预热时间，MySQL 会在 关闭 Server 时，将 Buffer Pool 的数据保存到磁盘，这样再次启动时，就可以根据这些数据加速预热过程了\n可以通过配置 innodb_buffer_pool_dump_at_shutdown 和 innodb_buffer_pool_load_at_startup 来控制这个行为\n默认情况下，这两个选项都是开启的\n调整 Buffer Pool 大小 如果在运行过程中，想要将 Buffer Pool 的空间调大一点，怎么做？\n比如一开始 Buffer Pool 的空间为 2G，要调整成 4G，是不是需要先分配一块 4G 大小的内存空间，然后将旧数据迁移过去？\nInnoDB 的实现并不是这样的，效率太低了\n为了动态扩展 Buffer Pool 的大小，InnoDB 引入了 chunk 的概念：\n一个 Buffer Pool 中可以有多个 chunk，每个 chunk 有自己的控制块和缓存页\n多个 chunk 共享 Free 链表、Flush 链表、LRU 链表\n如果需要动态扩容，只需要申请新的 chunk 就行了，默认情况下，每个 chunk 的大小为 128M（可以修改 innodb_buffer_pool_chunk_size 来调整）\nInnoDB Buffer Pool 架构图 最后给出一张 InnoDB Buffer Pool 的架构图（如有错误还请指出）：\n","permalink":"https://blogs.skylee.top/posts/mysql/buffer-pool/note/","tags":["MySQL"],"title":"InnoDB Buffer Pool"},{"categories":["MySQL"],"content":" MySQL 有哪些日志？\n错误日志 查询日志 慢查询日志 事务日志 二进制日志 undo log 什么是 undo log？\nundo log 是一种「逻辑日志」，用于记录数据修改前的信息\n例如，当我们执行一次 insert 语句，undo log 中就会出现一条 delete 语句\nundo log 的使用场景？\nundo log 主要用于：\n事务回滚：原子性保障 MVCC 的版本链 redo log 什么是 redo log？\nredo log 是一种物理日志，是事务的 持久性保障\n当 MySQL 或者 OS 宕机而重新启动时，就可以使用 redo log 作数据恢复\n在执行增删改操作时，会先将对应的操作记录到 redo log buffer 中，然后在「合适时机」将 redo log buffer 刷到磁盘中\n刷盘时机 那什么时候会触发 redo log 的刷盘呢？\n事务提交：在事务提交的时候，会刷新 redo log buffer 到磁盘，具体策略通过 innodb_flush_log_at_trx_commit 选项控制 后台线程：InnoDB 会启动一个后台线程，每 1s 将 redo log buffer 的数据刷到磁盘 Log Buffer 容量到达一半 正常关闭 MySQL Server 可以发现，在 事务提交前，redo log 也是 有可能被刷新 到磁盘的\n下面来看看 innodb_flush_log_at_trx_commit 选项：\ninnodb_flush_log_at_trx_commit = 0 当 innodb_flush_log_at_trx_commit = 0 时：\n在事务执行期间，事务线程将 redo log 写到 redo log buffer 后台线程每隔 1s 调用 write，将 redo log buffer 写到 OS 的 page cache，并 立即调用 fsync，保证数据的落地 事务提交后，事务线程不需要执行任何操作 这个选项提供了 最好的性能，因为数据真正刷新到磁盘的操作是由后台线程执行的\n但是，这个选项存在 数据丢失 的风险：如果在后台线程在调用 write 前，MySQL 挂了，那么这 1s 产生的 redo log 就丢失了\ninnodb_flush_log_at_trx_commit = 1 innodb_flush_log_at_trx_commit = 1 时，与之前不同，事务提交时，事务线程 会 主动调用 write，将 redo log buffer 的数据写到 OS 的 page cache，并立即调用 fsync，完成数据的落地，最后再通知客户端事务已经提交\n这个选项提供了 最高的可靠性：只要事务提交，就能保证 redo log 已经刷新到磁盘\n就算 MySQL 挂了，由于事务并没有提交，丢失了 redo log 也没关系\n同时，这个选项的 性能是最差 的：每次提交事务，都涉及磁盘 IO\ninnodb_flush_log_at_trx_commit = 2 innodb_flush_log_at_trx_commit = 2 时，事务提交时，事务线程 只会调用 write，将 redo log buffer 写到 OS 的 page cache，然后通知客户端事务已经提交\n这个选项的可靠性相较于等于 0 时，更高：事务提交，只要 OS 没挂，就能保证 redo log 的落地\n同时性能也不错：只是调用了 write，并没有涉及磁盘 IO，只涉及将用户态的 redo log buffer 写到内核态的 page cache\n总结：\n如果需要绝对的性能，即使丢失 1s 的数据也没关系，设置为 0 如果需要绝对的可靠性，设置为 1 如果对可靠性要求不太高，同时对性能有一定要求，设置为 2 日志文件组 redo log file 实际上并不只有一个文件，而是多个，以日志文件组的形式出现\nredo log file 是循环写入的：多个 redo log file 之间构成一个「循环」，就像一个环形数组：\n数据从 buffer pool 刷到磁盘后，对应的 redo log 也就没有作用了\n日志文件组通过 write pos 和 checkpoint 来标记两个特殊的位置\n图中绿色的部分是还可以写入的部分 图中红色的部分是 buffer pool 中，还没来得及写到磁盘的数据对应的 redo log 写入数据到 redo log 时，write pos 会顺时针移动\n随着数据从 buffer pool 刷到磁盘，checkpoint 也会顺时针移动\n当 write pos 追上 了 checkpoint，说明没有位置写入 redo log 了，此时，MySQL 会 停下来，等待 buffer pool 中的数据写入到磁盘，以此释放 redo log 的空间\n从 MySQL 8.0.30 后，redo log file 的数量被确定为 32 个，可以修改配置文件中的 innodb_redo_log_capacity 来控制 redo log 的总大小\n为什么要使用 redo log 也许你会问：搞这么复杂干嘛，提交事务时，直接将修改后的数据写到磁盘不就能保证持久性吗？还要 redo log 干什么呢？\n理论上的确可以，但是存在性能问题：\n修改的数据可能原本存放在磁盘的不同位置，如果每次修改都要直接写到磁盘（跳过 buffer pool 以及 redo log），会产生大量 随机 IO\n而随机 IO 是很慢的\n因此为了保证写入性能，在修改数据时：\n先将修改的数据写到 buffer pool 再写到 redo log file（防止 MySQL 崩溃，buffer pool 的数据丢失，保证持久性） 那你可能会问：写 redo log 不也是磁盘 IO 吗，为什么性能会更好呢？\n根据 上面的分析 ：写入 redo log file 这个过程是 append（追加） 的，即 顺序 IO，性能更好（这个技术又叫做 WAL：write-ahead logging）\n注意：\n虽然写入 redo log 是顺序 IO，但是 最终的修改操作，还是随机 IO\n因此，使用 redo log 的目的：\n将事务执行期间涉及到的随机 IO 转化为顺序 IO，提高读写性能 防止 MySQL 崩溃后，buffer pool 的数据丢失，保证持久性（重启后可以用 redo log 恢复数据） binlog binlog 是物理日志，记录了执行过的 SQL 语句，用于 数据备份 、主从同步，保证数据一致性\n记录格式 binlog 有三种记录格式：\nStatement Row Mixed Statement 记录的是 原始的 SQL 语句，例如：update users set update_at = now();\n记录原始的 SQL 语句存在一个问题：如果有「动态函数」，如 now()，会 存在数据不一致问题\n为了解决 Statement 数据不一致问题，可以使用 Row 格式，记录的不单单是原始 SQL 语句，还 包括实际值，例如：update users set update_at = '2024-02-24-19-05-12';\n但是采用这种格式，会 占用比较多的空间，在数据恢复和主从同步时，会占用较多 IO 资源\n于是可以使用 Mixed 格式：MySQL 会 自行选择 某一个 SQL 语句该使用 Statement 还是 Row 格式来记录\n然而，现在很多场景下，都将记录格式设置为 Row，而不是 Mixed，因为 binlog 还可以做 数据恢复 使用\n为什么不用 redo log？\nredo log 只适合灾难恢复，循环写入机制决定了文件会被覆盖\n所以使用 Row 格式记录 binlog（接受文件大的缺点以换取恢复能力）\n写入机制 事务需要保证 原子性，无论事务多大，都需要确保一次性写入，因此，Server 会给 每一个事务线程 分配一个 binlog buffer\n如果 binlog buffer 不够，会暂时将 binlog 写到磁盘\n当一个事务提交时，工作线程会调用 write，将 binlog buffer 的内容写到 OS 的 page cache\n那什么时候将 page cache 的内容刷到磁盘呢？\n这个由 sync_binlog 参数控制：\n如果为 0，那么由 OS 来决定什么时候刷到磁盘 如果为 1，那么事务结束后，除了调用 write 外，还要主动调用 fsync 将 page cache 的内容写到磁盘 如果为 N(N \u0026gt; 1)，每次事务结束后，都会调用 write，但是只有累积了 N 个事务，才调用 fsync 默认值为 1，保证事务提交后，binlog 一定持久化到磁盘，如果对数据有一定可靠性需求，并且还要性能，那么可以将 N 的值设置的稍大一些\n与 redo log 的区别 redo log 是在 InnoDB 实现的，而 binlog 是在 Server 层实现的，不论什么引擎都能用 redo log 记录的是 “在某个数据页，做了什么修改”，而 binlog 记录的是 “数据修改的原始逻辑” redo log 有可能在事务提交前就部分持久化到磁盘了，而 binlog 只有在事务提交后，才有可能持久化到磁盘 为什么有了 redo log，还要有 binlog？ 你可能会问：为什么有了 redo log，还要有 binlog？redo log 不是已经可以保证数据的持久化了吗，binlog 是不是有点多余 ？\nredo log 是在 InnoDB 引擎实现的，而早期的 MySQL 采用 MyISAM 作为默认存储引擎，不支持 redo log，也就无法保证 crash safe\n因此 MySQL 在 Server 层实现 binlog，保证在任何存储引擎下的 crash safe\n此外，由于 binlog 是在 Server 层实现的，更有利于用于主从同步\n反过来问：为什么有了 binlog，还要有 redo log？\nbinlog 没有一个类似「锚点」的功能，即 crash 后，无法得知应该从哪里恢复数据，此外 binlog 比较重，恢复起来也比较慢\nbinlog 的这个特点决定了不太适合做 crash recovery\n还有一个历史因素：InnoDB 是作为一个「插件」用于 MySQL 的，redo log 作为 InnoDB 自己的 crash recovery，性能更好\n总结一波：redo log 与 binlog 可以共同协作，专心的做自己的事：\nredo log 负责 crash recovery，数据的持久化保障 binlog 负责数据备份，以及主从同步（集群数据一致性保障） 两阶段提交 前面提到了：“redo log 有可能在事务提交前就部分持久化到磁盘了，而 binlog 只有在事务提交后，才有可能持久化到磁盘”\n那么这个是否会存在问题呢？\n来看一下这个场景：\n图片来自 JavaGuide 在 redo log 持久化之后，binlog 写入磁盘之前，MySQL 挂了，重新启动后，InnoDB 根据 redo log 恢复数据\n注意，这里恢复的数据就是 更新后的数据 了，但是 binlog 并没有写入成功，于是造成了 主从不一致问题\nMySQL 采用 两阶段提交 来避免主从不一致问题\n两阶段提交将 事务提交过程 分为两个阶段：\n准备阶段（prepare） 提交阶段（commit） 使用两阶段提交时，MySQL 内部开启一个 XA 事务：\n在 prepare 阶段，将 XID 写入 redo log，并 将 redo log 状态设置为 prepare，然后调用 write、fsync（类似于 innodb_flush_log_at_trx_commit = 1），将 redo log 持久化到磁盘\n在 commit 阶段，将 XID 写入 binlog，然后调用 write、fsync（类似于 sync_binlog = 1），将 binlog 持久化到磁盘，并 将 redo log 状态设置为 commit\n来看看两阶段提交是如何解决主从不一致问题的：\n还是假设 redo log 持久化到磁盘，在 binlog 持久化到磁盘之前，MySQL 挂掉\n那么重启以后，MySQL 开始做 crash recovery，读到 redo log，检查 redo log 状态：\n处于 prepare 状态：寻找 binlog，看看有没有与其 XID 一致的 binlog 如果有，说明 binlog 成功持久化，提交事务 如果没有，说明 binlog 持久化失败，回滚事务 处于 commit 状态：说明 binlog 成功持久化，提交事务 在这个假设中，是没有的，于是，MySQL 会 回滚事务，这样，主从一致性保证\n总结 undo log\n回滚、MVCC 的基础，事务原子性保障\nredo log\nredo log 以 日志文件组 的形式存储的，是事务持久性保障，用于灾难恢复\n在事务提交阶段，可以通过 innodb_flush_log_at_trx_commit 来控制刷盘策略\nbinlog\n数据备份，主从同步 都离不开 binlog，两阶段提交保证了主从一致性\n","permalink":"https://blogs.skylee.top/posts/mysql/%E6%97%A5%E5%BF%97/note/","tags":["MySQL","日志"],"title":"MySQL 日志"},{"categories":["MySQL"],"content":" 全局锁 全局锁就是 对整个数据库实例加锁，加锁后整个实例就处于 只读状态，后续的 DML 的写语句，DDL 语句，已经更新操作的事务提交语句都将被 阻塞 。\n全局锁的使用场景是在对 DB 做全量备份，对所有的表进行锁定，从而获取一致性视图，保证数据的完整性。\n基本语法：\nflush tables with read lock; -- 加锁，确保当前数据库是要备份的数据库 unlock tables; -- 解锁 一般使用 mysqldump 工具做备份：\nmysqldump -h \u0026lt;主机地址\u0026gt; -u \u0026lt;用户名\u0026gt; -p \u0026lt;待备份数据库名称\u0026gt; \u0026gt; \u0026lt;路径\u0026gt; 加全局锁是一个很 重 的操作：\n主库加全局锁，整个加锁过程都不允许 insert、update、delete，业务基本停摆 从库加全局锁，整个加锁过程都不允许同步主库的 binlog，会造成主从延迟 有没有什么方式避免呢？\nmysqldump 提供了一个 --single-transcation 选项，用于 不加锁 获取数据一致性备份，\n--single-transcation 选项的原理是基于 MVCC 的，在整个备份期间使用同一个 Readview，保证数据一致性\n表级锁 表锁 Table Lock 表锁会锁住一张特定的表，有两种类型：\n表 共享锁 表 独占锁 可以简单将共享锁看作读锁，独占锁看作写锁，二者之间的互斥性与读写锁是一致的\n由于表锁锁定粒度太大，一般不使用\n元数据锁 MDL 元数据锁 MDL 主要用于锁住一张表的元数据，保证读写的正确性\nMDL 由 MySQL 自己控制，在访问一张表的时候会自动锁上\nMDL 也有两种类型：\n当我们对一张表进行 CRUD 时，会加上 MDL 读锁 当我们尝试修改一张表的元数据时（即 alter 操作），会加上 MDL 写锁 二者之间的互斥性与读写锁一致\n注意，在同一个事务内，即使加了 MDL 读锁，也不会影响 alter 操作：\n意向锁 表锁与行锁之间也是满足：读读共享、读写互斥、写写互斥的\n假设表内有一个 X Record Lock，那么就不能加上表共享锁、表独占锁\n因此，在加表锁之前，需要检查表内有没有行锁\n但是表内可能有很多行数据，如果每一行都去判断，会浪费很多时间\n因此出现了意向锁：\n意向共享锁：说明行内存在共享锁 意向独占锁：说明行内存在独占锁 在给某一行加共享锁 之前，先加上意向共享锁；在给某一行加独占锁 之前，先加上意向独占锁；\nAUTO-INC 锁 在插入数据时，可以不指定主键的值，数据库会自动给主键赋值递增的值，这主要是通过 AUTO-INC 锁实现的。\n与其它锁不同，AUTO-INC 锁在数据 插入完毕后就会立即释放，而不是事务结束后\n行级锁 Record Lock 实验：select、update、delete、insert 的行锁、意向锁情况 可以使用：\nselect object_schema, object_name, index_name, lock_type, lock_mode, lock_data from performance_schema.data_locks; 来查看加锁情况\n先来看看 select：\n普通的 select 是快照读，不加锁：\nselect ... lock in share mode 会对当前行加上 行锁（共享锁 S），并在加行锁前，给表加上 意向共享锁（IS）\n加这个锁就好像告诉其它事务：“我要读取这行的数据了，为了保证数据一致性，你们只可以读，不要修改哈”\nS, REC_NOT_GAP 的含义就是 Record Lock\nselect ... for update 会对当前行加上 行锁（独占锁 X），并在加行锁前，给表加上 意向独占锁（IX）\n加这个锁就好像告诉其它事务：“我读取这行的数据是为了后续的更新，也就是要写数据，为了保证数据一致性，你们既不要读，也不要修改”\n再来看看 update：\nupdate 是当前读，会为当前行加上 行锁（独占锁 X），并在加行锁前，给表加上 意向独占锁（IX）\n再来看看 delete：\ndelete 与 update 一样，也是当前读，会为当前行加上 行锁（独占锁 X），并在加行锁前，给表加上 意向独占锁（IX）\n这里就不再演示了\n最后来看看 insert：\ninsert 与 delete、update 一样，也是当前读，会为当前行加上 行锁（独占锁 X），并在加行锁前，给表加上 意向独占锁（IX）\nX 锁呢？\n这里就涉及到之前讲的 AUTO-INC 锁了\nAUTO-INC 锁是特殊的表锁机制，锁 不是 在一个事务 提交后才释放，而是在 执行完插入语句后就会立即释放。\nGap Lock 间隙锁会将一个「间隙」锁住，避免其它事务在这个「间隙」插入数据，产生幻读现象\n间隙锁之间是 可以 共存的，即两个事务可以同时持有包含相同范围的间隙锁，并不存在互斥关系\nGap locks in InnoDB are “purely inhibitive”, which means that their only purpose is to prevent other transactions from Inserting to the gap. Gap locks can co-exist. A gap lock taken by one transaction does not prevent another transaction from taking a gap lock on the same gap. There is no difference between shared and exclusive gap locks. They do not conflict with each other, and they perform the same function.\n间隙锁仅存在于 RR 隔离级别\nNext-Key Lock 间隙锁的范围是「左开右开」区间，而临键锁的范围是「左开右闭」区间\n假设有一个临键锁的范围是 (114, 514]，那么除了不允许其它事务在这个「间隙」插入数据外，还不允许修改 514 这一行的数据\n可以将 Next-Key Lock 看作 Gap Lock + Record Lock\nMySQL 是怎么加锁的？ MySQL 加锁的原则：避免幻读现象的发生，保证一个 SQL 语句在一个事务期间的执行结果不受其它事务干扰\n因此，在分析如何加锁时，我们只需要看看加了锁以后，会不会有幻读现象发生即可\n下面的实验基于 MySQL8.1，RR 隔离级别\ncreate table user ( id int auto_increment primary key, name varchar(10) not null, sex char null, phoneNum varchar(20) not null, email varchar(128) null, age int null, constraint idx_email unique (email), constraint idx_phone unique (phoneNum) ); create index idx_age on user (age); create index idx_name on user (name); 唯一索引等值查询 记录存在 可以看到，只加了 Record Lock\n我们来分析一下：\n根据加锁原则：避免幻读发生\n对于当前这个 SQL 语句，只要不允许其它事务修改（或删除）id = 1 这一行的数据，就完全可以 避免幻读 发生了，因此，只需要加 Record Lock\n再来看看以二级唯一索引作为条件的情况：\n可以看到，除了对二级索引加了 Record Lock 以外，还对主键索引加了 Record Lock\n分析一下：\n行锁是针对 索引 加的锁，这里为了 避免幻读，在对 idx_phone 加了 Record Lock 后，还要对查询到的记录的主键索引项加 Record Lock，避免其它事务通过 where id = 1 修改这一行的数据造成幻读\n记录不存在 分析一下：\n由于 id = 2 这一行并不存在，为了 避免幻读，只能锁住 (1,5) 之间的间隙，避免其它事务插入 id = 2、3、4 的数据行\n因此，这里 InnoDB 加了一个 间隙锁\n再来看看以二级唯一索引作为条件的情况：\n由于 email = '15' 这一行并不存在，为了 避免幻读，只能锁住 ('10','20') 之间的间隙\n为什么这次没有对主键索引加行锁？\n因为只需要对 idx_email 加锁，其它事务就无法插入 email ('10','20') 这个间隙，可以避免幻读\n唯一索引范围查询 查询 id \u0026gt; 25 的数据，为了 避免幻读，需要锁住 (25, 30]，(30, +∞] 的数据，避免其它事务插入或者删除这个区间的数据，造成幻读\n因此，这里 InnoDB 加了两个 临键锁\n如果是 id \u0026gt;= 25，还要额外加一个 记录锁，锁住 id = 25 这一行的数据，避免其它事务修改、删除，造成幻读：\n再来看看以二级唯一索引作为范围条件的情况：\n除了通过三个临键锁锁住 ('50', +∞] 的间隙以外，还有两个对 主键索引 加的 记录锁，避免其它事务 通过 id 修改 id = 25、30 的数据，造成幻读\n非唯一索引等值查询 记录存在 可以发现，加了三个行锁：\n临键锁 (10, 13] 间隙锁 (13, 16) 记录锁 id = 13 为了 避免幻读，需要保证其它事务：\n不能删除 age = 13 的数据：记录锁 保证 不能插入 age = 13 的数据：临键锁 + 间隙锁保证 为什么有了临键锁还要创建间隙锁？\n临键锁事实上无法 完全 保证 age = 13 的数据不被插入，这点可以通过 lock_data 中的 13, 5 来判断：\n如果插入的 age 对应的 id \u0026lt; 5，不允许插入 如果插入的 age 对应的 id \u0026gt; 5，允许插入 因此，为了完全保证 age = 13 的数据不被插入，需要加一个间隙锁 (13, 16)\n2024.3.3 更新\n对主键索引加锁的原因是：SQL 语句没有使用到覆盖索引，也就是说，还 要访问主键索引 来获取 name、sex 等字段的值\n为了防止幻读，就需要对主键索引加锁\n如果修改成 select id, age from user where age = 13 lock in share mode;，就不会对主键索引加锁\n但是，如果修改成 select id, age from user where age = 13 for update;，仍然会 对主键索引加锁\n这是因为 MySQL 会认为要修改这一行的数据，会顺便给主键索引上满足条件的行加上锁\n记录不存在 为了 避免幻读，加上 (10, 13) 的间隙锁，避免其它事务插入 age = 11、12 的数据\n非唯一索引范围查询 加了三个临键锁和两个记录锁，基于上面的分析，这个结果也就不意外了，故不再分析\n行锁升级 行锁是针对 索引 加的锁，如果不通过索引字段作为查询依据，那么 InnoDB 会为每一行加上行锁（Next-Key Lock），就好像整张表都被锁住了一样，可以理解为 行锁升级成表锁\n例如，对于 User 表，只有 id 有主键索引，其它字段均没有索引\n在以 id 为检索条件下：\n只对 id = 1 这一行加了 Record Lock\n在以 sex 为检索条件下：\n对 每一行 都加了 S 型的 Next-Key Lock，可以看成行锁升级成表锁\n这种情况一定要避免，防止 update、insert、delete 语句被阻塞，导致业务停摆\n如何避免？ 首先我们在执行当前读时，加上 where 条件，并且保证走了索引（用 explain 查看）\n如果实在怕忘记，可以将 MySQL 里的 sql_safe_updates 参数设置为 1，开启安全更新模式。\nIf set to 1, MySQL aborts UPDATE or DELETE statements that do not use a key in the WHERE clause or a LIMIT clause. (Specifically, UPDATE statements must have a WHERE clause that uses a key or a LIMIT clause, or both. DELETE statements must have both.) This makes it possible to catch UPDATE or DELETE statements where keys are not used properly and that would probably change or delete a large number of rows. The default value is 0.\n翻译过来：\n对于 update：\n使用 where，并且 where 条件中必须有索引列； 使用 limit； 同时使用 where 和 limit，此时 where 条件中可以没有索引列； 对于 delete：同时使用 where 和 limit，此时 where 条件中可以没有索引列；\n2024.3.3 更新：\ndelete 的时候，尽量加上 limit，不仅可以确保数据的安全（避免删除过多的数据），还可以 减少加锁的范围，提高并发度\nMVCC + Gap Lock 是否完全解决幻读问题？ 前面在「事务篇」提到过：MVCC 解决了快照读的幻读问题\n那么幻读问题是否完全解决了呢？\n很遗憾，并没有\n经过上文的分析，我们发现 Record Lock + Gap Lock + Next-Key Lock 可以 避免绝大多数幻读 现象的发生\n但还有一种幻读，在 RR 隔离级别下，还是无法避免：\n这种幻读的出现场景在于：快照读与当前读 混合使用，看起来二者的查询结果不一致\n那如何避免呢？\n避免混合使用快照读与当前读\n如果能保证一个事务中：\n只使用快照读，那么 MVCC 完全可以解决幻读 只使用当前读，那么 Record Lock + Gap Lock + Next-Key Lock 也完全可以避免幻读 在开启事务后，迅速加锁\n如果确实要混合使用，那么可以在事务开始后，立即加锁：\n避免其它事务插入 id \u0026gt; 30 的数据，造成幻读\n使用串行化隔离级别\n实在不行，就使用串行化隔离级别吧，也可以保证避免幻读\n这是 MySQL 官方的文档，提到了：串行化可以阻止幻读现象的发生，而 RR、RC、RU 都允许幻读现象的发生\n死锁 insert 是怎么加行锁的？ 首先要了解一个概念：MySQL 加锁时，是 先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁\ninsert 是当前读，但是与 update、delete 不同，insert 正常执行时，不会生成锁结构，而是加一个 隐式锁\n什么是隐式锁？\n当事务需要加锁时，如果这个锁 不可能发生冲突，InnoDB 会 跳过加锁环节，这种机制称为隐式锁。隐式锁是 InnoDB 实现的一种延迟加锁机制，其特点是只有在可能发生冲突时才加锁，从而减少了锁的数量，提高了系统整体性能。\n那什么时候，insert 会将隐式锁转化为显式锁？\n如果 insert 的位置 有间隙锁 如果 insert 冲突 来看第一种情况：\n当 insert 的位置 有间隙锁 时，隐式锁转换为显式锁，即 INSERT_INTENTION\nAn Insert intention lock is a type of gap lock set by Insert operations prior to row Insertion. This lock signals the intent to Insert in such a way that multiple transactions Inserting into the same index gap need not wait for each other if they are not Inserting at the same position within the gap. Suppose that there are index records with values of 4 and 7. Separate transactions that attempt to Insert values of 5 and 6, respectively, each lock the gap between 4 and 7 with Insert intention locks prior to obtaining the exclusive lock on the Inserted row, but do not block each other because the rows are nonconflicting.\ninsert 的显式锁是一种特殊的间隙锁，它锁住的是一个 点\ninsert 的显式锁的应用在 并发场景 下，如果有多个并发的 insert 操作，即使它们插入的是同一个「间隙」，只要 插入的点不同，无需互相等待，保证了并发性能\n来看第二种情况：\n如果 主键索引冲突，插入新记录的事务会给已存在的主键值重复的聚簇索引记录添加 S 型记录锁\n如果 二级唯一索引冲突，插入新记录的事务会给已存在的主键值重复的聚簇索引记录添加 S 型 Next-Key 锁\n为什么插入失败要加锁？\n感觉主要原因还是 防止「幻读」\n插入失败，给主键 id 对应行加上 S Record Lock，可以 避免其它事务删除这一行数据 ，从而避免幻读（这里的幻读指：第一次插入失败，第二次却插入成功）\n死锁的产生 来看一个产生死锁的例子：\n有一个 t_student 表：\nCREATE TABLE `t_student` ( `id` int NOT NULL, `no` varchar(255) DEFAULT NULL, `name` varchar(255) DEFAULT NULL, `age` int DEFAULT NULL, `score` int DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 表内数据如下：\nINSERT INTO t_student (id, no, name, age, score) VALUES (15, \u0026#39;S0001\u0026#39;, \u0026#39;Bob\u0026#39;, 25, 24), (18, \u0026#39;S0002\u0026#39;, \u0026#39;Alice\u0026#39;, 24, 77), (20, \u0026#39;S0003\u0026#39;, \u0026#39;Jim\u0026#39;, 24, 5), (30, \u0026#39;S0004\u0026#39;, \u0026#39;Eric\u0026#39;, 23, 91), (37, \u0026#39;S0005\u0026#39;, \u0026#39;Tom\u0026#39;, 22, 22), (49, \u0026#39;S0006\u0026#39;, \u0026#39;Tom\u0026#39;, 25, 83), (50, \u0026#39;S0007\u0026#39;, \u0026#39;Rose\u0026#39;, 23, 89); 按照如下顺序并发执行两个事务：\n图片来自小林 coding time 1 阶段\n可以看到，事务 A 上了一个间隙锁 (20, 30)\ntime 2 阶段\n可以看到，事务 B 也上了一个间隙锁 (20, 30)\n注意：间隙锁之间互相可以共存\ntime 3 阶段\n事务 A 的 insert 操作被阻塞了，因为 insert 的间隙锁与事务 B 的间隙锁冲突\ntime 4 阶段\n出现了死锁：事务 A 在等待事务 B 释放间隙锁，事务 B 也在等待事务 A 释放间隙锁，二者循环等待，产生死锁\n如何避免死锁 那怎么避免死锁呢？\n预防死锁产生\n经过上面的示例，可以总结 MySQL 出现死锁的场景：\n两个事务都加了间隙锁 两个事务分别向对方的间隙插入数据，产生 insert_intention 锁 由于间隙锁与 insert_intention 锁冲突，二者循环等待，产生死锁 因此，可以在编写 SQL 语句的时候想想有没有死锁产生的风险，特别是 加了间隙锁后，尝试 insert 数据，这个操作并发执行有可能产生死锁\n提前释放锁\n当死锁产生时，事务可以释放自己手中的资源，以达到解除死锁\n例如，当检测到死锁产生，可以回滚事务（此时会释放所有持有的锁），然后重新执行事务\nMySQL 有一个选项 innodb_deadlock_detect，开启后就会检测死锁，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行\n例如对于上面的示例，开启这个选项后，执行结果如下：\n这样就避免了死锁，该选项默认开启：\nmysql\u0026gt; show variables like \u0026#39;innodb_deadlock_detect\u0026#39;; +------------------------+-------+ | Variable_name | Value | +------------------------+-------+ | innodb_deadlock_detect | ON | +------------------------+-------+ 1 row in set (0.01 sec) ","permalink":"https://blogs.skylee.top/posts/mysql/%E9%94%81/note/","tags":["MySQL","锁"],"title":"MySQL 有哪些锁？"},{"categories":["MySQL"],"content":" 事务的四大基本特性 事务的四大基本特性指：A（Atomicity）、C（Consistency）、I（Isolation）、D（Durability），即：\nA：原子性，事务要么整体成功，要么整体失败 C：一致性，事务提交后，数据保持一致 I：隔离性，保证事务不受并发因素的影响，独立执行 D：持久性，保证事务提交或回滚后，数据在 DB 的改变是永久的 并发事务问题 并发事务问题指：多个事务并发执行时，可能存在：脏读、不可重复读、幻读现象，导致数据与预期不一致，具体来说：\n脏读：一个事务读取到另外一个事务 尚未提交 的数据。例如：当事务 A 修改了数据，但还没有提交，事务 B 就读取了这个修改的数据。如果事务 A 最后回滚，那么事务 B 就读取到了错误的数据。 不可重复读：在一个事务内多次读取同一数据，但在事务执行期间，其他事务修改了该数据（并且已经提交），导致事务多次读取时得到的结果不一致。 幻读：在一个事务内多次查询时，由于其他事务 插入或删除 了数据，导致事务得到了不同的查询结果。比如在事务内查询某个范围的数据总数，但在事务执行过程中，有其他事务插入了符合条件的数据，导致事务多次查询时结果不一致。 事务隔离级别 为了解决上面的并发事务问题，MySQL 给出了四种解决方案：\n读未提交（Read UnCommitted） 读已提交（Read Committed） 可重复读（Repeatable Read） 串行化（Serializable） 读未提交是最低的隔离级别，允许一个事务读取到另外一个事务 尚未提交 的数据，也就是说会存在：脏读、不可重复读、幻读问题\n读已提交就只允许一个事务读取到另外一个事务 已经提交 的数据，解决了脏读的问题\n可重复读保证了在整个事务执行期间，看到的数据是一致的，其他事务的修改对该事务不可见。可以避免脏读和不可重复读问题， 但仍可能出现幻读问题 。这是 MySQL 默认的事务隔离级别\n事实上，严格意义上来说，可重复读 RR 是不存在幻读问题的，出现幻读的原因是「当前读」破坏了「快照读」的 MVCC，下文有分析 验证 验证脏读现象 我们先将隔离级别设置为 Read Uncommitted，然后再来观察是否出现脏读现象：\n再将隔离级别设置为 Read Committed，然后再来观察是否出现脏读现象：\n验证不可重复读现象 我们先将隔离级别设置为 Read Committed，然后再来观察是否出现不可重复读现象：\n再将隔离级别设置为 Repeatable Read，然后再来观察是否继续出现不可重复读现象：\n验证幻读现象 这一部分可以看 下文：RR 解决了幻读问题吗？ 事务机制如何实现 A、I、D 这两个特性的实现主要基于 redo log 和 undo log\nredo log（持久性保障）\nredo log，即重做日志，记录数据提交时，数据页的物理修改，实现事务的持久性\nredo log 由两部分组成：\nredo log buffer：缓冲区，存储在内存 redo log file：存放重做日志，存储在磁盘 事务提交时，会先将数据页的物理修改写到 buffer pool，然后写到 redo log buffer\n后面再将 redo log buffer 的数据一起刷到磁盘，即 redo log file\nredo log 的作用是：起一个备份作用，如果「刷新 buffer pool 的内容到磁盘」这一操作失败了，还可以读取 redo log 进行 数据恢复\nundo log（原子性、隔离性保障）\nundo log，即回滚日志，用于记录数据修改前的信息，是一种逻辑日志，可以这么认为：当 delete 一条记录时，undo log 会记录一条 insert 记录\nundo log 的作用是：\n提供回滚 MVCC 这里补充一下：\n数据一致性保障是 A + I + D，即原子性、隔离性、持久性三者共同保障的\n快照读 普通的 select 语句就是快照读，读取的是记录对于当前事务的可见版本，有可能是历史数据\n快照读的实现 不需要加锁，是一种非阻塞的行为，并发保证是 MVCC 提供的\n当前读 update、insert、delete 这三个操作就是当前读，读取的是 已提交 的最新数据，这很好理解，在 update、insert、delete 前需要先读取，判断这一行记录是否存在\n此外，select ... for update、select ... in share mode 也是当前读\n当前读的实现 需要加锁，是一种阻塞的行为，并发保证是「锁」提供的\nMVCC MVCC，即多版本并发控制协议，用于 实现事务隔离\nMVCC 维护了一个记录的多个版本，使得不同的事务可以读取到对应版本的记录\n实现原理：MVCC + undo log MVCC 的实现，基于：\nRecord Row：主要是 trx_id、roll_ptr Read View 版本链访问规则 record row：trx_id、roll_ptr\n我们知道，在聚集索引（主键索引）的每个叶子节点中，记录了每一个数据行的真实数据\n数据行包含了以下字段：\n图片来自小林 coding 而用于实现 MVCC 的字段是：\ntrx_id：记录了修改该行记录的事务 id roll_ptr：记录了指向该行记录的历史版本（undo log）的指针 Read View：min_trx_id、m_ids、max_trx_id、creator_trx_id\nRead View（读视图）是 快照读 SQL 执行时，MVCC 提取数据的依据，记录并维护当前 活跃 事务的 trx_id，有四个核心字段：\nmin_trx_id：该 Read View 生成时，活跃的最小事务 id m_ids：该 Read View 生成时，活跃的事务 id 列表 max_trx_id：记录分配给下一个事务的事务 id creator_trx_id：创建该 Read View 的事务 id 版本链访问规则\n版本链访问规则决定了一个事务是否可以读取某一个版本的行记录\n假设一个行记录的事务 ID 为 trx_id 具体规则如下：\ntrx_id \u0026lt; min_trx_id：说明该事务已经提交，可以读取 trx_id == creator_trx_id：说明这个记录就是当前事务产生的，可以读取 trx_id \u0026gt; max_trx_id：说明这个事务是在该 Read View 生成后才开启的，不可用读取 min_trx_id \u0026lt;= trx_id \u0026lt;= max_trx_id：这个就需要判断 trx_id 是否在 m_ids 里面了： 如果 trx_id 在 m_ids 里面，说明该事务还没有提交，不可以读取 否则，说明事务已经提交，可以读取 示例：\nRC\nRC（读已提交）隔离级别下，每次快照读时都会生成一个新的 Read View\n事务 5 第一次 查询 id 为 30 的记录，由于 trx_id = 3，min_trx_id = 3，二者相等，并且 trx_id = 3 在 m_ids 里面，因此，最新的数据不可以读取\n于是顺着 undo log 版本链读取下一个版本的记录，trx_id = 2，小于 min_trx_id ，可以读取，于是读取到（30，3，A30）\n事务 5 第二次 查询 id 为 30 的记录，重新生成一个 read view，此时的 min_trx_id = 4，最新记录的 trx_id = 4，二者相等，并且 trx_id = 4 在 m_ids 里面，因此，最新的数据不可以读取\n于是顺着 undo log 版本链读取下一个版本的记录，trx_id = 3，小于 min_trx_id ，可以读取，于是读取到（30，3，A3）\nRR（可重复读）隔离级别下，会 复用 ==第一次快照读== 时生成的 Read View\n事务 5 第一次 查询 id 为 30 的记录，由于 trx_id = 3，min_trx_id = 3，二者相等，并且 trx_id = 3 在 m_ids 里面，因此，最新的数据不可以读取\n于是顺着 undo log 版本链读取下一个版本的记录，trx_id = 2，小于 min_trx_id ，可以读取，于是读取到 （30，3，A30）\n事务 5 第二次 查询 id 为 30 的记录，复用 read view，此时的 min_trx_id = 3，最新记录的 trx_id = 4，并且 trx_id = 4 在 m_ids 里面，因此，最新的数据不可以读取\n于是顺着 undo log 版本链读取下一个版本的记录，trx_id = 3，min_trx_id = 3，二者相等，并且 trx_id = 3 在 m_ids 里面，因此，这个数据也不可以读取\n于是顺着 undo log 版本链读取下一个版本的记录，trx_id = 2，小于 min_trx_id ，可以读取，于是读取到 （30，3，A30）\n可以发现，事务 5 两次读取到的数据是一样的，也就是实现了可重复读\n事实上，RR 和 RC 唯一的区别就是：\nRR 隔离级别是事务查询过程中都复用第一次查询时生成的 readview， RC 隔离级别是事务查询过程中，每次查询都会新生成 readview。 正是对 ReadView 的使用不同，导致了 RC、RR 的隔离级别不同\n为什么 RC 的性能还比 RR 好 在 MVCC 下，RR 只创建一次 Read View，而 RC 每次读取都要创建 Read View，那为什么 RC 的性能还比 RR 好？\n似乎不是「锁」的问题\n快照读不加锁，而是使用 MVCC 实现并发读\nRR 在整个事务使用 同一个 Read View，这意味着对于同一个 SQL 查询语句，结果始终是一样的\n而一致性的保证是 MVCC 的 版本链\n随着其它新的事务的提交，版本链会 越来越长，因此在 RR 隔离级别下，读取到「合法」的数据的时间就会越来越长（遍历链表，直到 trx_id \u0026lt; 当前 Read View 的 min_trx_id）\n而 RC 每次读取数据都使用新的 Read View，读取的数据都是提交的最新数据，因此，相较于 RR 来说，几乎不用遍历版本链，因此 RC 性能比 RR 高\n创建 Read View 并不是 RC 和 RR 之间性能差异的主要原因\n性能差异的原因在于遍历版本链的长度\nRR 解决了「幻读」问题吗？ 我们先将隔离级别设置为 Repeatable Read，然后再来观察是否出现幻读现象：\n事务 A 查询有没有 id 为 3 的记录，没有，于是插入一条 id 为 3 的记录，但失败了，显示 Duplicate entry '3' for key 'account.PRIMARY，再次查询有没有 id 为 3 的记录，还是没有，这就是所谓「幻读」问题的一种\n看起来似乎 RR 没有解决幻读问题\n但在这种场景下，幻读出现的本质原因还是因为 「快照读」与「当前读」结合使用，破坏了 MVCC\n事实上，RR 是可以保证 快照读 不会出现幻读问题的\n例如：\n只要事务期间没有「当前读」的干扰，RR 完全可以避免幻读问题\n因此可以得出以下结论：\n快照读使用 MVCC 确保幻读现象不会发生，其它事务的增删改，在当前事务是不可见的 当前读本质上读取的是最新的已提交数据，与 MVCC 是冲突的 从严格意义上来说，RR 解决了快照读的幻读问题\n","permalink":"https://blogs.skylee.top/posts/mysql/%E4%BA%8B%E5%8A%A1/note/","tags":["MySQL","事务","MVCC"],"title":"MySQL 事务机制"},{"categories":["MySQL"],"content":" 索引有哪些？ B+ 树索引、Hash 索引、Full-Text 索引、聚集索引、二级索引、主键索引、唯一索引、普通索引、前缀索引、单列索引、联合索引\u0026hellip;\u0026hellip;\n感觉有点懵 😳，下面来分析一波\n按数据结构分类 索引按照数据结构，大致分为三类：\nB+ 树索引 Hash 索引 Full-Text 索引 B+ 树索引结构如下：\n图片来自小林 coding 注意，叶子节点之间构成的双向链表，图中有误\nB+ 树索引结构的 非叶子节点存储索引数据\n如果是主键索引，叶子节点存储行数据 如果是二级索引，叶子节点存储主键 id B+ 树索引结构相比 B 树、AVL 树而言，树的深度更浅（看起来更矮胖），这意味者只需要 更少的磁盘 IO 次数，就可以找到想要的数据\n而 Hash 索引最大的优势在于可以在期望时间复杂度为 O(1) 的情况下，查找数据，但是 无法范围查询\nFull-Text 索引主要用于大文本的索引\n在不指定索引类型的情况下，创建索引时，InnoDB 默认创建的是 B+ 树索引\n按存储数据分类 如果按照存储数据分类，可以分为：\n聚集索引，叶子节点存储行数据 二级索引，叶子节点存储主键 id 如果查询时，走的是二级索引，就有可能涉及到 回表查询：\n图片来自小林 coding 按字段特性分类 如果按照字段特性分类，可以分为：\n主键索引 唯一索引：以 unique 字段建立的索引 普通索引 前缀索引：在索引数据时，仅索引前面的部分数据 唯一索引\nCREATE UNIQUE INDEX index_name ON table_name(index_column_1,index_column_2,...); 前缀索引\n使用前缀索引的目的主要是减少占用空间\nCREATE INDEX index_name ON table_name(column_name(length)); 按字段个数分类 如果按照字段个数分类，可以分为：\n单列索引 联合索引 联合索引指为多个字段建立索引，例如：\nCREATE INDEX index_product_id_name ON product(product_id, name); 图片来自小林 coding 每个非叶子节点，存储的是 product_id、name 两个字段的值，且排序规则为：先按照 product_id 排序，如果 product_id 相同，再按 name 排序\n对于 name 字段来说，是 整体无序，局部有序 的\n因此，联合索引遵循 最左匹配法则\n最左前缀法则：查询从联合索引的 最左列 开始，并且不跳过索引中的列如果跳过某一列，该列后面的索引将失效\n对于 where product_id=1 and name = \u0026lsquo;iphone\u0026rsquo;，索引有效 对于 where name = \u0026lsquo;iphone\u0026rsquo;，索引无效 因此，在建立联合索引时，应该 将使用频率较高的字段放在前面，防止索引失效\n此外，就算遵循最左匹配法制，联合索引也有可能失效\n具体来说，联合索引中，出现范围查询（即 \u0026lt; 和 \u0026gt;），范围查询右侧的列索引失效\n举个例子，假设有联合索引（profession，age，status）\n-- 遵循最左匹配法则：profession，age，status 都存在 explain select * from tb_user where profession = \u0026#39;Engineer\u0026#39; and age \u0026gt; 20 and tb_user.status = 1; 输出：\nkey_len = 85，说明 status 索引没有用于加速\n如果将 \u0026gt; 改成 \u0026gt;=，输出：\nkey_len = 90，说明 status 索引用于加速\n因此，如果业务逻辑允许 \u0026gt;= 和 \u0026lt;=，尽量使用，避免索引失效\n2024.3.31 更新\n虽然使用 \u0026gt;=，key_len = 90，看起来 status 部分被用于加速了\n但是我们要关注 Extra 部分：Using Index Condition，说明使用了索引下推，在判断 status = 1 时，还是回表了的（在存储引擎层做的）\n因此，这个 key_len = 90，更像是一种显示上的假象，status 部分并没有被用于加速\n而 status 部分真正用上，输出应该像这样：\n什么时候需要创建索引？什么时候不需要？ 索引虽然可以带来读取速度的提升，但是也会带来一定的不良影响：\n需要占用物理空间 降低增删改的效率，因为创建索引、维护索引需要时间 因此不是什么时候都能创建索引，使用索引的原则如下：\n可以创建索引\n字段有唯一性限制的 经常作为 where 条件的，如果有多列，可以建立联合索引 经常用于 order by 的字段，避免 file sort 不要创建索引\n读少写多的场景：相比读取性能的提升，写操作性能下降可能更严重，收益不高 字段区分度低：比如性别字段，索引的区分度很小，字段的值分布均匀，无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引。MySQL 有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比（惯用的百分比界线是\u0026quot;30%\u0026quot;）很高的时候，它一般会忽略索引，进行全表扫描。 表中数据少：全表查询也很快，建立索引反而浪费空间 在哪些情况下，索引会失效？ 对索引字段使用最左模糊匹配 例如：where name = '%ack 或者 where name = %ack%\n这两种情况，都会导致索引失效，转而使用全表查询\n因为为 name 建立索引时，是按照 name 的 ascii 码排序的（从前往后，例如 ab \u0026lt; b \u0026lt; cd，最左模糊匹配当然会导致索引失效\n对索引字段使用函数 例如 select id from users where length(name) = 6;\n即使对 name 建立索引，走的仍然是全表查询，因为并没有给 length(name) 建立索引\nMySQL8.0 开始支持了函数索引，可以单独对 length(name) 建立索引：\nalter table user0 add key idx_len_name((length(name))) 对索引字段使用计算表达式 例如 select id from users where age - 1 = 10;\n即使对 age 建立索引，走的仍然是全表查询，因为并没有给 age - 1 建立索引\nMySQL 没有实现「优化」计算表达式，即将上面的 SQL 优化成 select id from users where age = 11;，而是需要我们自己优化\n对索引字段隐式类型转换 例如 select id, phone from users where phone = 114514，其中 phone 的类型为 varchar(20)\n虽然走了索引，但是类型为 index，还是要扫描整个索引树，与全表查询差不多，性能差\n上面的 SQL 语句，实际上被翻译成：\nselect id, phone from users where CAST(phone AS signed int) = 114514 相当于对 phone 字段使用了函数，当然会造成索引失效\n而对于:\nselect id, phone from user0 where id = \u0026#39;1\u0026#39;; 是会走索引的：\n因为翻译过来就是：\nselect id, phone from user0 where id = CAST(\u0026#34;1\u0026#34; AS signed int); 并没有对 id 使用函数\n补充：如何判断 MySQL 类型转换规则\n执行下面的 SQL：\nselect \u0026#39;10\u0026#39; \u0026gt; 9 如果结果为 1，说明默认将字符串转换为数字 如果结果为 0，说明默认将数字转换为字符串 我本地执行的结果为 1，说明默认将字符串转换为数字，这也印证了上面 id，即主键索引没有失效\n联合索引不遵循最左匹配法则 联合索引指为多个字段建立索引，例如：\nCREATE INDEX index_product_id_name ON product(product_id, name); 图片来自小林 coding 每个非叶子节点，存储的是 product_id、name 两个字段的值，且排序规则为：先按照 product_id 排序，如果 product_id 相同，再按 name 排序\n对于 name 字段来说，是 整体无序，局部有序 的\n如果不遵循最左匹配法则，即跳过 product_id，直接使用 name 作为查询条件，由于 name 整体无序，联合索引自然失效\nwhere 子句的 or 例如：select id from users where name = 'Jack' or age \u0026gt; 10，其中 name 建立了二级索引，但 age 没有索引\nname 索引会失效，因为 OR 的含义就是两个只要满足一个即可，因此只有一个条件列是索引列是没有意义的，只要有条件列不是索引列，就会进行全表扫描。\n怎么优化索引？ 前缀索引优化 对于比较长的字段，如果要建立索引，可以建立前缀索引，以减少空间占用\n但前缀索引也有局限性：\norder by 无法使用前缀索引，而是直接 file sort 无法实现覆盖索引（即避免回表查询） 上面两点不难理解，其本质原因还是因为字段的不完整性\n覆盖索引优化 尽量使用覆盖索引，减少回表查询，例如 select id, name from users where name like 'n%';，其中 id 为主键，name 为普通索引\n这句话就使用到了覆盖索引：在查询时，走 idx_name，不需要回表查询\n此外，如果一个 SQL 要查询多个字段，如：select name, phone, sex from users where name like 'n%';，可以建立联合索引 idx_name_phone_sex 来避免回表查询\n主键索引尽量自增 在设计主键时，最好保证主键是递增的\n如果使用非递增主键，那么插入数据时，主键 id 完全是随机的，必须移动业内的其它数据来插入新的数据，还有可能出现一种叫做「页分裂」的现象\n图片来自小林 codig 这种现象不仅影响性能，还会产生内存碎片，空间利用率不高\n索引字段尽量 NOT NULL 索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化\n避免索引失效 设计索引时，要结合业务需求，避免建立的索引失效，不仅不能提高查询效率，还会降低增删改的性能\n如何判断一个查询语句的索引使用情况？ 使用 explain 执行计划可以查看一个 SQL 语句索引的使用情况\n例如：\nexplain select * from User order by id; 输出：\nmysql\u0026gt; explain -\u0026gt; select * from User order by id; +----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------+ | 1 | SIMPLE | User | NULL | index | NULL | PRIMARY | 4 | NULL | 4 | 100.00 | NULL | +----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------+ 1 row in set, 1 warning (0.00 sec) type（重点） 在 MySQL 的 EXPLAIN 查询计划中，type 字段表示查询操作使用的访问类型（Access Type）。type 字段的值描述了 MySQL 优化器选择用于执行查询的访问方法，它是评估查询性能和优化查询的关键指标之一。\n以下是 type 字段可能出现的常见值及其含义：\nALL：全表扫描，表示查询将扫描全表的所有行。这是最慢的访问类型，通常应该尽量避免。\nindex：索引扫描，表示查询将扫描整个索引树，但不需要访问表的数据行。这比全表扫描快，但仍需要较多的 IO 操作。例如上面的示例，基于 id 来排序\nrange：范围扫描，表示查询将在索引树中扫描一个范围的索引值，而不是整个索引。\nref：基于非唯一索引的等值查找，表示查询将使用一个非唯一索引来定位数据行。\neq_ref：基于唯一索引的等值查找，表示查询将使用一个唯一索引来定位数据行。\nconst：常量查找，表示查询使用一个常量条件（例如主键或唯一索引条件），只返回一行结果。\nsystem：表中只有一行（或零行），表示查询将只访问一行数据。\nNULL：在一些特殊情况下，type 字段的值可能为 NULL，表示查询无效或无法优化。\n一般来说，我们希望查询使用 const、eq_ref 或 ref 等基于索引的访问类型，而不是 ALL 或 index 这样的全表扫描或索引扫描类型。\n其它 possible key（重点）：可能用到的索引 Key（重点）：实际使用的索引，如果为 NULL，则没有使用索引。 Key len（重点）：表示索引中使用的字节数，该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下，长度越短越好。 rows：MySQL 认为必须要执行查询的行数，在 innodlb 引擎的表中，是一个估计值，可能并不总是准确的。 filtered：表示返回结果的行数占需读取行数的百分比，filtered 的值越大越好。 还有一个 Extra 字段也需要关注：\nUsing filesort：通常出现在 order by 中，如果 order by 的字段没有建立索引（或者建立的是前缀索引），那么就会出现 filesort，意味着可能在文件内进行排序，效率低，应该避免 Using temporary：使用临时表保存中间结果，MySQL 在对查询结果排序时使用临时表，常见于排序 order by 和分组查询 group by。效率低，要避免 Using Index Condition：虽然用了索引，但是还是要回表查询（索引下推） Using Index：所需数据只需在索引即可全部获得，即覆盖索引，不需要回表 MySQL 为什么默认使用 B+ 树存储索引和数据？ 一步一步分析\nMySQL 除了支持精确查询外，还支持 范围查询，对查询数据 排序\n也就是说，MySQL 的查询需求：\n尽量少的磁盘 IO 支持精确查询，也要支持范围查询，还能高效排序 二叉搜索树 二叉搜索树可以满足第二个需求，在精确查询某个数据，期望时间复杂度是 O(logn)，n 待查询数据的深度\n也可以范围查询：先查询第一个边界值，然后中序遍历直到第二个边界值\n也可以高效排序：二叉搜索树的中序遍历本身就是有序的\n但是，二叉搜索树存在 失衡问题，即树的高度可能失衡，退化成一个链表，这样查询数据的最坏时间复杂度就为 O(n) 了，效率低\nAVL 树 为了避免二叉搜索树的失衡，出现了 AVL 树，即自平衡的二叉搜索树\n虽然解决了失衡问题，但是随着数据的增多，树的深度也更深，这意味着可能需要更多的磁盘 IO 次数才能读取到目标页\nB 树 AVL 树，本质还是一棵二叉树，只有两个子节点\n而 B 树就不一样了，在 AVL 树的基础上，引入了「阶」的概念，即可能有多个子节点，这在一定程度上解决了 AVL 树的深度问题\nB+ 树 单点查询\nB 树的每个节点除了包含索引数据外，还包含了记录（行数据），这意味着：在页的大小相同的情况下，存放的索引数据较少，那么子节点的数目更少，存储相同的数据，树的深度更深\n而 B+ 树只有叶子节点存放记录，非叶子节点只存放索引数据，那么一个非叶子节点就可以有更多的子节点，使得 B+ 树的深度比 B 树更浅，看起来更加「矮胖」\n这样，查询单个数据，平均磁盘 IO 的次数 相较 B 树更少\n范围查询\n如果要范围查询，B 树需要中序遍历很多节点，性能不太理想\n而 B+ 树的叶子节点之间构成「双向链表」，如果要范围查询，只需要定位到第一个叶子节点，然后遍历链表直到不满足范围要求\n数据的插入与删除\nB 树在插入和删除数据时，树形变化可能较大，这会带来一定性能开销\n而 B+ 树由于节点数据时冗余的，插入和删除数据时，树形变化较小\nMySQL 单表行数不要超过 2000w 行？ 页结构 首先来看看 MySQL 的页结构\n图片来自小林 coding 每一页记录的数据，除了包含我们的用户数据，还要包含其它元数据\n索引结构 再来看看主键索引结构：\n每个索引的非叶子节点记录了索引数据，即页号 + 对应页的主键 id 的最小值，在根据主键 id 查询数据时，就可以使用到这些节点加速\n叶子节点记录了主键 id + 行数据\n理论行数上限 假设：\n非叶子节点记录的索引数据有 x 行 树的深度为 d 每个叶子节点能记录的数据有 y 行 那么，一颗深度为 d 的 B+ 树的理论行数为\nx^(d-1)*y\n那么 x、y 的值怎么确定呢？\n下面的计算，页的大小都为 16K\nx 的值\n页结构告诉我们：每页的元数据大概会占用 1K 左右的空间\n那么剩余空间还有 15K\n每一个索引数据由 页号 + min_id 组成\n假设页号占 4 字节，id 为 bigint，占 8 字节，那么一个索引数据就是 12 字节\n因此，x 的值为：(16K - 1K) / (4 + 8) = 1280 行\ny 的值\n与非叶子节点一样，叶子节点用于记录真实行数据的空间也是 15K\n由于每张表，字段不同，数据也不同，每一行的大小也不同，因此，y 的值也会有较大差异\n这里假设每一行的数据为 1K，那么 y 的值为 15\n理论行数上限\n在上面的条件下，假设 B+ 树的高度为 3，那么理论行数上限值为：1280^(3-1)*15，约为 2450w 行，与「建议值 2000w」差不多\nMySQL 单表行数不要超过 2000w 行？ 如果树的高度超过 3，那么每查询一个行数据，就要多一次磁盘 IO，对性能的影响很大，因此，尽量让树的高度 \u0026lt;= 3\n经过上面的分析，我们发现：在 单行数据约为 1K 的情况下，如果要保证树的高度 \u0026lt;= 3，那么最多就只能存放约 2450w 行的数据\n因此这个 2000w 行还是有依据的，但是也要结合实际情况：\n如果每行数据较大，那么在保证树的高度小等于 3 的情况下，单表行数就不能达到 2000w 行 如果业务可以接受较高延迟，那么单表也可以存放超过 2000w 行的数据 因此，2000w 这个值是一个普适性的建议值，能够保证基本查询性能，也符合大部分业务需求\nMySQL 使用 like “%x“，索引一定会失效吗？ 对于这个查询语句：\nselect * from user0 where name like \u0026#39;%ack\u0026#39;; 如果 name 字段建立了索引 idx_name，索引一定会失效吗？\n场景一 这是一个一般场景：\ncreate table user1 ( id int primary key , name varchar(10), age tinyint ); create index idx_name on user1(name); 执行 explain 执行计划：\n+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+ | 1 | SIMPLE | user1 | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+ 1 row in set, 1 warning (0.00 sec) 可以发现，索引失效了，走的全表查询\n场景二 这个场景比较特殊：\ncreate table user0 ( id int primary key , name varchar(10) ); create index idx_name on user0(name); 可以发现：user0 只有两个字段\n执行 explain 执行计划：\nmysql\u0026gt; explain select * from user0 where name like \u0026#39;%ack\u0026#39;; +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+--------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+--------------------------+ | 1 | SIMPLE | user0 | NULL | index | NULL | idx_name | 43 | NULL | 1 | 100.00 | Using where; Using index | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+--------------------------+ 1 row in set, 1 warning (0.00 sec) 可以发现，idx_name 并没有失效，type 为 index，即全扫描 idx_name 索引树\n为什么？\n因为 user0 只有 id、name 两个字段，select * from user0 这个操作 等价于 select id, name from user0，不涉及回表查询\n由于二级索引相较于主键索引，更加轻量（主键索引的叶子节点还要记录 trx_id、roll_pointer），在 不涉及回表 的情况下，查询性能更好\n因此，优化器决定使用 idx_name，而不是全表查询（即使用主键索引）\n但是，即使使用了 idx_name，类型还是为 index，性能也是比较差的，只是比全表好一点点。。。\n此外，对于 user1 表，如果是 select id, name from user1，还是会走 idx_name 的，理由与上面一致\n补充 select * from user0 where name like '%ack' 一定会走索引 idx_name 吗？\n实际上也不一定，前面提到了，idx_name（二级索引）不包含 trx_id、roll_pointer\n如果直接走二级索引，可能出现「脏读」问题\nMySQL 对于二级索引所在的数据页有一个 Page Header 的结构，里面含有一个名为 PAGE_MAX_TRX_ID 的字段，这个字段标明了修改当前索引页面的最大事务 ID，当查询进来的时候会先判断本次 Read View 的 min_trx_id 跟 PAGE_MAX_TRX_ID 的关系，如果 min_trx_id \u0026gt; PAGE_MAX_TRX_ID，说明 PAGE_MAX_TRX_ID 对应的事务已经提交了，就可以走二级索引，否则就要回聚簇索引里面拿更详细的信息做判断。\ncount(*)、count(1) 、count(主键字段) 、count(其它字段) 有什么区别？性能如何？ count(*) count(1) count(主键字段) count(其它字段，not null) count(其它字段，可以为 null) count(其它字段，not null，有索引) count(其它字段，可以为 null，有索引) 首先要弄清楚 count 函数的作用：统计某个字段不为 NULL 的行数\n其查询的本质是 遍历整个索引树 的叶子节点\ncount(*) 实际上与 count(1) 等价（包括性能），不涉及到任何字段，即统计整张表的行数\nInnoDB handles SELECT COUNT(*) and SELECT COUNT(1) operations in the same way. There is no performance difference.\n因此，count(*) 在遍历索引树节点时，每从 InnoDB 读取到一条记录，Server 不会将字段值取出来，而是直接将 count + 1\n而 count(主键字段) 实际上作用与 count(*) 是一样的，都是统计整张表的行数，因为主键不能为 NULL\n因此，count(主键字段) 的性能与 count(*) 基本一致\n而 count(其它字段)，就要看看是否允许该字段为 NULL 了：\n如果不允许为 NULL，那么性能与 count(主键字段) 基本一致 如果允许为 NULL，那么 Server 每从 InnoDB 读取到一条记录，需要将字段值取出来，判断是否为 NULL，如果不为 NULL，将 count + 1，性能较差 理论分析完毕，做个实验：\n-- 创建 test 表 create table test( id int primary key auto_increment, col_not_null varchar(10) not null, col_null varchar(10) ); -- 创建存储过程，用于插入 100w 行数据 DELIMITER $$ CREATE PROCEDURE InsertTestData(i int, j int) BEGIN WHILE i \u0026lt;= j DO INSERT INTO test (col_not_null, col_null) VALUES (CONCAT(\u0026#39;Val\u0026#39;, i), (CONCAT(\u0026#39;Val\u0026#39;, i))); SET i = i + 1; END WHILE; END$$ DELIMITER ; -- 插入 100w 行数据 CALL InsertTestData(1, 1000000); 现在依次执行：\nselect count(*) from test; select count(1) from test; select count(id) from test; select count(col_not_null) from test; select count(col_null) from test; 每条语句执行 5 次，平均查询时间如下：\n类型 平均查询时间 count(*) 98.2ms count(1) 97.2ms count(id) 93.2ms count(col_not_null) 93.2ms count(col_null) 326.8ms 可以发现，count(*)、count(1) 、count(主键字段)、count(其它字段，not null) 四者性能接近，而 count(其它字段，可以为 null) 的性能差\n因此在 只有主键索引 的情况下，可以得出以下结论：\n性能排行 count(*) = count(1) ≈ count(主键字段) ≈ count(其它字段，not null) \u0026gt;\u0026gt; count(其它字段，可以为 null) 此外，网上有人说：对于确实要统计可以为 null 的字段的 count，可以对这个字段建一个二级索引，避免走主键索引遍历（二级索引要轻量一些）\n对 col_null 建一个二级索引后， count(col_null) 5 次平均执行时间为 313ms，看起来似乎快了一些\n但是 业务需求如果就是查询该字段的 count，并且 表的数据量比较少（比如本例的 100w 行数据），建立索引带来的收益还比不上建立索引带来的开销\n可以认为：建立索引基本上不会对 count 的性能有太大的提升\ncount 的性能主要取决于目标字段是否允许为 null\n那如果就是要统计可以为 null 的字段的 count，怎么优化？\n如果业务允许一定的误差，可以使用 explain 执行计划，来 模糊查询\nmysql\u0026gt; explain -\u0026gt; select count(col_null) from test; +----+-------------+-------+------------+-------+---------------+--------------+---------+------+--------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+--------------+---------+------+--------+----------+-------------+ | 1 | SIMPLE | test | NULL | index | NULL | idx_col_null | 43 | NULL | 998602 | 100.00 | Using index | +----+-------------+-------+------------+-------+---------------+--------------+---------+------+--------+----------+-------------+ 1 row in set, 1 warning (0.00 sec) 在 rows 字段，给出了模糊值 998602，整个统计时间为 0.00 sec，非常快\n","permalink":"https://blogs.skylee.top/posts/mysql/%E7%B4%A2%E5%BC%95/note/","tags":["MySQL","索引"],"title":"MySQL 索引"},{"categories":["MySQL"],"content":" 执行一个 SQL 语句，发生了什么？ SQL 语句 在 MySQL 的大致执行流程 先来看看整体流程：\n图片来自小林 coding Client 先与 Server 建立连接，确定 Client 的权限，后续请求的权限都基于此 执行 SQL 语句前，先看看 Cache 有没有这个 SQL 的缓存，如果有直接返回，如果没有： 解析 SQL 语句 执行 SQL 语句： 预处理：看看字段、表等是否存在 确定执行计划：选择合适的索引 执行，向存储引擎发起 API 请求 可以发现，MySQL 的整体架构主要分为两部分：\nServer：负责与 Client 的通信，SQL 语句的分析，执行 SQL 存储引擎：负责数据的落地 建立连接 \u0026mdash; 连接器 在执行 SQL 语句前，Client 需要先与 Server 建立连接\nMySQL 建立的连接是基于 TCP 的\nClient 建立连接时，需要指定用户名和密码，如果验证通过，连接器就会获取该用户拥有的权限，后续的 SQL 调用，都是基于这个时候获取的权限来鉴权的\n查询缓存 连接建立后，如果 Client 想要执行一个 SQL 语句，Server 会先看看这个 SQL 语句的查询结果有没有缓存（缓存是 KV 结构），如果有缓存，就可以直接返回结果给 Client\n为了确保数据的一致性，只要表的数据发生更改，MySQL 就会清空该表对应的缓存\n因此，对于更新频繁的表，查询缓存的命中率是很低的\nMySQL 8.0 版本直接将查询缓存删掉了\nSQL 语句解析 \u0026mdash; 解析器 执行 SQL 语句之前，还要看看有没有语法错误\nMySQL 会使用「解析器」将一个 SQL 语句解析成若干个词\n关键词：如 select、from、where 非关键词：如 user 解析过后，看看有没有语法错误，如果有，直接返回错误给 Client，如果没有，构建语法树，为后续执行 SQL 做准备\n执行 SQL 语句 预处理器 预处理器会基于之前构建的语法树来 判断表、字段在数据库中是否存在\n优化器 在执行之前，还要确定 执行计划\n由于一个 SQL 语句使用到的索引可能有多个，优化器需要选择一个查询性能较高的索引\n例如，假设有一个 user 表：\ncreate table User ( id int auto_increment primary key, name varchar(10) not null, sex char null, phoneNum varchar(20) null ); 并且有两个索引：\n+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | Expression | +-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+ | User | 0 | PRIMARY | 1 | id | A | 0 | NULL | NULL | | BTREE | | | YES | NULL | | User | 1 | idx_name | 1 | name | A | 0 | NULL | NULL | | BTREE | | | YES | NULL | +-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+ 如果执行 select id, name from User where id \u0026gt; 1 and name like 'f%';，会用到哪个索引呢？\n可以使用 explain 来看看：\n+----+-------------+-------+------------+-------+------------------+----------+---------+------+------+----------+--------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+------------------+----------+---------+------+------+----------+--------------------------+ | 1 | SIMPLE | User | NULL | range | PRIMARY,idx_name | idx_name | 46 | NULL | 1 | 75.00 | Using where; Using index | +----+-------------+-------+------------+-------+------------------+----------+---------+------+------+----------+--------------------------+ 可以看到，使用了 idx_name，而不是聚集索引（主键索引）\n因为使用 idx_name，对于这个 SQL 语句来说，不涉及回表查询，相当于是覆盖索引了，相对主键索引，性能更好\n如果是 select * from User where id \u0026gt; 1 and name like 'f%'，那么就要走主键索引了，避免一次回表查询\n执行器 确定了执行计划，Server 层就会与存储引擎层交互了\n以两个例子来说说交互过程：\n全表查询 索引下推 全表查询\n例如：select * from user where sex = 'F';\n执行器第一次查询，让存储引擎读取表中第一条记录 执行器会判断这个记录是否符合条件 如果不符合，跳过 如果符合， 将结果返回给客户端 一直重复上述过程，直到数据全部读取过一遍 可以发现，执行器每得到一个合法数据，就会返回给客户端，之所以客户端显示的是所有记录，是因为客户端会在查询全部执行完毕后，才会将结果返回给用户\n索引下推\n索引下推是 MySQL5.6 版本引入的策略，用于联合索引的优化\n具体来说，索引下推将回表这个操作下放到了存储引擎层，而不是 Server，减少回表次数，提高效率\n例如假设有一个联合索引（age，salary）\n执行 select * from user where age \u0026gt; 10 and salary = 1000\n根据查询条件，联合索引会部分失效：age 字段可以用到索引，但 salary 不能\n如果不启用索引下推：\n执行器调用 API，使存储引擎定位到 age \u0026gt; 10 的第一条记录 存储引擎获取主键值，执行回表查询，将完整的查询结果返回给 Server Server 来判断 salary 是否等于 1000，如果相等，返回给 Client 如果启用索引下推：\n执行器调用 API，使存储引擎定位到 age \u0026gt; 10 的第一条记录 由于建立联合索引，存储引擎可以自己判断 salary 是否等于 1000，如果相等，根据主键 id 执行回表查询，将完整的查询结果返回给 Server Server 判断其它条件是否满足（本例没有其它条件）如果满足，将结果返回给 Client 可以发现，如果不启用索引下推，当满足 age \u0026gt; 10，就要回表查询；而启用索引下推，只有当 salary = 1000 时才会回表查询\n可以使用 explain 执行计划看看有没有使用索引下推：\n如果出现 Using Index Condition，说明使用了索引下推\nMySQL 一行记录是如何存储的？ 下面的示例都是基于 MySQL8.1 版本\nMySQL 的数据存放在哪里？ 可以使用 SHOW VARIABLES LIKE 'datadir'; 来查看 MySQL 数据的存储位置：\nmysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;datadir\u0026#39;; +---------------+-----------------+ | Variable_name | Value | +---------------+-----------------+ | datadir | /var/lib/mysql/ | +---------------+-----------------+ 1 row in set (0.09 sec) 这个文件夹内存放了 MySQL 存储的所有数据，这里面还有若干个子目录，每个子目录代表一个 db，存储了表数据\nbash-4.4# cd bluebell/ bash-4.4# ls -l total 3420 -rw-r----- 1 mysql mysql 114688 Feb 18 20:11 comment_contents.ibd -rw-r----- 1 mysql mysql 114688 Feb 18 20:11 comment_indices.ibd -rw-r----- 1 mysql mysql 114688 Feb 18 20:11 comment_subjects.ibd -rw-r----- 1 mysql mysql 131072 Feb 18 20:11 comment_user_hate_mappings.ibd -rw-r----- 1 mysql mysql 147456 Feb 18 20:11 comment_user_like_mappings.ibd -rw-r----- 1 mysql mysql 147456 Feb 18 20:11 communities.ibd -rw-r----- 1 mysql mysql 114688 Jan 11 16:37 expired_post_scores.ibd -rw-r----- 1 mysql mysql 114688 Feb 15 09:10 fts_000000000000042e_00000000000000ac_index_1.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_00000000000000ac_index_2.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_00000000000000ac_index_3.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_00000000000000ac_index_4.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_00000000000000ac_index_5.ibd -rw-r----- 1 mysql mysql 114688 Feb 15 09:10 fts_000000000000042e_00000000000000ac_index_6.ibd -rw-r----- 1 mysql mysql 163840 Feb 15 09:10 fts_000000000000042e_00000000000000b3_index_1.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_00000000000000b3_index_2.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_00000000000000b3_index_3.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_00000000000000b3_index_4.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_00000000000000b3_index_5.ibd -rw-r----- 1 mysql mysql 294912 Feb 15 09:10 fts_000000000000042e_00000000000000b3_index_6.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_being_deleted.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_being_deleted_cache.ibd -rw-r----- 1 mysql mysql 114688 Feb 15 09:10 fts_000000000000042e_config.ibd -rw-r----- 1 mysql mysql 114688 Feb 18 16:53 fts_000000000000042e_deleted.ibd -rw-r----- 1 mysql mysql 114688 Nov 4 09:08 fts_000000000000042e_deleted_cache.ibd -rw-r----- 1 mysql mysql 262144 Feb 18 20:11 posts.ibd -rw-r----- 1 mysql mysql 163840 Feb 19 09:00 users.ibd MySQL 的表空间结构？ 表空间由：段、区、页、行 这四个基本结构组成\n行（row） 数据库的 record 都是按照 行 来存储的\n页（page） 如果数据读取的基本单位是 行 的话，每次只能读极少数据，会增加很多 IO 次数，效率低\nInnoDB 存储引擎读取数据的基本单位是 页，一页的大小默认是 16K\n数据页内，存储了很多行\n每次读取数据，最少都要读取一页到内存，也就是 16K\n区（extent） B+ 树的每个节点就是一个数据页，其中，每层之间的节点组成一个双向链表\n在查询时，可能涉及到读取多个数据页，如果这些数据页在磁盘不是连续的，那么会带来很多随机 IO，严重降低读写性能\n因此，当数据量较大时，为索引分配空间就不能以页来分配了，而是按照 区 作为基本单位\n一个区的大小默认为 1M，可以存放 64 个页，在这个区内的所有数据页的物理位置都是连续的\n段（segment） 表空间是由若干个 段 组成的，一般分为：\n索引段：存放索引数据 数据段：存放记录的数据 回滚段：用于 MVCC 图片来自小林 coding InnoDB 的行格式有哪些？ InnoDB 提供了 4 种行格式，分别是 Redundant、Compact、Dynamic和 Compressed 行格式\nRedundant 太过古老，为 MySQL5.0 版本之前使用，性能不佳 Compact 是一种紧凑的行格式，可以让一个页存放更多行数据 Dynamic和 Compressed 是基于 Compact 改进的，在 5.7 版本后，默认使用的是 Dynamic 行格式 COMPACT 行格式 图片来自小林 coding 变长字段列表（可选）：逆序 记录了每一变长字段的实际长度 NULL 值列表（可选）：逆序 记录了每列是否为 NULL 记录头信息（必需）：包含信息较多，这里写几个重要的： delete_mask：标记该行数据是否删除（这里可以发现删除数据并不是马上删除） next_record：下一条记录的位置 record_type：记录类型（普通记录、B+ 树非叶子节点记录、最小记录、最大记录） row_id（可选）：如果没有主键，也没有 unique 列，那么 InnoDB 会为该表添加一个隐藏的 row_id 字段 trx_id（必需）：事务 id，记录了该行是哪个事务生成的 roll_pointer（必需）：指向上一个版本的指针 列 1、列 2\u0026hellip;：记录每一列的数据 varchar(n) 的 n 的最大取值为多少？ 首先弄清楚：n 并不是字节数，而是 字符数\nMySQL 规定：除了 TEXT、BLOBs 这种大对象类型之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节。\n例如，假设要创建这样一个表：\nCREATE TABLE test ( `name` VARCHAR(65535) NULL ) ENGINE = InnoDB DEFAULT CHARACTER SET = ascii ROW_FORMAT = COMPACT; 报错：\nThis includes storage overhead，这个 overhead 指的就是前面提到的：变长字段列表和 NULL 值列表\n在这个示例中，NULL 值列表的大小为 1 字节（因为允许 name 为 NULL）\n而变长字段列表的大小为 2 字节\n「变长字段长度列表」所占用的字节数 = 所有「变长字段长度」占用的字节数之和。\n所以，我们要先知道每个变长字段的「变长字段长度」需要用多少字节表示？具体情况分为：\n条件一：如果变长字段允许存储的最大字节数小于等于 255 字节，就会用 1 字节表示「变长字段长度」； 条件二：如果变长字段允许存储的最大字节数大于 255 字节，就会用 2 字节表示「变长字段长度」； 因此，在本例的条件下，name 能存储的的最大值为 65535-1-2=65532 字节\n而我们采用的又是 ascii 编码，每个字符占 1 字节，因此，n 的最大值为 65532\nmysql\u0026gt; CREATE TABLE test1 ( -\u0026gt; `name` VARCHAR(65532) NULL -\u0026gt; ) ENGINE = InnoDB DEFAULT CHARACTER SET = ascii ROW_FORMAT = COMPACT; Query OK, 0 rows affected (0.02 sec) 如果我们采用 utfm8 编码，n 的最大值就不是 65532 了，因为 utf8 编码，每个字符占 3 个字节，那么 n 的最大值应该为 65532/3=21844：\nmysql\u0026gt; CREATE TABLE test1 ( -\u0026gt; `name` VARCHAR(65532) NULL -\u0026gt; ) ENGINE = InnoDB DEFAULT CHARACTER SET = utf8 ROW_FORMAT = COMPACT; ERROR 1074 (42000): Column length too big for column \u0026#39;name\u0026#39; (max = 21845); use BLOB or TEXT instead 此外，一个表通常有多个字段，即使采用 ascii 编码，n 的值也应该 \u0026lt; 65532\n总结：\nn 代表的是字符数，而不是字节数 n 的最大值限制于编码方式、表的字段数等 行溢出后，MySQL 是如何处理的？ 如果表中包含了 TEXT、BLOB 字段，那么一行的数据可能会超过 65535 字节，此时就会发生行溢出\n对于 COMPACT 行格式而言，如果发生行溢出，在记录的真实数据处，只会保存 该列 的 一部分数据，留 20 字节存储溢出页地址，多的数据会存储到溢出页中：\nCompressed 和 Dynamic 这两个行格式和 Compact 非常类似，主要区别就在于对行溢出的处理\n这两个行格式，如果发生行溢出，在记录的真实数据处，不会保存 该列 的数据，而是仅存储溢出页的地址，溢出页来负责存储实际数据\n图片来自小林 coding ","permalink":"https://blogs.skylee.top/posts/mysql/%E5%9F%BA%E7%A1%80/note/","tags":["MySQL"],"title":"MySQL 基础"},{"categories":["OS"],"content":" 为什么要有 DMA？ 传统的方式，要发送一个数据包，实现方式如下：\n用户调用 read，读取待发送的数据 用户调用 write，写入待发送的数据 图片来自小林 coding CPU 需要参与整个数据搬运的过程，这无疑会浪费宝贵的 CPU 性能\n而如果有 DMA 技术，那 CPU 就可以解放出来，让 DMA 参与数据的搬运\n什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务\n传统的网络文件传输性能为什么差？ 传统的文件传输（read/write），整个过程涉及到 两次系统调用，即四次「变态」的过程，四次拷贝（磁盘到内核 buffer、内核 buffer 到用户内存空间、用户内存空间到 socket buffer、socket buffer 到网卡 buffer）\n图片来自小林 coding 如果进程不会修改要发送的数据，那么完全 没有必要将数据拷贝到用户空间\n优化网络传输：零拷贝 零拷贝机制，简单理解，就是 CPU 不参与数据拷贝的过程，并不是真的没有数据的拷贝\n如何实现零拷贝？ mmap + write 为了避免数据拷贝到用户空间，可以使用 mmap 而不是 read 来读取数据\nmmap 系统调用在读取文件后，会创建一个内核空间到用户空间的映射\n当进程在访问这部分的文件时，实际上访问的是内核空间，避免了数据拷贝到用户空间\n图片来自小林 coding 但是这还不是真正的零拷贝，因为还是需要 CPU 参与从 page cache 到 socket buffer 的拷贝过程\n此外，这种方式仍需要两次系统调用\nsendfile 为了避免两次系统调用带来的开销，Linux 提供了一个系统调用 sendfile，可以实现一次系统调用，就将一个文件描述符的指定数据发送到另一个文件描述符\nsendfile 定义如下：\nssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 这种方式，不仅只需要一次系统调用，也避免了数据在内核空间和用户空间之间的相互拷贝\n但是似乎还不是真正的零拷贝？\n如果网卡支持 SG-DMA，那么就可以利用网卡的 DMA 技术，直接从内核的 page cache 读取数据到网卡，避免到 socket buffer 的拷贝\n图片来自小林 coding 大文件传输 大文件就不适合使用 page cache 了\n因为 page cache 就那么大，往往大文件后面部分读进来以后，前面部分的数据就被 LRU 淘汰掉了，下一次读取相同的大文件也享受不到 page cache 带来的加速，反而导致 page cache 频繁淘汰影响性能\n因此，大文件的传输反而适合 直接 IO\nDMA 的方式，将大文件从硬盘 拷贝 到 用户空间，用户空间在 直接 写到网卡发送即可\nReactor 与 Proactor Reactor 与 Proactor 是两种常见的网络 IO 处理模式\n传统的网络 IO 处理模式：\n单进程 多进程 多线程 往往都无法支持较高的流量\n对于单进程，只有一个进程处理用户的并发请求 对于多进程，虽然可以并发处理请求，但是进程之间的切换需要较大开销 对于多线程，相较于多进程而言，虽然调度开销小了，但性能仍然一般 而 Reactor 与 Proactor 模式，可以处理较高的并发量，下面来看看怎么实现\n对于 Reactor 模式，无论采用哪种模式，本质都是基于 IO 多路复用\n而 Proactor 模式是基于 异步 IO 实现的\n单 Reactor 单线程 这种模式只有一个线程来处理客户端的并发请求\n将服务器 socket 添加到 select/epoll 感兴趣事件 Reactor 循环调用 select/epoll_wait，监听 select/epoll_wait 返回，分发 事件： 连接请求事件：交给 Acceptor 建立连接，注册 client_socket 到 select/epoll 其它事件：交给 Handler，使用对应业务逻辑处理即可 图片来自小林 coding 对于 Reactor 模式，有三个基本对象：\nReactor：监听、分发事件 Acceptor：接受连接，注册 client_socket 到 Reactor Handler：处理事件（业务逻辑） 而单 Reactor 单线程，这三个对象的处理，都是在同一个线程进行的\n这种模式能支持的并发量也比较有限，如果某个 Handler 的处理时间过长，会造成后面请求的延迟\n6.0 版本以前的 Redis 采用的就是这个模式，因为 Redis 的业务逻辑是纯内存操作，单线程的 Handler 的处理速度已经很快，使用多线程 Handler 反而会增加额外的调度开销\n在 6.0 版本的 Redis，在 IO 处理部分 引入了多线程\n具体来说：\nReactor 在分发事件时：\n如果该事件是建立连接事件，分发给 Acceptor 否则，将 client_socket 分发给 IO threads IO threads 会 并发 的读取 client 发来的请求，并将请求扔到主 Reactor 的 task queue\n主 Reactor 处理 task queue 的请求时，仍然是 单线程 的，具体来说：\n主 Reactor 拉取 task queue 的每一条请求，处理 处理完毕后，将「响应数据 + client_socket」分发给 IO threads IO threads 会 并发 的将 response_data 写入到对应的 client_socket\n单 Reactor 多线程 事实上，Handler 的处理逻辑 可以是多线程的\nHandlers 通常使用线程池复用线程，避免线程的频繁创建与销毁带来的开销\n单 Reactor 多线程，Reactor 在分发事件时：\n如果该事件是建立连接事件，分发给 Acceptor 否则，处理 IO，并将请求扔到工作队列 工作队列内部有一个线程池，一个 task 队列，采用 生产-消费 模式\n每个 work thread 的工作逻辑如下：\n阻塞等待 task_queue，直到 task_queue 不为空 拉取 task_queue 的任务，执行对应业务逻辑 写响应结果给客户端，并 注册写事件 到主 Reactor 真正的返回响应时机是主 Reactor 处理写事件时\n图片来自小林 coding 多 Reactor 单线程 多 Reactor 单线程相较于单 Reactor 单线程没有任何性能优势，因此不被使用\n多 Reactor 多线程 与单 Reactor 多线程不同，多 Reactor 多线程模式下，有多个 Reactor：\n一个主 Reactor 多个从 Reactor 主 Reactor 负责处理客户端的连接请求，并将 client_socket 均匀地（即负载均衡） 注册到多个从 Reactor 中\n从 Reactor 就专心处理客户端的读写事件，逻辑与单 Reactor多线程基本一致\n多 Reactor 多线程相比较单 Reactor 多线程，主要优势在于：在 accept 新的连接的「同时」，可以处理已建立连接的客户端的请求，提高 CPU 利用率\nProactor Proactor 与 Reactor 最大的不同就是使用 异步 IO\nReactor 采用的 IO 多路复用，可以看作「来了事件，内核通知应用进程，应用进程负责处理」\n而 Proactor 采用的异步 IO，可以看作「来了事件，内核把事件处理好了（即准备好数据），再通知应用进程」\n无论 Reactor 还是 Proactor，都是 事件驱动 IO 模型，只不过 Reactor 是基于「未完成」IO 事件，Proactor 是基于「已完成」IO事件\n图片来自小林 coding 虽然目前 Linux（4.18 以上） 原生支持了异步 IO（io_uring）\n但在这之前，aio 系列函数是 POSIX 在用户层面实现的异步 IO，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 socket 是不支持的\n这也使得目前基于 Linux 的高性能网络程序都是使用 Reactor 方案。\n一致性哈希 传统的负载均衡策略 通常的负载均衡，由一个「负载均衡层」来实现请求均衡的发送到不同的节点\n图片来自小林 coding 负载均衡策略可以是轮询等等\n但是这种方式一般只用于应用服务器\n像 Redis 集群，每个节点存储的数据可能不同，每个 key 的数据存储在哪些节点是确定的，如果采用轮询的策略，不能保证访问的节点一定存储着想要的数据\n使用普通哈希算法实现负载均衡有什么问题？ 对于上面的场景可以想到哈希取模策略：\n在存储数据时，算出一个 key 的哈希值，并对节点数取模，得到待存储节点 在访问数据时，算出该 key 的哈希值，并对节点数取模，得到存储节点 这种方法确实解决了问题，但是并没有实现 均衡，可能出现 倾斜 的情况\n还有一个更严重的问题：不利于节点的扩缩容\n如果要对节点进行扩缩容，需要迁移大量数据（最坏的情况，需要迁移所有数据），成本太高了\n一致性哈希 那么有没有一种方式避免迁移大量数据带来的开销呢？\n一致性哈希算法就是用来解决这个问题的\n一致性哈希除了会对 key 哈希取模以外，首先要先 对节点取模，例如按照节点的 IP:Port 取模\n取模之后，将节点放在「哈希环」的对应位置上：\n对 key 哈希取模以后，将 value 存储在该位置顺时针第一个 Node 上\n如果发生节点的扩缩容，例如删除节点 14，只需要迁移 Node1 ～ Node14 之间的数据到 Node514，大大减小了扩缩容带来的成本\n如果出现映射节点时，出现哈希冲突？\n这个概率很小，因为是对 2^32 - 1 取模，很难出现冲突\n如果出现冲突，可能会向应用进程抛出异常？\n但是，如果节点映射到位置很接近，比如：\n那么对 Node11 的压力就会很大，并且，如果要删除 Node11，那压力全跑到 Node45 上，Node45 扛不住，挂了，压力又转移到 Node114 上，造成 雪崩\n引入虚拟节点 为了尽可能的实现负载均衡，不让大部分数据压在同一节点，可以为一致性哈希引入 虚拟节点 的概念\nNode11-0～Node11-1 映射到 Node11 Node45-0～Node45-3 映射到 Node45 Node114-0～Node114-3 映射到 Node114 这样负载会更加均衡\n此外，删除一个节点，也不会迁移大量数据到同一个节点，会有多个节点共同分担\n","permalink":"https://blogs.skylee.top/posts/os/%E7%BD%91%E7%BB%9C%E7%B3%BB%E7%BB%9F/note/","tags":["OS","Linux","Network","IO"],"title":"Linux Network IO"},{"categories":["OS"],"content":"一个进程最多创建多少个线程？ 由于每个线程都有自己独立的栈空间，因此，创建的线程数量会受到栈空间大小的限制\n我们使用 ulimit -s 查看栈的大小：\n[root@localhost test]# ulimit -s 8192 对于 32 位 OS 来说，用户空间的虚拟内存最大为 3G，那么，理论上一个进程能创建的线程数量为 3G / 8192K = 384 个\n但是，实际数量通常还会更低，一个线程所需的虚拟内存空间应该大于 8M（包括了 TLS，以及内核管理的 TCB）\n做个实验：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; void *task(void *arg) { while (1) { sleep(100); } } int main(void) { int cnt = 0, err = 0; pthread_t tid; while (err == 0) { err = pthread_create(\u0026amp;tid, NULL, task, NULL); ++cnt; } printf(\u0026#34;pid: %d\\ntotal thread number: %d\\nerror: %s\u0026#34;, getpid(), cnt, strerror(errno)); getchar(); } 由于我没有 32 位的环境，从网上扒了一个别人的结果：\n大约为 300 个\n对于 64 位 OS，用户虚拟内存空间通常为 128T，理论上可以创建千万级别的线程\n但是实际并不是如此，线程数还取决于：\n/proc/sys/kernel/threads-max，表示系统支持的最大线程数 /proc/sys/kernel/pid_max，表示系统全局的 PID 号数值的限制，每一个进程或线程都有 ID，ID 的值超过这个数，进程或线程就会创建失败； /proc/sys/vm/max_map_count，表示限制一个进程可以拥有的 VMA(虚拟内存区域)的数量，超过该数量，就无法为该进程分配内存了 在我的环境（64 位，2 Cores，4G RAM），运行结果如下：\n[root@localhost test]# cat /proc/sys/kernel/threads-max 29918 已经达到系统全局支持的最大线程数了\n总结一波：\n进程创建的线程数与 虚拟内存空间大小、栈空间大小 有关 对于 64 位 OS，由于虚拟内存空间很大，基本上不存在虚拟内存不足导致线程无法创建 对于 64 位 OS，限制线程的最大创建数的主要原因取决于：/proc/sys/kernel/threads-max、/proc/sys/kernel/pid_max、/proc/sys/vm/max_map_count 一个线程崩溃了，所属进程也会崩溃吗？ 在 C/C++，默认是这样的\n看一个代码：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;signal.h\u0026gt; void *task(void *arg) { sleep(1); int *p = NULL; *p = 1; // null pointer error printf(\u0026#34;sub thread exit\\n\u0026#34;); return NULL; } int main(void) { pthread_t tid; pthread_create(\u0026amp;tid, NULL, task, NULL); pthread_join(tid, NULL); printf(\u0026#34;main thread exit\\n\u0026#34;); } 运行结果：\n[root@localhost test]# clang test.c -o test -pthread [root@localhost test]# ./test Segmentation fault (core dumped) 可以发现，主线程还没有输出 main thread exit，进程就退出了\n事实上，崩溃的本质是内核向进程发出了 SIGSEGV 信号，而 SIGSEGV 是段错误\n由于线程之间（包括主线程）共享 mm_struct，内核认为如果出现段错误，如果进程不退出，可能会造成更严重的后果\n我们可以注册一个信号处理函数，单独处理 SIGSEGV 信号：\nvoid sigHandler(int sig) { printf(\u0026#34;Catched SIG_%d\\n\u0026#34;, sig); if (sig == SIGSEGV) { // 可以记录一下日志。。。 exit(-1); // 如果是段错误，退出进程 } } int main(void) { signal(SIGSEGV, sigHandler); // ... } 输出：\n[root@localhost test]# ./test Catched SIG_11 ","permalink":"https://blogs.skylee.top/posts/os/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/note/","tags":["OS","Linux","进程管理"],"title":"OS 进程管理"},{"categories":["OS"],"content":" 本文讨论的都是 Ext 系列的文件系统\n基本组成 Linux 的每个文件都有两个基本结构：\n索引节点 inode 目录项 dentry inode 记录了一个文件的元数据，包括：文件类型、权限、所有者和组、大小、时间戳、链接数、数据块指针，可以使用 stat 查看一个文件的元数据：\nroot@SkyLee:~# stat snap/ File: snap/ Size: 4096 Blocks: 8 IO Block: 4096 directory Device: fc03h/64515d Inode: 785828 Links: 3 Access: (0700/drwx------) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2023-09-07 17:19:11.646706615 +0800 Modify: 2023-09-07 17:03:25.256000000 +0800 Change: 2023-09-07 17:03:25.256000000 +0800 Birth: 2023-09-07 17:03:25.256000000 +0800 目录项 dentry，可以简单的看成 \u0026lt;文件名，inode\u0026gt; 这样一个二元组，具体结构如下：\nstruct dentry {//目录项结构 struct inode *d_inode; /*相关的索引节点*/ struct dentry *d_parent; /*父目录的目录项对象*/ struct qstr d_name; /*目录项的名字*/ struct list_head d_subdirs; /*子目录*/ struct dentry_operations *d_op; /*目录项操作表*/ struct super_block *d_sb; /*文件超级块*/ ... }; 一块硬盘格式化后，包含以下内容：\n超级块：用于存储整个文件系统的基本信息 索引节点区：存储 inode 数据块区：存放文件的具体内容 超级块会 在文件系统挂载时读取到内存，而索引节点区会按需读取到内存进行缓存\n虚拟文件系统 图片来自小林 coding 文件系统可以大致分为三类：\n磁盘文件系统 内存文件系统：例如 /proc 目录下的文件，实际上是存储在内存的 网络文件系统 文件的使用 在 Linux 中，每个进程的 task_struct 包括了一个 file_struct\nfile_struct 记录了该进程：\n打开的文件数组 下一个未使用的 fd \u0026hellip;\u0026hellip; 打开的文件数组元素类型为 struct file，这个结构记录了一个文件的：\n文件描述符 下一次读写的位置 文件路径 文件描述符的引用计数 进程每打开一个文件，内核就会追加一个 file 到 file_struct 的文件数组中\n而关闭文件需要判断文件描述符的引用计数是否为 0，如果为 0，可以释放相关资源\n文件系统的基本单位是 块\n当一个进程尝试读取数据时，OS 会从磁盘对应的 块 上读取数据 当一个进程尝试写数据时，OS 先找到磁盘对应的 块，再修改数据 上面的过程没有考虑 page cache\n文件的存储 前面提到了：inode 记录了一个文件的数据块指针\n在 Ext4 文件系统中，inode 包含了 12 个直接指针，1 个间接指针，1 个双间接指针以及 1 个三间接指针。\n存储文件时：\n先寻找一个空闲的 inode 号，分配给即将创建的文件 寻找空闲的磁盘块，存储文件数据 如果 12 个直接指针不足以容纳数据，那么再使用 1 个间接指针，直到存储完毕 这种方式可以灵活的存储大小文件，文件内容的增删查改也比较方便\n对于大文件的访问，多级索引会使访问很慢\nExt4 的 extends 特性，优化了这个点\nextents 是一种 连续 的物理块区间。在 Ext4 中，文件的数据部分由一个或多个 extents 描述，而不再是由单独的数据块描述。\n这样可以减少随机 IO，读写性能较好\n空闲空间管理 一个硬盘要么使用，要么没使用，可以简单的用 0、1 表示\nOS 使用 位图 来记录数据块是否被使用\n如果某一位为 0，表示这一块空闲 如果某一位为 1，表示这一块已使用 空闲空间包括了数据块的空闲空间和索引块的空闲空间\n文件系统的结构 前面提到的位图也是存储在硬盘块中的\n一个硬盘块的大小通常为 4K，能表示 2^12 * 8 = 2^15 位\n每一位对应一个硬盘块，那么一个位图能表示的空间最大为 2^15 * 4K，即 128M\n这对于当今的文件大小来说，肯定不够\n于是 OS 将若干个块组合在一起，成为「块组」\n图片来自小林 coding 超级块：记录了 整个文件系统 的基本信息 块组描述符表：记录了 整个文件系统 块组的基本信息 数据位图：记录 这一个块组 数据块的空闲情况 inode 位图：记录 这一个块组 inode 的空闲情况 inode 列表：包含了 这一个块组 所有的 inode 数据块：用于存储数据 单个块为 4K 的情况下，一个块组的大小为 128M\n为什么每一个块组都要记录 整个文件系统 的基本信息和 整个文件系统 块组的基本信息呢？不会造成空间浪费吗？\n主要有两方面原因：\n可用性 效率 这种 冗余 主要是 保证 文件系统的 高可用\n如果全局只有一个超级块和块组描述符表，如果主机突然宕机，或者超级块/块组描述符表对应的数据块损坏，那么整个文件系统就不可用了\n如果有冗余的副本，那么即使一个出现问题，还可以恢复\n此外，每个块都有文件系统的基本信息，使文件系统的基本信息与该数据块管理的数据较接近，减少随机 IO\n目录的存储 目录也是文件的一种，它的 inode 记录了该目录下所有目录项的存储地址\n图片来自小林 coding 软连接与硬连接 硬连接 之前提到：Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件\n实际上，还可以让多个文件对应同一个 inode：\nSky_Lee@SkyLeeMBP test % ln abc def Sky_Lee@SkyLeeMBP test % ls -1i 8369476 abc 8369476 def 可以看到，abc 与 def 的 inode 值是一样的，它们被称为硬链接，此时，查看 abc 的硬链接数：\nSky_Lee@SkyLeeMBP test % stat -x abc File: \u0026#34;abc\u0026#34; Size: 1 FileType: Regular File Mode: (0644/-rw-r--r--) Uid: ( 501/ Sky_Lee) Gid: ( 20/ staff) Device: 1,9 Inode: 8369476 Links: 2 Access: Tue Jun 13 15:57:21 2023 Modify: Tue Jun 13 15:57:21 2023 Change: Tue Jun 13 16:50:34 2023 Birth: Tue Jun 13 15:56:50 2023 可以看到，Links 的值为 2\n我们对 abc 做出的任何修改都会反应到 def 上，同样的，对 def 做出的任何修改都会反应到 abc 上，这不难理解，因为它们的 inode 值相同，文件的 blocks 肯定也相同，本质上就是同一个文件\n当我们删除一个文件时，实际上是让 Links 的值减 1，如果 Links = 0，那么系统将会回收这个 inode，以及对应的 blocks\n软链接 软链接类似 windows 上的快捷方式\n可以使用 ln -s 来创建软链接：\nSky_Lee@SkyLeeMBP test % ln -s abc softabc Sky_Lee@SkyLeeMBP test % ls -1i | grep abc 8369476 abc 8370725 softabc 可以看到，abc 与 softabc 的 inode 值不同，说明是不同的文件\n文件 IO 缓冲与非缓冲 IO 缓冲 IO 指在用户区实现的缓存，例如 cstdio 中的 printf，fput 等库函数，就是在库内实现了自己的缓冲\n例如，用户调用 fput，并不会立即调用 write，而是写到缓冲区，当缓冲区满/遇到换行符/用户手动刷新（fflush）时，才会调用 write\n优点主要是可以减少系统调用的次数，减少「变态」的开销\n缺点就是有 可能丢数据 ，如果进程挂了，由于部分数据还在用户缓冲区，数据就无法落地到磁盘\n直接与非直接 IO 由于磁盘 IO 很慢，内核通常不会直接对磁盘进行读写操作，而是加一个 buffer\n例如，用户在调用 write 系统调用时，内核不会直接将数据写到磁盘，而是写到 page cache 中，当满足特定条件，或者用户手动刷新（fsync）时，才会真正的写入磁盘\n因此，直接 IO 与非直接 IO 本质区别就是：是否经过了 OS 的 page cache\n直接 IO 读写数据不会经过内核，而是通过 DMA 的方式，直接操作磁盘 非直接 IO 读写数据会经过内核的 page cache，读数据检查 page cache 是否命中，写数据写到 page cache 图片来自小林 coding 非直接 IO 也存在丢失数据的风险：如果在 OS 刷新 buffer 之前，主机宕机 了，那么数据就丢失了\n阻塞与非阻塞 IO 阻塞 IO：当用户发起一个 read 系统调用，线程会阻塞，直到内核将 数据准备好、并且 拷贝到用户空间 后，唤醒用户线程 非阻塞 IO：当用户发起一个 read 系统调用，内核会 立即返回 数据是否准备完毕并且已经拷贝到用户空间 传统的阻塞 IO，效率不高，进程需要一直等待直到数据拷贝到用户空间\n而非阻塞 IO 往往需要多次轮询，感觉也不行\n为了避免非阻塞 IO 的多次轮询带来的性能开销，引入了 IO 多路复用的概念，比如 select、poll、epoll\nIO 多路复用本质是一种 同步 IO，等待事件的过程是同步的，但一个线程可以「同时」处理多个文件描述符上的事件\n无论阻塞 IO，还是非阻塞 IO，抑或是 IO 多路复用，本质都是 同步 IO，都需要等待内核将数据准备好\n真正的异步 IO，「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待\n当调用异步 IO，会立即返回，内核会在数据从内核态拷贝到用户态后，通知进程，在此期间，进程可以去做别的事情\n文件读取的过程 第一步：解析文件路径\n例如：对于可执行文件 bluebell 的路径 /Users/Sky_Lee/Documents/Blue-Bell/bin/bluebell ，OS 会将其解析成一个个的目录：/、/Users、\u0026hellip;\u0026hellip;\n第二步：确定文件的 inode 号\n根目录的 inode 已经确定，根据二级目录名的哈希值，可以在根目录的 inode 中找到二级目录的 inode\n第三步：确定文件的 inode 存储位置\n确定了二级目录的 inode 号，就可以根据 inode 号寻找到 inode 号所在的 块组\n计算出 inode 在块组中的偏移，确定 inode 在块组存储的物理位置\n有了二级目录的 inode 的物理位置，就可以读取二级目录的 inode，进而确定三级目录的 inode 号\n第四步：重复第二、三步，直到找到最终文件存储的物理位置\n递归的进行二、三步，最终可以确定文件存储的物理位置\n第五步：读取文件到内存\n根据是否为直接 IO，确定是否读取到 page cache\n然后再返回数据给用户进程\n","permalink":"https://blogs.skylee.top/posts/os/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/note/","tags":["OS","Linux"],"title":"Linux File System"},{"categories":["OS"],"content":" 虚拟内存 为什么要有虚拟内存？ 如果每个进程都直接操作物理内存，存在安全问题：一个进程可以操作另一个进程的内存空间\n对于单片机而言，采用的就是直接使用物理内存，因此，单片机只能运行一个进程\n为了使进程彼此的内存空间隔离，OS 采用虚拟内存机制，每个进程都有自己独立的虚拟内存，相互不干扰，实现：\n多任务处理 相互隔离，安全性保障 一个进程可以使用比物理内存更大的空间（swap） 内存分段 程序内部可以根据逻辑，分成若干个段：\n代码段 数据段(.data) .bss 栈 堆 分段式管理内存的原理，就是基于程序分段的\n每个进程维护自己的一个 段表，段表记录了每一段对应的物理地址\n进程可以通过计算逻辑地址代表的 段号、段内偏移，再查段表，来得到真实的物理地址\n图片来自小林 coding 内存分段实现了进程彼此的内存隔离，确保了安全性，但是存在以下问题：\n外部碎片 实现 swap 机制，需要大量的磁盘 IO 使用分段管理内存，交换的单位是段，而如果一个段太大，要换到磁盘，必然需要大量 IO\n说说内存分页机制？ 与分段不同，分页机制，OS 会将内存分成若干「页」，一般来说，一页的大小为 4K\n每个进程维护自己的 页表，页表记录了每一页对应的起始物理地址\n进程可以计算一个逻辑地址对应的 页号、页内偏移，以此得到真实的物理地址\n采用内存分页管理，解决了分段管理的两个痛点，分页管理：\n不会出现外部碎片 交换基本单位为「页」，粒度小，每页仅为 4K，成本低 简单内存分页的缺点\n上面的分页方式就是简单内存分页，即只有一级页表\n简单内存分页存在空间浪费的问题，例如，对于 4G 内存的系统来说，假设一页大小为 4K（2^12），那么单个页表就有 2^20 个页表项，所占的空间为 2^20 * 8b = 8M\n对于一个进程而言，8M 看起来并不是很多，但是，一个系统运行的进程会很多，如果有 100 个，就需要 800M 的空间存放页表，太浪费空间了\n如何解决简单内存分页导致的空间浪费问题？ 在单级页表的基础上，再添加一层，形成多级页表\n图片来自小林 coding 此时，完全 映射 4G 内存，所需空间为 8K（一级页表）+ 8M（二级页表）\n那不是比一级页表占的空间更多了吗？\n看起来是这样，但是，一个进程往往不会用到这么多内存，如果一个一级页表项没有用到，自然也不需要分配对应二级页表的空间\n但是每引入一级页表，逻辑地址到物理地址的转换过程就多一步，性能会下降\n如何解决多级页表带来的性能问题？ TLB，也叫快表\nCPU 可以把访问频率较高的页表项缓存到快表中，这样，就 只需要一次访存\n由于空间局部性原理，快表的命中率还是很高的\n分段和分页是绝对的吗？ 不是，还有段页式内存管理\n谈谈 Linux 内存布局？ Linux 主要采取 页式管理 ，但是不可避免的涉及了段机制\n由于历史原因，Intel 的 CPU 一律先进行段式映射，再进行页式映射，因此，Linux 无法避免段机制\n但是，Linux 系统的每一个段都是以 0 为起始地址开始的整个内存空间，这样就相当于 屏蔽 了段的概念\nLinux 的内存空间分布如下：\n你认为分段和分页两者有什么区别？为什么 Linux 采用分页机制，而不是分段？ 分段与分页两者最主要的区别在于：内存基本管理单元\n分段管理，基本单元为 段；而分页管理，基本单位为 页\n分段管理最严重的问题是：swap 不友好\n基本管理单元为「段」，每个段的大小不一，可能较大，可能较小，如果要 swap 一个比较大的段到磁盘，速度慢，给用户体验就是延迟高\n并且，分段管理容易出现外部碎片，这些碎片无法得到利用，造成浪费\n而分页管理就没有这两个问题：\n基本单位为「页」，swap 友好 虽然有内部碎片，但一个「页」很小，浪费比较少 因此，Linux 主要采取 页式管理，而不是分段\n虚拟内存的作用？ 保护进程之间的内存空间不受干扰，独立性保障 使进程可以使用比物理内存更大的空间（swap） malloc malloc 是怎么分配内存的？ malloc 是一个库函数，调用 malloc，有两种方式分配内存：\n内部调用 brk 系统调用，在堆区分配内存 内部调用 mmap 系统调用，在文件映射区分配内存 图片来自小林 coding malloc 分配的是物理内存吗？ 不是\nmalloc 分配的是虚拟内存，调用 malloc，不会真正的分配物理内存\n当进程 第一次 使用 malloc 分配的内存时，会触发缺页中断，因为对应的页表项还没有物理内存块的分配\nmalloc(1) 会分配多少内存？ 这个取决于 malloc 的具体实现，但绝对不是 1 个字节\nmalloc 在分配内存时，会 预分配 部分内存，以减少潜在的系统调用次数\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(void) { void *addr = malloc(1); printf(\u0026#34;pid: %d, addr: %p\\n\u0026#34;, getpid(), addr); getchar(); } 可以看到，由于拟分配内存大小为 1，调用的是 brk（有 heap 标识），并且实际分配的虚拟内存大小为 132K\nfree 释放内存，会立即将内存还给 OS 吗？ 这个要分情况\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(void) { void *addr = malloc(1); printf(\u0026#34;pid: %d, addr: %p\\n\u0026#34;, getpid(), addr); printf(\u0026#34;about to free...\\n\u0026#34;); getchar(); free(addr); printf(\u0026#34;freed.\u0026#34;); getchar(); } 可以看到，即使调用了 free，也没有真正释放\n再来看一个示例：\nint main(void) { void *addr = malloc(1024 * 1024); // ... 省略 } 可以发现，这次没有 [heap] 标识，说明调用的是 mmap，free 以后，内存也立即释放了\n因此，结论如下：\nmalloc 通过 brk() 方式申请的内存，free 释放内存的时候，并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用； malloc 通过 mmap() 方式申请的内存，free 释放内存的时候，会把内存归还给操作系统，内存得到真正的释放。 为什么 brk 释放内存，不会直接归还内存给操作系统，而 mmap 会？\nbrk 申请/释放内存的方式是移动堆顶指针，不支持非连续内存释放，要想归还内存给 OS，不太好操作\n只有在堆的 \u0026ldquo;break\u0026rdquo; 指针下方没有任何分配的内存块时，brk 才能实际减少数据段的大小并将内存还给操作系统。\n而 mmap 是在文件映射区直接映射一块空间，不需要的话，直接解除映射就行\n为什么不全部调用 mmap 申请内存？ 在上面的示例我们知道，调用 mmap，free 会立即释放内存\n如果我们的程序还要再次调用 malloc，就势必 会再一次调用 mmap，多次的系统调用 会影响性能\n为什么不全部调用 brk 申请内存？ 我们知道，brk 存在预分配，即分配的内存比要求的要多一些（冗余），下次再调用 malloc，就直接从预分配的取就行，减少系统调用\n但是，free brk 分配的内存，并不会真正的将内存还给 OS，存在潜在的 内存泄漏，而且这个内存泄漏无法被 valgrind 检测\n因此，malloc 在：\n分配的内存小于 128K，调用 brk 分配的内存大于 128K，调用 mmap free 仅传入一个起始地址，怎么知道该释放多大内存？ 可以发现：malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节\n而这 16 字节就是用于存储这个 memory block 的大小的\n因此，free 仅传入一个起始地址，就可以通过读取起始地址前 16 个字节的数据，获取要释放的 memory block 的大小\n内存满了，会发生什么？ 内存分配的过程 哪些内存可以回收？ 文件页 干净页：没有修改过的 脏页：修改过，与磁盘内容不一致 匿名页：进程的堆栈区域，由于可能频繁使用，不能直接回收，由 swap 机制回收 回收内存带来的性能影响 有两种方式回收内存：\n后台回收：唤醒 kswapd 内核线程，异步回收 直接回收：阻塞当前运行的进程，同步回收 对于文件页：\n干净页的回收，直接释放内存即可，对性能影响不大 脏页的回收，需要将脏页写回磁盘，影响性能 对于匿名页，使用 swap 机制回收，将不常用的页换出到磁盘，也有大量磁盘 IO\n可以发现，回收内存的过程，基本上都要涉及到将内存的数据写到磁盘\n并且，这个过程会带来大量磁盘 IO，造成 OS 的响应时间变长，给人的感觉就是很卡\n如何降低回收内存带来的性能影响？ 调整回收偏好 调整后台回收的时间点 经过上面的分析，可以发现：\n对于文件页的回收，由于干净页的存在，回收影响的性能相对较小 直接回收是同步的，会带来延迟，可以考虑提前进行后台回收 调整回收偏好 在 Linux 中，提供了 /proc/sys/vm/swappiness 选项，用于调整 swap 的偏好，值越小，越消极使用 swap\n我们可以降低这个值，提高文件页的回收优先级：\n# 已经是 0，无需修改 root@SkyLee:~# cat /proc/sys/vm/swappiness 0 尽早触发 kswapd 后台回收，避免直接回收带来的高延迟 在 Linux 中，提供了 /proc/sys/vm/min_free_kbytes 选项，用于调整 最小阈值\n最小阈值与页低阈值、页高阈值的关系：\npages_min = min_free_kbytes pages_low = pages_min*5/4 pages_high = pages_min*3/2 当 pages_min \u0026lt; pages_free \u0026lt; pages_low，即内存压力大时，就会触发 kswapd 后台回收\n因此，可以适当的调高 pages_low（也就是调节 min_free_kbytes），以提前触发后台回收\n但是，调高 pages_min，意味着可能剩余较多内存时，就会触发直接回收，这在一定程度上浪费了内存，可能导致 OOM 的发生\n因此对于内存使用量敏感的进程，可以延后后台回收的时间点，可以使用更多的内存\n在调整 pages_min 时，要关注进程到底是关注延迟，还是内存使用量\nNUMA 架构下的内存回收策略 图片来自小林 coding 在 NUMA 架构下，每若干个 CPU 核心组成一个 node，每个 node 都有自己独立的内存，一个 node 也可以访问另一个 node 的内存，但是延迟肯定更高\n在 NUMA 架构下，如果一个 node 发现内存不够，可以回收自己 node 的内存，也可以去寻找其它 node 有没有可用空间\nLinux 提供了 /proc/sys/vm/zone_reclaim_mode 选项来控制：\n0 （默认值）：在回收本地内存之前，在其他 Node 寻找空闲内存； 1：只回收本地内存； 2：只回收本地内存，在本地回收内存时，可以将文件页中的脏页写回硬盘，以回收内存。 4：只回收本地内存，在本地回收内存时，可以用 swap 方式回收内存。 推荐使用 0，即使延迟高一些，但是比起回收的延迟，根本不算什么\n如果在 Server 发现内存还比较充足，但是却频繁触发直接回收，可能就是 NUMA 架构下的内存回收策略没有设置正确\nroot@SkyLee:~# cat /proc/sys/vm/zone_reclaim_mode 0 如何避免一个进程被 OOM Killer 杀掉？ 发生 OOM 后，OOM Killer 会为每个进程打分，分数最高的，将会被杀掉，直到剩余内存足够分配\n打分机制：\n进程已经使用的物理内存页面数。 每个进程的 OOM 校准值 oom_score_adj。它是可以通过 /proc/[pid]/oom_score_adj 来配置的。我们可以在设置 -1000 到 1000 之间的任意一个数值，调整进程被 OOM Kill 的几率。 points = process_pages + oom_score_adj*totalpages/1000 想要避免被 OOM Killer 杀掉，可以调低 oom_score_adj，如果为 -1000，代表不会被杀掉\n但是，对于我们自己的业务程序，不应该设置为 -1000，如果有内存泄漏，由无法被 OOM Killer 杀掉，OOM Killer 就只能被不断唤醒，并杀掉其它进程\n在 4G 内存大小的 Server 上，申请 8G 内存会怎么样？ OS 虚拟内存的大小 对于 32 位的 OS 来说，虚拟内存最大大小为 3G 对于 64 位的 OS 来说，虚拟内存最大大小为 128T Swap 机制 Swap 机制可以：\n将不常用的页 换出 到磁盘 将磁盘的页 换入 到内存供进程使用 触发条件？ 内存闲置：进程在启动阶段申请的内存，大部分都不会被使用，可以将那些仅用过一次的页面换出到磁盘 内存不足：如果进程使用的内存超过了物理内存的限制，就会 swap 一部分相对不常用的页到磁盘 换入换出的是什么类型的内存？ 换入换出的是 匿名页\n在 4G 内存大小的 Server 上，申请 8G 内存会怎么样？ 32 位的 OS 由于理论只能申请 3G 的内存，因此，申请 8G 会失败\n64 位的 OS 理论可以申请 128T 的内存\n在我的电脑（16G 的物理内存）下，进程申请 128T 内存是可以成功的：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main(void) { const int SIZE = 1024 * 1024 * 1024; const int N = 128 * 1024; char* buff[N]; for (size_t i = 0; i \u0026lt; N; i++) { buff[i] = malloc(SIZE); printf(\u0026#34;申请了 %lu G 内存\\n\u0026#34;, i + 1); } getchar(); } 申请了以后，能使用多少？\n如果没有开启 swap 机制，那使用量会受到物理内存大小的限制\n如果开启了 swap 机制，由于可以将不常用的页换出到磁盘，因此，实际可使用的内存会大于物理内存的大小\n但是，如果使用的内存太多，swap 就会很频繁，带来很大的磁盘 IO\n有了 swap，是不是意味着进程使用的内存没有上限？\n当然不是，在开启了 swap 机制的 64 位 OS 中，一个进程的最大内存使用量通常取决于 RAM 大小，swap 分区大小，以及操作系统的虚拟内存管理策略。\n因此，\u0026ldquo;在 4G 内存大小的 Server 上，申请 8G 内存会怎么样\u0026rdquo; 这个问题就可以解答了：\n对于 32 位的 OS，无法申请，进程内存最大使用量理论上限为 3G 对于 64 位的 OS，可以申请 8G 内存，但是如果要完整使用 8G 内存，需要开启 swap 机制 如何在 Linux 上启用 swap 在 Linux 系统中启用 swap（交换空间），可以分为几个步骤：创建 swap 文件或分区、启用 swap、并设置开机自动挂载。以下是具体的步骤：\n创建 Swap 文件 创建一个 Swap 文件： 使用fallocate或dd命令创建一个用于交换的文件。例如，创建一个 4GB 的 swap 文件：\nsudo fallocate -l 4G /swapfile 如果你的系统中没有fallocate命令，可以使用dd命令来创建：\nsudo dd if=/dev/zero of=/swapfile bs=1024 count=4096k 其中bs是块大小，count是块的数量，两者相乘等于 swap 文件大小。\n设置 Swap 文件权限： 出于安全考虑，Swap 文件的权限应该被设置为仅 root 用户可读写。\nsudo chmod 600 /swapfile 格式化文件为 Swap 格式： 使用mkswap命令将文件设置为 swap 使用：\nsudo mkswap /swapfile 启用 Swap 文件 执行以下命令来启用 swap 文件：\nsudo swapon /swapfile 执行这个命令之后，系统就会开始使用/swapfile文件作为交换空间。\n设置开机自动挂载 Swap 文件 为了在系统启动时自动启用 swap 文件，需要将它添加到/etc/fstab文件中。\n编辑 fstab 文件:\nsudo nano /etc/fstab 使用你喜欢的文本编辑器打开/etc/fstab文件。\n添加 swap 条目:\n在文件的最后添加下面的行：\n/swapfile none swap sw 0 0 保存并关闭文件。\n使配置生效：\n通常来说，再次启动时修改将生效，或者您可以用mount -a命令来使改动立即生效：\nsudo mount -a 完成这些步骤后，swap 文件应该会在每次启动时自动启用。可以通过执行free -m或者swapon -s命令来检查 swap 是否已经被系统正确识别和使用。\n有了 swap 文件，还需要设置 OS 使用 swap 的倾向，前面提到可以修改 /proc/sys/vm/swappiness 临时 修改使用 swap 文件的倾向\nswappiness 的范围为 0 ～ 100，越大，越倾向使用 swap\n为了避免设置重启失效，需要修改 /etc/sysctl.conf，添加：\nvm.swappiness = 30 # 使用 swap 的倾向 再执行：\nsysctl -p 使设置生效\n预读失效与缓存污染 什么是预读机制？ OS 在读取文件到内存时，通常会「预读」一部分到内存中，如果下一次要访问预读部分，就不需要再次读取磁盘，减少磁盘 IO\n预读失效是什么？会导致什么后果？ 如果预读的数据一直不访问，那么预读也就没有作用了\n此外，如果使用传统的 LRU 算法，就会把「预读页」放到 LRU 链表头部，而当内存空间不够的时候，还需要把末尾的页淘汰掉。\n而末尾淘汰的的页可能是热点数据，导致访问热点数据又要重新读磁盘，严重降低性能\n如何避免预读失效带来的影响？ Linux 通过两个 LRU 链表来解决问题：\nactive inactive 一开始预读的数据先放到 inactive，只有访问一次后，才提升到 active\n这样即使预读的页一直不访问，也不会导致热点数据（在 active 中）淘汰\n编号为 20 的页被预读：\n即使 20 页一直没有被读取，也不会影响 active，如果被读取，提升到 active，并且 active 的最后一页降级到 inactive 的头部：\n图片来自小林 coding 什么是缓存污染？ 如果采用 一开始预读的数据先放到 inactive，只有访问一次后，才提升到 active 的机制，会存在缓存污染的问题\n举个例子，在批量读取一批数据（假设为 0 ～ 15）的过程：\n读取 0，预读 1 ～ 15 读取 1，1 提升到 active 读取 2，2 提升到 active \u0026hellip; 读取 15，15 提升到 active 如果之前 active 的 LRU 链表中的热点数据因为这次批量读取而淘汰，并且这一批数据 仅仅用到了一次（事实上，这是很常见的），那么热点数据的淘汰就显得性价比很低了\n这就是缓存污染\n缓存污染会导致什么后果？ 缓存污染会导致热点数据可能被淘汰，导致大量磁盘 IO\n如何避免缓存污染带来的影响？ 问题出在：一开始预读的数据先放到 inactive，只有访问一次后，才提升到 active\n如果读一次，还不足以提升到 active，不就解决了吗？\nLinux 会在内存页被访问第二次的时候，才将页从 inactive list 升级到 active list 里\n这样，在批量读取数据时候，如果这些大量数据只会被访问一次，那么它们就不会进入到活跃 LRU 链表，也就不会导致热点数据的淘汰\n当然，InnoDB 自己也有应对预读失效和缓存污染的策略，可以看看这篇文章：InnoDB Buffer Pool Linux 虚拟内存管理 进程虚拟内存空间管理 OS 对进程虚拟内存的管理，主要是通过 task_struct 中的 mm_struct\n当我们 fork 一个子进程，OS 会 拷贝 父进程的 mm_struct 给子进程，也就是说，父子进程的 mm_struct 是独立的\n这个过程是「写时拷贝」\n当一个进程（父进程）试图复制（fork）一个新的进程（子进程）时，按理说，这两个进程应有各自独立的内存空间，互不影响。然而在实际情况下，操作系统并 不会立即为子进程复制 一份父进程的内存数据，而是采用了一种叫做写时拷贝的策略：\n初始时，子进程会共享父进程内存中所有尚未修改的页面，而不是复制它们。在此期间，这些内存页面 被标记为只读，以防止修改。 当父进程或子进程想对这些共享页面进行写操作时，发生写保护中断，操作系统调用写保护中断处理函数，会先制作一个新的页面副本，新的写操作会定向到这个副本上，而不影响原来的页面。这就是所谓的写时拷贝。 这种策略的优点是如果复制的内容没有被修改，则可以节省大量的内存和 CPU 时间。缺点是如果数据经常被修改，每次修改都要复制，反而会增加开销。因此这种方法适用于读多写少的场合。\n而调用 vfork 或者 clone 出的子进程，OS 会递增父进程 mm_struct 的引用计数，子进程与父进程共享虚拟内存空间\n事实上， pthread_create 内部就是调用的 clone 创建的「线程」\n进程与线程的本质区别？\n是否共享内存空间，是进程与线程的本质区别，Linux 并不区分进程与线程，线程对于内核来说，不过是一个共享特定资源的进程而已\n内核线程的 mm_struct\n内核线程的 task_struct 包含的 mm_struct 为 NULL，因为 内核线程间共享内核虚拟内存空间 ，内核线程之间的调度不涉及内存上下文切换\n内核如何划分用户空间与内核空间？ mm_struct 中有一个 task_size，记录了用户进程的合法虚拟内存地址空间大小\n在 32 位 OS 下，task_size 的大小为 3G\n而 64 位 OS，这个值为 128T（与 PAGE_SIZE 有关）\n进程虚拟空间的布局 mm_struct 的定义如下：\nstruct mm_struct { unsigned long task_size; /* 用户进程的合法虚拟内存地址空间大小，超过这个范围的地址用户进程无法访问 */ unsigned long start_code, end_code; /* 代码段 在虚拟内存中的起始和结束地址 */ unsigned long start_data, end_data; /* 数据段 在虚拟内存中的起始和结束地址 */ unsigned long start_brk, brk; /* 堆 在虚拟内存中的起始地址和当前结束地址（堆可以动态扩展和收缩） */ unsigned long start_stack; /* 栈 在虚拟内存中的起始地址 */ unsigned long arg_start, arg_end; /* 程序启动参数在虚拟内存中的范围 */ unsigned long env_start, env_end;/* 程序环境变量在虚拟内存中的范围 */ unsigned long mmap_base; /* mmap区域的基地址 */ unsigned long total_vm; /* 总映射内存页数 */ unsigned long locked_vm; /* 被锁定不能换出到磁盘的内存页总数 */ unsigned long pinned_vm; /* 既不能换出到磁盘，也不能移动的内存页总数 */ unsigned long data_vm; /* 数据段中映射的内存页数目 */ unsigned long exec_vm; /* 代码段中存放可执行文件的内存页数目 */ unsigned long stack_vm; /* 栈中所映射的内存页数目 */ ...省略... struct vm_area_struct *mmap;\t/* VMA 链表 */ } 图片来自小林 coding 而具体到代码段、数据段、BSS 段\u0026hellip; 的管理，Linux 引入了一个结构体 vm_area_struct，简称 VMA，来表示\nstruct vm_area_struct { struct vm_area_struct *vm_next, *vm_prev; struct rb_node vm_rb; struct list_head anon_vma_chain; struct mm_struct *vm_mm;\t/* The address space we belong to. */ unsigned long vm_start; /* Our start address within vm_mm. */ unsigned long vm_end; /* The first byte after our end address within vm_mm. */ /* * Access permissions of this VMA. */ pgprot_t vm_page_prot; unsigned long vm_flags; struct anon_vma *anon_vma; /* Serialized by page_table_lock */ struct file * vm_file; /* File we map to (can be NULL). */ unsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE units */ void * vm_private_data; /* was vm_pte (shared mem) */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; } 许多 VMA 连在一起，组成一个双向链表，在 mm_struct 中，有一个 mmap 变量，记录了 VMA 链表的起始地址\n每个 VMA 的 vm_mm 变量记录了所属 mm_struct 的地址\n进程访问一个虚拟内存地址，发生了什么？ 虽然 mm_struct 数据结构保存了跟进程地址空间相关的信息，但实际上将虚拟地址转换成物理地址的过程（地址翻译）是由内核配合硬件（具体来说就是内存管理单元，MMU）完成的。\n首先，操作系统内核会检查这个地址是否在进程的地址空间——也就是在 mm_struct 中定义的范围内——如果不在，那么就会引发一个段错误（segmentation fault）。 如果这个地址在有效的地址范围内，那么 MMU 就会接管这个过程。MMU 使用页表（页表的位置和结构由 mm_struct 定义）来将虚拟地址翻译成物理地址。 MMU 首先在 TLB（快表）中查找这个虚拟地址对应的物理地址。如果在 TLB 中找到了对应条目，那么转换就完成了。 如果在 TLB 中找不到对应的转换条目，那么 MMU 就会在实际的页表中查找。如果在页表中找到了，那么物理地址就被确定了，同时，这个信息会被添加到 TLB 中以便下次快速查找。如果在页表中也找不到，那么就会发生缺页错误（page fault），这时操作系统会接管，可能会从磁盘中加载所需要的页面到内存中。 程序编译后的二进制文件如何映射到虚拟内存空间中？ 代码编译过后，会生成一个 ELF 格式的二进制文件\n这个文件内部也分为若干个段\n当我们运行这个程序时，内核实际上做了：\n创建进程，并分配一块虚拟内存空间 解析 ELF 文件 将代码段、数据段\u0026hellip;映射到正确的位置 运行程序 而映射这个过程是通过 load_elf_binary 这个函数实现的\nstatic int load_elf_binary(struct linux_binprm *bprm) { ...... 省略 ........ // 设置虚拟内存空间中的内存映射区域起始地址 mmap_base setup_new_exec(bprm); ...... 省略 ........ // 创建并初始化栈对应的 vm_area_struct 结构。 // 设置 mm-\u0026gt;start_stack 就是栈的起始地址也就是栈底，并将 mm-\u0026gt;arg_start 是指向栈底的。 retval = setup_arg_pages(bprm, randomize_stack_top(STACK_TOP), executable_stack); ...... 省略 ........ // 将二进制文件中的代码部分映射到虚拟内存空间中 error = elf_map(bprm-\u0026gt;file, load_bias + vaddr, elf_ppnt, elf_prot, elf_flags, total_size); ...... 省略 ........ // 创建并初始化堆对应的的 vm_area_struct 结构 // 设置 current-\u0026gt;mm-\u0026gt;start_brk = current-\u0026gt;mm-\u0026gt;brk，设置堆的起始地址 start_brk，结束地址 brk。 起初两者相等表示堆是空的 retval = set_brk(elf_bss, elf_brk, bss_prot); ...... 省略 ........ // 将进程依赖的动态链接库 .so 文件映射到虚拟内存空间中的内存映射区域 elf_entry = load_elf_interp(\u0026amp;loc-\u0026gt;interp_elf_ex, interpreter, \u0026amp;interp_map_addr, load_bias, interp_elf_phdata); ...... 省略 ........ // 初始化内存描述符 mm_struct current-\u0026gt;mm-\u0026gt;end_code = end_code; current-\u0026gt;mm-\u0026gt;start_code = start_code; current-\u0026gt;mm-\u0026gt;start_data = start_data; current-\u0026gt;mm-\u0026gt;end_data = end_data; current-\u0026gt;mm-\u0026gt;start_stack = bprm-\u0026gt;p; ...... 省略 ........ } 内核虚拟内存空间管理 用户空间，每个进程的内存空间是相互隔离的，那么内核空间呢？\n事实上，内核线程之间 共享 内核虚拟内存空间\n32 位 OS 内核虚拟内存空间布局 直接映射区 在内核空间的低地址处，有一块直接映射区，大小为 896M，这一部分，虚拟内存与物理内存是一对一的映射关系\n直接映射区映射的起始地址为物理内存的低地址处（0x00）\n在 X86 体系下，由于 DMA 只能对内存的前 16M 寻址，因此，直接映射区还有一块区域用于 DMA\n虽然直接映射区的虚拟内存与物理内存是一对一的映射，但还是会为这块区域创建页表\nZONE_HIGHMEM 高端内存 图片来自小林 coding 这里讲一下动态映射区\n动态映射区使用 vmalloc 进行内存分配，vmalloc 分配的 虚拟内存是连续的，但 物理内存不是连续的，因此，性能会比直接映射区的性能差一些\n整体布局 64 位 OS 内核虚拟内存空间布局 32 位 OS 的内核空间只有 1G，太小了，所以需要精细的控制\n而 64 位 OS 的内核空间足足有 128T，很大，就不需要那么细粒度的控制\n图片来自小林 coding ","permalink":"https://blogs.skylee.top/posts/os/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/note/","tags":["OS","Linux","内存管理"],"title":"OS 内存管理"},{"categories":["OS"],"content":" 32 位 CPU 最大只能操作 4G 内存吗？ 两个基本概念：\nCPU 位宽 地址总线 位宽 地址总线：\n如果地址总线只有 1 条，只能表示：0、1 两种地址 如果地址总线有 2 条，能表示：00、01、10、11 四种地址 \u0026hellip; CPU 想要操作内存地址，需要 地址总线\n由于 CPU 能操作的地址总线长度 不应该 超出 CPU 的位宽，因此，32 位的 CPU 能操作的地址总线条数最多为 32 条，寻址空间为 2^32，也就是 4G\n如果超过 32 条，32 位 CPU 一次性不能完全计算出内存地址，操作十分繁琐，严重耗费性能！\n因此，32 位 CPU 理论 操作的最大内存为 4G\nPAE 技术 可以使 32 位的 CPU 支持超过 4GB 的物理内存。此技术是通过在地址翻译过程中增加 额外的一级页表 来实现的，这使得 CPU 可以使用多达 36 位的物理地址，支持的物理内存 理论上可以达到最大 64GB。\n64 位相比 32 位 CPU 的优势在哪？64 位 CPU 的计算性能一定比 32 位 CPU 高很多吗？ 问题一：\n单次可以计算的数字最大为 64 位，可以计算更大的数据；而 32 位 CPU 要想计算超过 32 位的数据，就需要多次分步骤计算，性能较差 内存寻址空间更大：64 位 CPU 的理论寻址空间为 2^64，一般的 64 位 CPU 的地址总线为 48 位，可以允许更大的内存 问题二：\n不一定，只有计算大数（超过 32 位），64 位 CPU 的性能优势才能体现出来\n软件的 32 位和 64 位之间的区别？ 软件的 32 位和 64 位之间的区别？ 32 位的操作系统可以运行在 64 位的电脑上吗？ 64 位的操作系统可以运行在 32 位的电脑上吗？如果不行，原因是什么？ 区别在于指令是 32 位还是 64 位\nOS 本质也是一种软件，32 位的 OS 经过一个「兼容层」就可以在 64 位的电脑上运行\n相反，64 位 OS 无法在 32 位的电脑上运行，因为 32 位 CPU 的寄存器无法存储 64 位的指令\nCPU Cache 结构：\n图片来自小林 coding 首先要讲一下 Cache line 的概念：\nCPU Cache 是由很多个 Cache Line 组成的，Cache Line 是 CPU 从内存读取数据的 基本单位\nCPU 从内存读取数据，并缓存到 L1、L2 缓存的过程，一次是读取「一批」数据，这一批数据会被存放到 Cache line 中\n图片来自小林 coding 在 Linux 上，可以使用以下指令查看每一级缓存的 cache line 的大小：\nroot@SkyLee:~# cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size 64 上面的结果表示 L1 缓存的 cache line 的大小为 64 字节\n如何写出让 CPU 跑得更快的代码？ 要想让 CPU 跑得更「快」，就得提高 CPU Cache 的命中率，减少内存的访问次数\n从 L1 缓存的结构可以看出，有两个优化点：\n数据缓存 指令缓存 提高数据缓存命中率 分析下面的代码，哪个更快？\n// code 1 const int N = 10000; int main(void) { int arr[N][N]; for (int i = 0; i \u0026lt; N; ++i) for (int j = 0; j \u0026lt; N; ++j) arr[i][j] = 0; } // code 2 const int N = 10000; int main(void) { int arr[N][N]; for (int i = 0; i \u0026lt; N; ++i) for (int j = 0; j \u0026lt; N; ++j) arr[j][i] = 0; } 对于 c 来说，答案是第一种方式更快\n因为 c 的二维数组的存储方式是 按行存储，例如 arr[2][2] 的存储顺序为: arr[0][0]、arr[0][1]、arr[1][0]、arr[1][1]\n前面提到，CPU 从内存读取数据的基本单位是 Cache Line\n那么，如果 Cache Line 为 64 字节，第一次就可以读取 arr[0][0] ~ arr[0][15] 共 64 字节的数据到 L1 Cache 的数据缓存部分，后续访问 arr[0][1] ~ arr[0][15] 时，就不用读取内存，直接读 L1 Cache 即可\n如果采取按行遍历，就可以大大提高数据缓存的命中率，进而提升性能\n提高指令缓存命中率 观察下面代码：\nconst int N = 100; std::vector\u0026lt;int\u0026gt; arr(N); for (int i = 0; i \u0026lt; N; ++i) arr[i] = rand() % 100; std::sort(arr.begin(), arr.end()); // 1 for (int i = 0; i \u0026lt; N; ++i) // 2 { if (arr[i] \u0026lt; 50) arr[i] = 0; } 先执行 1 后执行 2 先执行 2 后执行 1 哪个更快？\n答案是：先执行 1 后执行 2\nCPU 有一个功能叫做 分支预测\n如果分支预测可以预测到接下来要执行 if 里的指令，还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中 ，这样 CPU 可以直接从 Cache 读取到指令，于是执行速度就会很快。\n因此，先升序排序，那么小于 50 的元素就会被 集中 放在前面，大概率 会命中指令缓存，性能更高\n提高多核 CPU 的缓存命中率 现在的 CPU 基本上都是多核的，每个核有独立的 L1、L2 Cache\n而线程可以在不同核上交替执行，如果一个线程在多个核之间来回切换，缓存命中率就会下降\n如果想避免这种情况，可以将某个线程「绑定」在一个核心上\nLinux 提供了 sched_setaffinity 方法做到这一点\n缓存一致性问题 数据写入内存的步骤？ 首先说说数据读取的步骤：\n检查 L1 Cache 有没有该数据，如果没有： 检查 L2 Cache 有没有该数据，如果没有： 检查 L3 Cache 有没有该数据，如果没有： 从内存读取数据 再来讨论数据写入的步骤\n写直达\n写直达会将数据 同时 写到 CPU Cache（如果数据存在）和 Memory\n优点就是 确保了数据的一致性\n但这种方式，会 浪费很多时间、性能（内存写入速率远小于 CPU Cache）\n写回\n在写入前，先检查 CPU Cache 中有没有该数据：\n如果没有，那直接写到内存 如果有，写到 CPU Cache，并打上「脏」标记，代表该数据被修改过 「写回」方式不会将新的数据直接写到内存，真正的写入内存时机，是在 该 Cache Block 被替换时\n图片来自小林 coding 这就是「懒更新」的思想，线段树也用到了该思想\n优点就是性能高，不用每次都写内存\n缺点就是存在数据不一致问题\n假设有一个变量 i（初始值为 0），且核心 A、B 的 L1 Cache 都有变量 i，初始，线程 T 在核心 A 上运行：\nT 修改 i 的值为 1 发生调度，T 下处理机 发生调度，T 上处理机，在核心 B 运行 T 想要读取 i，但是，B 的 L1 Cache 并没有更新，此时，T 就读到了脏数据！ 那么，如何保证 CPU Cache 之间的一致性呢？有两种方式：\n总线嗅探 MESI 协议 总线嗅探 总线嗅探的基本原理是：\n每个 CPU 核心监听总线上的事件 当 CPU 更新修改变量 i 的值时，将这个事件广播到总线 其它 CPU 监听到该事件，检查自己的缓存有没有该数据，如果有，那么更新 由于 CPU 需要每时每刻监听总线上的一切活动，但是不管别的核心的 Cache 是否缓存相同的数据，都需要发出一个广播事件，这无疑会 加重总线的负载。\n此外，总线嗅探看似解决了上面的数据不一致，但是，如果核心 B 的 L1 Cache 一开始没有变量 i：\nT 修改 i 的值为 1，广播该事件到总线 核心 B 监听到该事件，但自己的缓存没有 i，忽略 发生调度，T 下处理机 发生调度，T 上处理机，在核心 B 运行 T 想要读取 i，由于核心 B 的缓存没有变量 i，于是访问内存 但是，修改后的 i，还没有写入内存，此时，T 就读到了脏数据！ 因此，总线嗅探 不能完全解决 CPU Cache 的一致性问题\n事实上，要解决 CPU Cache 的一致性问题，需要满足两个条件：\n写传播：修改 L1 Cache 的消息，必须传播到其它核心 事务串行化：某个变量在一个核心的操作顺序，在其它核心看起来必须是一致的 图片来自小林 coding 总线嗅探仅满足了第一个条件\nMESI 协议 MESI 是：Modified, Exclusive, Shared, and Invalid 的缩写，具体含义如下\nM：已修改 E：独占 S：共享 I：无效 这四个状态是 Cache Line 的基本状态，在运行时，四个状态之间可以相互转换（状态机）\n「独占」的含义是：该数据只有当前的 CPU Cache 有，其它没有 「共享」的含义是：该数据除了当前的 CPU 有以外，其它某些 CPU 也有 「已修改」的含义是：该数据在当前的 CPU 有过修改（与内存不一致了） 「无效」的含义是：该 Cache Block 无效 处于「独占」和「已修改」状态的 Cache Line，可以直接修改数据而不通知其它核心，解决了 总线嗅探 的性能浪费问题\n处于「共享」状态的 Cache Line，若要修改数据，需要 通知 其它共享该 Cache Line 的核心，将「共享」状态修改为「无效」状态，并且，本核心的状态修改为「已修改」\n处于「无效」的 Cache Line，核心 不可以读取\n对于上面的示例，如果使用 MESI 协议：\n如果 Core1 想读取 i 的值：\n补充:\n状态转换规则：\n图片来自小林 coding 此外，还有一个 网站 帮忙理解 MESI\nCPU 是如何执行任务的？ 读写数据时的问题 假设现在有一个双核 CPU\n我们假设有两个线程 a、b，分别运行在核心 A、B 上\n有两个 long 类型的变量 v0、v1，二者之间 没有任何关系\n由于 v0、v1 的 存储位置恰好十分接近，导致 CPU 读取 v0 的时候，恰好 将 v1 也一并读入到 同一个 Cache Line 中，问题就产生了：\n如果核心 A 修改 v0 的值，由于核心 B 也有 v0 的值，因此，核心 B 的 v0、v1 所对应的 Cache Line 的状态为 I（无效）\n此时，线程 b 想要读取 v1 的值，由于 v1 所在的 Cache Line 的状态为 I，因此需要：\n核心 A 将 v0、v1 所在的 Cache Line 写回内存，状态修改为 S 核心 B 从内存中读取 v1，状态也为 S 如果线程 b 修改 v1 的值，也会影响线程 a 对 v0 读取\n可以发现，这种情况的出现，会 十分影响性能，因为 Cache miss\n什么是伪共享？ 上面提到的场景就是伪共享问题：这种因为多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPU Cache 失效的现象称为伪共享（False Sharing）\n怎么解决伪共享带来的性能损失？ 要想解决伪共享问题，需要避免一些不相关变量位于同一个 Cache Line 中\n例如，对于下面这个结构体：\nstruct Foo{ int a; int b; }; 有可能 a、b 会因为位于同一个 Cache Line，出现伪共享问题\n在 Linux 中，可以通过 __cacheline_aligned_in_smp 宏定义来避免：\nstruct Foo{ int a; int b __cacheline_aligned_in_smp; }; 本质上还是 填充字节以达到一个 Cache Line 的大小 来避免的，典型的用空间换时间的思想\n再来看一个 Java 的一个并发框架的部分实现：\n图片来自小林 coding p1 ~ p7 的大小加起来为 56 个字节，起到填充字节的作用，避免 其它不相关变量 与 final 常量处于一个 Cache Line，导致读取 final 常量 cache miss\nCPU 是如何选择任务的？ 无论是进程还是线程（轻量级进程），都是用 task_struct 来表示\n内核选择任务的过程，就是选择 task_struct 的过程\n调度类 Linux 有三个调度类：\nDeadLine RealTime Fair 图片来自小林 coding SCHED_DEADLINE：选择 deadline 离当前时间最近的 task SCHED_FIFO：先来先服务，但是优先级高的可以抢占 SCHED_RR：轮转法，但是优先级高的可以抢占 SCHED_NORMAL：普通任务调度策略 SCHED_BATCH：后台任务调度策略（优先级比前台任务低） DeadLine 和 RealTime 调度器，都是用于 实时任务\n而 Fair 用于 普通任务\n完全公平调度（CFS） 对于普通任务，公平最重要，Fair 的调度，又叫做完全公平调度\n算法的基本思想：\n为每个任务分配一个虚拟运行时间 vruntime 调度时，选择 vruntime 少的，尽可能保证每个任务都有相似的时间执行 vruntime 的计算规则：\n对于优先级一致的任务，实际运行时间越大，vruntime 越大 对于实际运行时间越大的任务，优先级越高，vruntime 越小 CPU Runtime Queue 图片来自小林 coding，csf_rq 应为 cfs_rq 三个队列的优先级为：DeadLine \u0026gt; RealTime \u0026gt; Fair\n也就是说：实时任务总是比普通任务先执行\n调整优先级 任务的优先级 priority 的范围为 [0, 139]，priority 越小，优先级越大\n实时任务 priority 的范围为 [0, 99] 普通任务 priority 的范围为 [100, 139] 对于普通任务，可以通过调整 Nice 值，来调整优先级\nnice 值的范围为 [-20, 19]\n普通任务 new_priority = old_priority + nice\n因此，如果要想提高普通任务的优先级，可以降低 nice 值\n在启动任务的时候，可以指定 nice 值\nnice -n -1 ./helloworld 也可以调整一个运行中的进程的 nice 值\n# 查看原 nice 值 root@SkyLee:~# ps -o nice -p 1050 NI 0 # 修改 nice 值 root@SkyLee:~# renice -1 -p 1050 1050 (process ID) old priority 0, new priority -1 # 查看新 nice 值 root@SkyLee:~# ps -o nice -p 1050 NI -1 当然，普通任务不管怎么调整，优先级也不会比实时任务高，如果对实时性有要求，可以修改一个运行中的进程为实时任务：\n# 将 pid 为 1145 的进程改为实时任务，并且优先级为 1 chrt -f 1 -p 1145 什么是中断？ 中断是一种事件处理机制，OS 在收到中断请求后，会中断正在执行的进程，然后调用内核中断程序处理\n但是，如果中断时间过长，会导致之前执行的进程一直等待，用户体验不好\n并且，在处理中断时，OS 往往会屏蔽其它的中断，如果中断时间过长，会导致其它中断请求漏掉\n什么是软中断？ 内核处理中断请求的过程叫做「硬中断」，为了避免硬中断的时间过长，内核会触发一个「软中断」来处理接下来的任务，当次硬中断就结束了\n例如，网卡接收到数据以后，会向 OS 发起一个硬中断，OS 收到该中断，就来处理这个事件 OS 先屏蔽其它的中断请求，避免频繁中断降低性能 OS 触发软中断 OS 取消屏蔽其它中断，硬中断结束，由软中断处理接下来的任务 软中断处理程序负责将网卡 ringbuffer 的数据取走，放到内核的接收缓冲区 可以发现，中断分为上下两部分：\n上部分：硬中断，快速 处理中断请求 下部分：软中断，延迟 处理上部分未处理的事件，内核线程 的形式 软中断有哪些类型？ 可以在 Linux 下查看 /proc/softirqs 的内容：\nroot@SkyLee:~# cat /proc/softirqs CPU0 CPU1 HI: 2 0 TIMER: 66975830 81145851 NET_TX: 5 7 NET_RX: 7477893 7523257 BLOCK: 2728 3216316 IRQ_POLL: 0 0 TASKLET: 691 13839 SCHED: 189534698 190142385 HRTIMER: 1785 0 RCU: 147754429 147986682 这里表示有 10 中软中断，例如：NET_RX 表示网络接收中断，NET_TX 表示网络发送中断、TIMER 表示定时中断、RCU 表示 RCU 锁中断、SCHED 表示内核调度中断。\n如果软中断占用了较高的系统资源，怎么定位，解决？ 首先使用 top 查看当前软中断占用了多少资源：\ntop - 10:16:08 up 38 days, 15:33, 1 user, load average: 0.01, 0.01, 0.00 Tasks: 132 total, 1 running, 131 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.5 us, 0.5 sy, 0.0 ni, 99.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st si 就是软中断占用 CPU 的比例，如果发现占用较高，可以使用 watch 来查看软中断次数的变化频率：\nwatch -d cat /proc/softirqs 如果发现某个软中断的变化频率较高，例如对于网络 IO 比较频繁的 Server，NET_RX 的变化频率需要格外注意，如果频率过高，可以使用 sar -n DEV 查看哪个网卡的数据包比较多\n这里的 eth0 网卡的包比较多，可以使用 tcpdump 抓包判断有没有非法的报文，可以考虑加防火墙，或者升级带宽等等\n","permalink":"https://blogs.skylee.top/posts/os/%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84/note/","tags":["OS","Linux"],"title":"Linux Hardware"},{"categories":["Network"],"content":" IP 分类的优缺点 优点：简单、选路方便\n缺点：\n同一网络下，没有层次 不能很好与现实匹配（对于 B 类地址，主机号太多，一个企业难以用完，造成浪费；对于 C 类地址，主机号太少，不够用） 这些缺点均在 CIDR 编址解决\nIPv6 基本认识 标识方法 IPv6 使用 128 位编码，每 16 位为一组，每组之间使用 : 隔开\n例如：fe80::18f9:e2fe:e673:fe24\n类型 回环地址 链路本地单播地址：不经过路由器转发 唯一本地地址：相当于 IPv4 的私有 IP 全局单播地址：相当于 IPv4 的公有 IP 多播地址 图片来自小林 coding 首部 删除了首部校验和字段 删除了是否允许分片字段 删除了选项字段 亮点 可以为很多主机分配一个 IPv6 地址，不用担心用不完 由于可分配的地址很多，可以避免多层 NAT 带来的性能开销 删除了是否允许分片字段，避免在路由器分片带来的性能开销 可自动配置，不需要 DHCP 服务器 IP 相关技术 ARP ARP 协议用于：根据 IP 地址获取对应主机的 MAC 地址\n由于数据的传输需要经过 IP 层 =\u0026gt; 数据链路层\n而数据链路层是 点对点 的，知道下一跳的 IP 还不行，必须知道下一跳的 MAC 地址，这样数据链路层才能知道该发给谁\nARP 协议获取 MAC 地址的大致流程：\n根据 IP 发送 广播 ARP 请求 子网内，每个逐渐会检查自己的 IP 地址与请求的 IP 地址是否一致，如果一致，该主机会广播自己的 MAC 地址 收到 MAC 地址后，请求方缓存 IP 到 MAC 的映射关系 还有一种协议：RARP\n与 ARP 相反，RARP 用于：一个主机根据自己的 MAC 地址，获取自己的 IP 地址\nDHCP DHCP 协议提供一种机制：即插即用联网。用户可以在不手动配置的情况下，自动配置 IP 等信息\nDHCP 有如下几种报文：\nDHCP DISCOVER：服务发现报文 DHCP OFFER：提供配置信息 DHCP RESQUEST：请求使用配置 DHCP ACK DHCP NACK DHCP RELEASE：释放配置 使用 DHCP 的过程如下：\nDHCP 协议是基于 UDP 的，在获取到 IP 之前，源 IP 为 0.0.0.0\n问题来了，由于路由器不会转发广播请求，那每个子网都要一个 DHCP 服务器吗？\n如果这样，比较浪费，实际处理时，会引入 DHCP 中继代理\n每个子网都有一个 DHCP 中继，多个 DHCP 中继可以请求同一个 DHCP Server\nNAT IPv4 只有 32 位，再加上分类编址，实际可用的 IP 数不能满足目前的需求\n于是出现了 NAT 技术来 缓解 这个问题\nNAT 可以将一个子网内的私有 IP 映射到同一个公网 IP\n但是，当子网内有一个设备正在使用 NAT 时，其它设备就得 等待 该设备释放 NAT Server 的资源\nNAPT NAPT 就是用来解决上面 串行化 的问题的\n大部分应用使用的都是 TCP、UDP 协议，这意味着是 IP + Port 的形式\nNAPT 就是引入了 端口 这个概念，实现一个公网 IP + Port 映射多个私有 IP + Port\n缺点 但是，NAPT 仍然存在缺点：\n外部无法主动向子网内部建立连接 通信过程中，如果 NAPT Server 挂了，那么 所有的 TCP 连接都将被 重置 转换过程存在 性能开销：实际应用通常有多层 NAT 转换（大内网） 如何解决这些缺点？\n使用 IPv6，源头上解决问题 NAT 穿透 NAT 穿透：NAT 内层设备获取 NAT Server 的公网 IP，并手动添加映射\nICMP ICMP（Internet Control Message Protocol），工作在网络层，用于：\n确认数据报是否到达 报告数据报丢弃原因 ICMP 有两大类型：\n查询报文类型：包含 Echo Request 和 Echo Reply 类型 差错报文类型：用于报告错误原因 Ping \u0026mdash; 基于 ICMP 查询报文 执行 Ping 命令，实际上用到了 ICMP 的 Echo Request 和 Echo Reply 类型\nIP 首部有一个 TTL 字段，每经过一跳，TTL 减一\n当 TTL 为 0，路由器将会抛弃该报文\n执行 ping 命令，发送 icmp echo request 如果目的主机收到 icmp echo request，会回复 icmp echo reply 如果本机收到 reply，说明目的主机可达 如果本机超时未收到 reply，说明目的主机不可达 traceroute \u0026ndash; 基于 ICMP 差错报文 差错报文包括：\n目标不可达 原点抑制：如果网络拥塞，返回原点抑制报文（不常用） 重定向：如果有更短的路径，返回重定向报文（包含更近的路由信息） 超时 目标不可达包括：\n网络不可达 主机不可达 协议不可达 端口不可达 需要分片，但是首部设置不允许分片 traceroute 的使用场景\n==追踪从原主机到目的主机到路由信息==\nSky_Lee@SkyLeeMacBook-Pro IP % traceroute baidu.com traceroute: Warning: baidu.com has multiple addresses; using 39.156.66.10 traceroute to baidu.com (39.156.66.10), 64 hops max, 52 byte packets 1 192.168.0.1 (192.168.0.1) 1.981 ms 1.303 ms 1.055 ms 2 192.168.1.1 (192.168.1.1) 2.001 ms 1.796 ms 1.740 ms 3 100.69.0.1 (100.69.0.1) 5.775 ms 8.928 ms 5.350 ms 4 110.190.151.153 (110.190.151.153) 7.396 ms 110.190.151.165 (110.190.151.165) 6.344 ms 110.190.151.153 (110.190.151.153) 5.360 ms 5 * 110.188.7.25 (110.188.7.25) 9.507 ms 171.208.196.125 (171.208.196.125) 13.722 ms 6 * * * 7 202.97.74.226 (202.97.74.226) 37.816 ms 202.97.17.74 (202.97.17.74) 37.063 ms 原理：\n第一次的 TTL 设置为 1，到第一个路由器，TTL 为 0，返回超时差错报文 第二次的 TTL 设置为 2，到第二个路由器，TTL 为 0，返回超时差错报文 \u0026hellip;\u0026hellip; 那么，traceroute 怎么知道是否到达目的主机？\ntraceroute 在发送 UDP 包的时候，为了避免目标误以为这是一个正常的 UDP 通信请求，通常会设置一个不太可能被应用程序使用的较大的端口号值作为目标 UDP 端口号。\n当目的主机收到这个 UDP 包，会返回一个 ICMP 端口不可达报文\n因此，当 traceroute 收到 ICMP 端口不可达报文，说明到达目的主机\n如果目的主机恰好监听了这个端口呢？\n这个行为是未定义的\n==路径 MTU 发现==\n原理：\n发送端将要传输的数据包大小设置为本地主机的 MTU 大小，并且设置为 不允许分片 数据包经过第一个路由器时，如果该路由器的 MTU 小于数据包大小，那么该路由器会丢弃该数据包，并发送一个 ICMPFragmentationNeeded（需要分片，但是首部设置不允许分片） 消息给发送端。 发送端收到 “ICMPFragmentationNeeded” 消息后，会根据该消息中指示的最小 MTU 大小，将数据包大小缩减为最小 MTU 大小，并重新发送。 经过多个路由器的重复上述过程，直到数据包能够成功传输到目的主机。 ","permalink":"https://blogs.skylee.top/posts/network/ip/%E5%9F%BA%E7%A1%80/","tags":["Network","IP"],"title":"IP 协议基础"},{"categories":["Network"],"content":" 如何理解 TCP 面向字节流？ 由于 TCP 的滑动窗口机制（发送窗口会动态变化），在发送数据时，原数据可能发生分片\n举个例子：原数据为一个字符串 \u0026ldquo;114514\u0026rdquo;\n经过分片后，接收方可能收到多个 TCP 报文，假设为：\n\u0026ldquo;11\u0026rdquo; \u0026ldquo;4\u0026rdquo; \u0026ldquo;514\u0026rdquo; 内核在收到这些数据后，会将数据载荷部分扔到接收缓冲区，等待应用程序调用 read 取走数据\n由于分片机制，我们 不能认为一个 TCP 报文对应一个原始的用户数据，因此，我们说 TCP 是面向字节流的\n相反，对于 UDP 协议而言，在传输层不存在分片机制，即使在 IP 层分片，接收方也会将分片后的数据重组为原数据，因此，我们说 UDP 是面向报文的\n如何解决粘包问题？ 例如，发送方发送了两条数据：\n\u0026ldquo;114514\u0026rdquo; \u0026ldquo;niuma\u0026rdquo; 如果使用 TCP 协议，并且发送窗口比较大，TCP 为了提高效率，会将这两条数据合并发送，接收方收到的数据就是 \u0026ldquo;114514niuma\u0026rdquo; 了\n问题是，接收方并不知道这个数据，实际上是两条，这就是所谓的粘包问题\n如何解决？\n固定长度消息：双方约定每条消息的长度（不灵活，一般不用） 添加分隔符（如 HTTP 协议的 \\n） 自定义消息格式：可以定义一个消息体，包含数据的字节数信息： struct { u_int32_t message_length; char message_data[]; } msg; 为什么 TCP 每次的初始 Seq 都要不一样？ 主要原因是：避免历史报文被现有连接错误接收\n这个问题主要是针对同一个四元组连接\n图片来自小林 coding 如果每次初始的 Seq 都不一样，就可以 很大程度 避免这个问题\n注意：并不是绝对避免，因为 Seq 存在「回环」问题\n在 TCP 建立连接的时候，客户端和服务端都会各自生成一个初始序列号，它是基于时钟生成的一个随机数，来保证每个连接都拥有不同的初始序列号。初始化序列号可被视为一个 32 位的计数器，该计数器的数值每 4 微秒加 1，循环一次需要 4.55 小时。\n如果 Seq 循环了一次，那还是有可能发生这个问题的：\nTCP 连接建立的时间很长，回环 带宽大，每次发送的数据包很大，序列号用完了，回环 为了解决这个问题，TCP 又引入了 时间戳（32 位） 的概念，以及 PAWS 算法\nPAWS 算法会在收到一个数据包时，将数据包内的时间戳（A）与上一个包的时间戳（B）做比较，如果：\nA \u0026gt; B：有效，接收 A \u0026lt;= B：过期，丢弃 因此，初始 Seq 随机 + 时间戳 可以有效避免历史报文的错误接收问题\nSYN 报文什么时候可能会被丢弃？ tcp_tw_recycle\n启用 tcp_tw_recycle，就会快速回收处于 TIME_WAIT 状态的连接\n但如果开启了 tcp_tw_recycle + 时间戳机制，就会启用 pre-host 的 PAWS，这个与之前的 PAWS 不同，它是对「对端 IP」做 PAWS 检查，而不是「对端 IP + Port」\n假设客户端的网络使用了 NAT，就会产生麻烦，这里假设客户端 A、B 处于同一个 NAT 下：\nB(11.45.1.4:1146) 先与 Server 建立连接 A(11.45.1.4:1145) 后与 Server 建立连接，但 SYN 报文比 B 先到达，Server 保存 A SYN 报文的时间戳 此时，B 的 SYN 报文到达，但 B 的时间戳比 A 小（因为先建立连接），由于启用了 NAT（A、B IP 是一样的），并且 Server 启用 pre-host 的 PAWS（只检查对端 IP），就会认为 B 的 SYN 报文是过期报文，丢弃 可见，tcp_tw_recycle 在 NAT 下是不安全的，存在潜在的错误丢弃 SYN 报文的可能性\n并且，当今网络多层 NAT 的使用非常普遍\n因此，4.12 版本之后的 Linux，取消了 tcp_tw_recycle\n半连接队列溢出\n在不开启 syn_cookies 的情况下，如果半连接队列满了，也会丢弃 SYN 报文\n全连接队列溢出\n如果全连接队列溢出，会丢弃新的连接请求\n已建立连接的 TCP，收到 SYN 报文会发生什么？ 这个要分两种情况，客户端发送 SYN 报文的端口号与之前是否一致：\n如果不一致\nServer 会认为这是一个新的连接请求，当然会回复 SYN + ACK 给对方\n如果一致\n说明客户端可能宕机，重启后再次用相同的 port 与 Server 建立连接\n由于初识 Seq 为随机值，基本上不可能与服务端的 last ack 的值相等\n当服务器返回 ack 后（这个 ack 对应的值基本上不可能等于 clnt_seq + 1），由于 ack 的 seq 不合法，client 会发生 RST 报文，终止 TCP 连接\n图片来自小林 coding 如何关闭一个 TCP 连接？ 直接关闭进程：内核会自动关闭进程存在的 TCP 连接 killcx tcpkill killcx 与 tcpkill 的原理都是基于伪造 RST 报文实现的，只不过获取伪造 RST 报文 seq 的方式不同\n注意：如果 RST 报文的 seq 与预期不一致，会丢弃该报文\nkillcx killcx 的原理是：根据提供的「源 IP + 源 Port」模拟一个 Client 向「目的 IP + 目的 Port」的 Server 发起一个 SYN 报文\n当 Server 收到这个 SYN 报文，会返回 ACK 报文（包含了 seq 以及 ack 的值）\nkillcx 收到 seq 和 ack，可以反推：\n原客户端的 seq（就是 server 返回的 ack） Server 的 seq（就是 server 返回的 seq） 于是，拿到这两个 seq，就可以分别向 Client、Server 发送伪造的 RST 报文，达到关闭 TCP 连接的效果\n图片来自小林 coding tcpkill 与 killcx 有点不同，tcpkill 获取伪造 RST 报文 seq 的方式是被动的，即监听对应的 TCP 连接\n只有当待 kill 的 TCP 连接有数据包的发送，才能获取 seq\n因此，tcpkill 只能关闭活跃的 TCP 连接\n四次挥手中收到乱序的 FIN 报文怎么处理？ 看下场景：\nTCP 里面那个 shutdown 函数主动方调用只关闭写端情况下，假如服务端在二三次挥手之间发的数据，或者是四次挥手之前的数据包，因为网络阻塞导致第三次挥手的 FIN 包比数据包先到主动关闭方，那么主动关闭方收到 FIN 是否就会进入 timewait 状态？这时候那个延迟的数据包到了还能正常接收并处理么？\n意思就是：在 FIN_WAIT_2 状态下，如果 FIN 报文比 data packet 先到达，是否还能正确处理 data packet？会不会丢数据？\n答案是：不会\n在 FIN_WAIT_2 状态下，收到一条消息，内核会做以下步骤：\n检查当前消息的 seq 是否符合预期（也就是是否乱序） 如果符合预期，判断消息是否包含 FIN 如果包含 FIN： 发送 ack 给对方，并进入 TIME_WAIT 清空「乱序队列」 如果不包含 FIN：将消息写入 buffer，判断「乱序队列」中是否有「符合条件的消息」 如果有，判断是否包含 FIN 如果包含 FIN，发送 ack 给对方，并进入 TIME_WAIT 否则，一并写到 buffer 否则，写到「乱序队列」 符合条件是指 seq 是否等于 lastSeq + lastLen\n处于 TIME_WAIT 的连接，收到 SYN 报文怎么处理？ 如果收到的是不同的 源 IP + Port 组，那么属于不同连接，直接建立即可，否则：\n这个主要是判断 SYN 报文的「合法性」\n如果 SYN 报文合法，复用连接，跳过 TIME_WAIT，进入 SYN_RECV 如果 SYN 报文不合法，发送 RST 报文给对方 这里的合法是指：SYN 报文的 Seq 比 Server 期望的 Seq 大\n如果启用了时间戳机制，那么 还要保证 SYN 报文的 timestamp 比 Server 期望的 timestamp 大\n处于 TIME_WAIT 的连接，收到 RST 报文怎么处理？ 在 RST 报文合法的前提下，处理方式与 tcp_rfc1337 参数有关\n0: 提前结束 TIME_WAIT，关闭 TCP 连接 1: 丢弃 RST 报文 默认值为 0，但建议修改成 1\nTIME_WAIT 是我们的朋友，它是有助于我们的，不要试图避免这个状态，而是应该弄清楚它。 \u0026mdash;《UNIX 网络编程》\n虽然 TIME_WAIT 状态持续的时间是有一点长，显得很不友好，但是它被设计来就是用来避免发生乱七八糟的事情。\nTCP 连接，客户端进程崩溃与断电，分别发生什么？ 客户端进程崩溃\n客户端进程崩溃，内核会自动关闭进程建立的 TCP 连接\n客户端断电\n服务端对客户端断电是无感的，分两种情况考虑：\n如果有数据传输，那么服务器会因为收不到 ack，而持续重传，直到重传次数上限 如果没有数据传输，那么只能依靠 TCP 的 keep-alive 机制来断开连接了 客户端拔掉网线后，原本的 TCP 连接还存在吗？ 存在\n拔掉网线，并不会直接影响已存在的 TCP 连接\n如果客户端与服务器之间 没有数据传输 ，那么断开网线的过程，对双方都是无感的，当然，甚至，如果没启用 keep-alive，TCP 连接将一直建立\n当然，如果二者之间存在数据传输，那么会由于重传次数到达上限导致 TCP 连接断开\nLinux 为什么默认关闭了 tcp_tw_reuse 选项？ 首先弄清楚 tcp_tw_reuse 选项的作用\n如果启用了 tcp_tw_reuse，对于 客户端 而言，在调用 connect 时，若 connect 的「四元组」处于 TIME_WAIT 状态，并且 时间超过 1s，就可以重用该四元组建立新的连接\nps：启用 tcp_tw_reuse，需要同时开启 tcp_timestamp\n如果启用 tcp_tw_reuse，对于客户端而言，就相当于没有 TIME_WAIT，或者时间很短\nTIME_WAIT 的作用：\n防止历史报文对现有连接的干扰 保证被动关闭方能正确关闭 第二点好理解，但第一点呢？\n不是开启了 timestamp 吗？就算序列号「回环」了，也不会有历史报文的干扰啊？\n事实上，对于 RST 报文，timestamp 规则不再适用\nIt is recommended that RST segments NOT carry timestamps, and that RST segments be acceptable regardless of their timestamp. Old duplicate RST segments should be exceedingly unlikely, and their cleanup function should take precedence over timestamps.\n这导致了连接的非正常关闭\n因此，为了 TIME_WAIT 能正常发挥作用，Linux 默认关闭了 tcp_tw_reuse\nHTTPS 中，TCP 三次握手与 TLS 握手能同时进行吗？ 可以，但有前提\n我们知道，传统的 HTTPS 的建立，需要经过 TCP 三次握手 + TLS 四次握手\n但是，如果\n打开了 TCP 的快启动（TFO），以及使用 1.3 版本的 TLS Client 与 Server 已经通信过一次 就可以实现「同时」进行\nTLS 1.3 引入了 会话恢复 机制，在第二次建立连接，只需要 0-RTT\n结合 TFO，在第一次握手时，携带 Cookie + TLS1.3 的 Client Hello，Server 就可以在三次握手结束前，发送 Server Hello，建立 TLS 连接\nTCP 的缺点 实现在传输层，如果需要修订，需要动内核 建立连接繁琐 存在「队头阻塞」 不支持连接迁移 QUIC 协议 概念 QUIC 是基于 UDP，在应用层实现的具有可靠传输、流量控制、拥塞控制的协议\n目前，HTTP/3 的实现就是基于 QUIC 协议的\n结构 Packet 首先是 Packet Header，有两类：\nLong Packet Header：初次建立连接使用 Short Packet Header：后续通信使用 QUIC 使用 ConnectionID 来标明每一个连接\n每一个 packet，都有一个 唯一的 PacketNumber，这个值是严格递增的\n使用严格递增的 PacketNumber，相较于 TCP 的 Seq，有两个优点：\n支持乱序确认 更精确的计算 RTT，从而得到更精确的 RTO Frame 每一个 Packet 中，包含了若干个 Frame\nFrame 有多种类型，这里仅介绍 Stream 类型\nStreamID：区别多个并发的 HTTP 连接 Offset：类似 TCP 的 Seq，保证 同一个 Stream 内数据的有序性 Length：负载数据的长度 三次握手 QUIC 协议的连接整合了 TCP 的三次握手和 TLS1.3 的安全握手。\n客户端发送 Initial 包：客户端首先生成一个独特的 Connection ID，并将其包含在 Initial 包中。这个包还包含一个 TLS 客户端 Hello 消息，内含用于握手和认证的相关参数。\n服务器回应 Initial 包：收到 Initial 包后，服务器会生成自己的 Connection ID，并回传一个 Initial 包，包含 TLS 服务器 Hello 消息，并可能还包含 TLS 的证书和 Finished 信息。\n客户端发送第二个 Initial 包：客户端会解密服务端发来的 Initial 包，并验证其中所包含的服务器证书。在验证通过后，客户端会产生一个新的 Initial 包，其中包含 TLS 的 Finished 信息，以确认握手的完成。此后，双方就建立起了一个加密的通信连接。\n重传机制 你可能会想：QUIC 的每个包的 PacketNum 都不一样，重传时，接收方怎么判断一个报文是新的 packet，还是重传的 packet？\n实际上，QUIC 使用 StreamID + Offset 来唯一确定\n即使两个包的 packetNum 不同，但只要 StreamID + Offset 相同，就认为这两个包是相同的\n流量控制 QUIC 也使用窗口来进行流量控制，有两个维度：\nStream Connection Stream 对于同一个 Packet 的多个 Stream 而言，每个 Stream 有自己独立的接收（发送）窗口\n因此，QUIC 可以实现基于 Stream 的细粒度的流量控制\nConnection 可以认为有一个「大」的窗口来控制整个 QUIC 连接的流量，其大小为所有 Stream 的窗口大小之和\n如何解决队头阻塞 前面提到每个 Stream 有自己独立的接收（发送）窗口\nStream 之间是独立的，没有关联（理解成并行的 HTTP 连接）\n因此，即使某个 Stream 的数据出现了丢失，也不会影响其它 Stream\n所以说 QUIC 解决了不同 Stream 间的队头阻塞问题，实现了「真正的」并行\n连接迁移 前面提到，QUIC 基于 ConnectionID 来标识每一个 QUIC 连接\n当用户的网络更改（如 WLAN 切换到 5G），也不用重新建立连接，复用 Connection ID 等上下文信息即可\n优势 经过上面的分析，可以总结：相较于 TCP，QUIC 有以下优势：\n基于应用层实现，便于修订 更快的连接建立：整合三次握手 + TLS 握手 细粒度的流量控制：解决队头阻塞，实现「并行」传输 支持连接迁移 参考资料：如何基于 UDP 协议实现可靠传输 端口问题 TCP 和 UDP 可以使用同一个端口吗？ 可以\nTCP 与 UDP 为传输层的不同协议，接收数据时，当有数据从网卡过来，会在网络层判断这个数据包是哪个协议的，进而送到不同协议栈去处理\n图片来自小林 coding 多个 TCP 服务进程可以使用同一个端口吗？ 分情况\n如果两个 TCP 服务进程的监听的 IP + Port 都相同，就不能使用同一端口 如果监听的 IP 不同，就可以使用同一端口 注意：如果某个进程监听的 IP 为 0.0.0.0，第二个原则就失效了，因为监听 0.0.0.0 意味着监听了本机的所有 IP\n如何解决服务器快速重启，出现 \u0026ldquo;Address already in use\u0026rdquo;？ 这个问题的出现，是因为快速重启时，监听的 IP、监听的 Port、目的 IP *、目的端口 * 四元组对应的连接还处于 TIME_WAIT 状态\n因此，会提示地址被占用\n服务器可以在设置 socket 的时候加上 SO_REUSEADDR 选项，以复用处于 TIME_WAIT 的连接\n多个 TCP 客户进程可以使用同一端口吗？ 一样的，对于同一个四元组对应的连接，只要连接建立，端口就不能复用\n相反，只要连接不同（如服务器 IP 不同），就可以使用相同的端口\nServer 没有调用 listen，Client 发起连接，会发生什么？ 如果 Server bind 了某个 IP + Port，但 没有 listen，当 Client 发送 SYN 给 Server，Server 不会回 ACK 给 Client\nClient 会一直重试，直到重试次数达到上限\n但是，如果 Server 没有 bind 客户端的目的 IP + Port ，Client 发起连接，Server 会直接发一个 RST 报文给对方\n通常情况下，当一个连接请求到达本地却没有相关进程在目的端口侦听时就会产生一个重置报文段。\nUDP 协议规定，当一个数据报到达一个不能使用的目的端口时就会产生一个 ICMP 目的地不可达（端口不可达）的消息。TCP 协议则使用 重置报文段 来代替完成相关工作。\n-- 《TCP/IP详解 卷1：协议（原书第2版）》第 13.6.1 节 半连接队列、全连接队列的本质？ 虽然都有「队列」二字，但二者 实际上都不是队列\n当一个连接请求到来，内核会将连接元数据存到 半连接队列，发送 ack 收到某个连接的 ack 的 ack 后，从半连接队列中取出该连接，发送 ack，并放到 全连接队列 中 服务进程调用 accept，取出一个建立好的 TCP 连接 “收到某个连接的 ack 的 ack 后，从半连接队列中取出该连接”的「取出」过程是一个随机的过程，因为这个 ack 的顺序并不固定\n如果半连接队列设计成线性结构，那么取出对应连接就需要 O(n) 的时间\n因此，半连接队列实际上是一个哈希表，取出对应连接的期望时间是 O(1)\n而全连接队列实际上是一个链表，当然，也可以理解成队列（毕竟队列也有基于链表实现的）\n没有 accept，可以建立 TCP 连接吗？ 弄清楚 accept 的本质：从全连接队列中取出一个 已经建立好的连接，处理接下来的请求\n也就是说，在 accept 前，TCP 连接就已经建立好了\n因此，accept 与建立 TCP 连接之间没有关系，没有 accept，当然可以建立 TCP 连接\n没有 listen，为什么可以建立连接？ 以 TCP 自连接为例：\n#include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #define LOCAL_IP_ADDR\t(0x7F000001) // IP 127.0.0.1 #define LOCAL_TCP_PORT\t(1145) // 端口 int main(void) { struct sockaddr_in local, peer; int ret; char buf[128]; int sock = socket(AF_INET, SOCK_STREAM, 0); memset(\u0026amp;local, 0, sizeof(local)); memset(\u0026amp;peer, 0, sizeof(peer)); local.sin_family = AF_INET; local.sin_port = htons(LOCAL_TCP_PORT); local.sin_addr.s_addr = htonl(LOCAL_IP_ADDR); peer = local; int flag = 1; ret = setsockopt(sock, SOL_SOCKET, SO_REUSEADDR, \u0026amp;flag, sizeof(flag)); if (ret == -1) { printf(\u0026#34;Fail to setsocket SO_REUSEADDR: %s\\n\u0026#34;, strerror(errno)); exit(1); } ret = bind(sock, (const struct sockaddr *)\u0026amp;local, sizeof(local)); if (ret) { printf(\u0026#34;Fail to bind: %s\\n\u0026#34;, strerror(errno)); exit(1); } ret = connect(sock, (const struct sockaddr *)\u0026amp;peer, sizeof(peer)); if (ret) { printf(\u0026#34;Fail to connect myself: %s\\n\u0026#34;, strerror(errno)); exit(1); } printf(\u0026#34;Connect to myself successfully\\n\u0026#34;); //发送数据 strcpy(buf, \u0026#34;Hello, myself~\u0026#34;); send(sock, buf, strlen(buf), 0); memset(buf, 0, sizeof(buf)); //接收数据 recv(sock, buf, sizeof(buf), 0); printf(\u0026#34;Recv the msg: %s\\n\u0026#34;, buf); sleep(1000); close(sock); return 0; } 运行，抓包结果如下：\n可以发现，自连接也是需要三次握手建立连接\n当发起 connect 系统调用，内核会将连接信息放到一个全局 hash 表中，再将连接消息发出， 由于是一个回环地址（127.0.0.1），消息又回到传输层 内核检测有没有进程 bind 了该 IP + Port，有 于是直接从全局 hash 表取出连接信息，发现源 IP、Port 与目的 IP、Port 相等，直接建立连接 用了 TCP，数据就一定不会丢失吗？ 建立连接丢包 如果 Server 的半连接队列或者全连接队列满了，会丢弃新的连接请求\n流量控制丢包 事实上，数据在被发送前，还需要排队\nqdisc 是\u0026quot;queueing discipline\u0026quot;的缩写，也就是排队规则，位于网络模型的链路层和网络层之间，它主要在操作系统内核对数据包进行排队，策略路由，速率限制等操作。\nqdisc 有容量限制，如果应用发送速率太快，导致队列的数据包太多，超过容量限制，内核就会丢弃这些数据包\n可以使用 tc 查看某个设备是否因为流量控制丢包：\n[skylee@localhost Test]$ tc -s qdisc show dev ens33 qdisc pfifo_fast 0: root refcnt 2 bands 3 priomap 1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1 Sent 6092849 bytes 20768 pkt (dropped 0, overlimits 0 requeues 1) backlog 0b 0p requeues 1 网卡丢包 接收数据时，网卡会先将数据放到 ringbuffer 中\n如果 ringbuffer 满了，还有新的数据包到来，就会丢包\n可以使用 ifconfig 查看是否因为 ringbuffer 满了导致丢包：\n[skylee@localhost Test]$ ifconfig ens33: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.124.114 netmask 255.255.255.0 broadcast 192.168.124.255 RX packets 127108 bytes 177474834 (169.2 MiB) RX errors 0 dropped 0 overruns 0 frame 0 RX dropped 为 0，说明没有发生\n可以通过 ethtool 查看网卡配置：\n[skylee@localhost Test]$ ethtool -g \u0026lt;device name\u0026gt; Ring parameters for ens33: Pre-set maximums: RX: 4096 RX Mini: 0 RX Jumbo: 0 TX: 4096 Current hardware settings: RX: 256 RX Mini: 0 RX Jumbo: 0 TX: 256 ringbuffer 的大小为 4096，但是只用了 256\n可以使用 ethtool 修改 ringbuffer 的大小：\n[root@localhost Test]# sudo ethtool -G ens33 rx 4096 tx 4096 接收缓冲区满导致丢包 如果接收缓冲区满了，接收方通常会发送一个「零窗口通告」报文\n但发送方如果仍继续发送数据，那么将会导致丢包\n不过这种情况一般不会出现，收到「零窗口通告」，发送方应该停止发送数据\n两端中间节点丢包 数据包的传输，需要经过许多节点，如果这些节点出现问题（如路由器负载高），可能发生丢包\n可以使用 mrt（my trace route）工具来查看是否出现这个问题：\n[root@localhost Test]# mtr -r baidu.com Start: Wed Jan 31 11:13:41 2024 HOST: localhost.localdomain Loss% Snt Last Avg Best Wrst StDev 1.|-- gateway 0.0% 10 0.4 0.6 0.4 0.8 0.0 2.|-- 192.168.0.1 0.0% 10 1.9 8.1 1.5 62.3 19.0 3.|-- 192.168.1.1 0.0% 10 2.8 2.8 2.3 3.6 0.0 4.|-- 100.69.0.1 0.0% 10 6.1 6.9 4.9 12.5 2.2 5.|-- 110.190.151.153 50.0% 10 9.7 6.9 5.6 9.7 1.5 6.|-- 171.208.198.185 40.0% 10 8.2 8.4 7.8 9.0 0.0 7.|-- 202.97.78.189 20.0% 10 31.5 31.9 30.8 32.9 0.5 8.|-- 202.97.17.74 30.0% 10 35.9 36.8 35.9 37.8 0.0 9.|-- 221.183.128.137 30.0% 10 36.1 37.0 36.1 38.7 0.7 10.|-- 221.183.94.21 60.0% 10 38.5 39.1 38.5 39.8 0.0 11.|-- 221.183.49.122 0.0% 10 36.5 37.5 36.0 38.9 0.8 12.|-- 111.13.188.38 0.0% 10 37.5 37.9 37.3 38.9 0.0 13.|-- 39.156.27.1 20.0% 10 37.6 37.6 37.0 38.9 0.0 14.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 15.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 16.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 17.|-- ??? 100.0 9 0.0 0.0 0.0 0.0 0.0 18.|-- 39.156.66.10 0.0% 9 40.1 40.1 39.1 41.7 0.5 最后一行是 0，说明没有丢包（不用管前面的）\n但是，如果不是 0，例如：\n说明丢包是从最接近的那一行产生的（这个示例为 11 行）\n如果这个 IP 是内网的话，就可以根据 IP 排查一波\n当然，由于 TCP 的重传机制，即使中间节点丢包，只要不超过最大重传次数，TCP 也可以保证数据的可靠到达\n回归问题 那么，使用了 TCP 就一定不会丢包吗？\n经过前面的分析，可以发现：\n建立连接时，如果半连接队列、全连接队列满了 流量控制，qdisc 满了 网卡的 ringbuffer 满了 都有可能导致丢包\n当然，接收缓冲区满了，或者中间节点丢包，这两个问题，TCP 可以解决\n但还有一种情况，就是在「应用层」发生了 “丢包”：\n图片来自小林 coding 应用在从接收缓冲区取出数据，还没来得及本地持久化，就崩溃了\n再次启动，会发现「消息」就丢失了\n因此，TCP 只能保证传输层的「可靠」\n如何解决？ 建立连接产生的「丢包」，可以调整队列的容量（backlog、SOMAXCONN、tcp_max_syn_backlog） 流量控制产生的丢包，可以调整 qdisc 策略 网卡 ringbuffer 满了产生的丢包，可以调整 ringbuffer 的大小 对于上面提到的应用层丢包的示例，可以在发送接收双方添加一个 Server，消息先持久化到 Server，即使应用崩溃了，也可以在 Server 拉取同步数据\nshutdown 与 close shutdown 可以控制关闭 socket 的读端还是写端，而 close 是直接关闭 socket\n// server.cpp #include \u0026#34;TCPSocket.hpp\u0026#34; #include \u0026lt;iostream\u0026gt; int main(void) { auto serverSocket = new TCPSocket(\u0026#34;127.0.0.1\u0026#34;, 1145, 100); auto socket = serverSocket-\u0026gt;accept(); std::cout \u0026lt;\u0026lt; \u0026#34;Linked with \u0026#34; \u0026lt;\u0026lt; socket-\u0026gt;getIP() \u0026lt;\u0026lt; \u0026#34;:\u0026#34; \u0026lt;\u0026lt; socket-\u0026gt;getPort() \u0026lt;\u0026lt; std::endl; socket-\u0026gt;send(\u0026#34;hello\u0026#34;); sleep(100000); // socket-\u0026gt;close(); delete socket; } // client.cpp #include \u0026#34;TCPSocket.hpp\u0026#34; #include \u0026lt;cstdio\u0026gt; int main(void) { auto clnt_sock = new TCPSocket(); clnt_sock-\u0026gt;connect(\u0026#34;127.0.0.1\u0026#34;, 1145); printf(\u0026#34;connented with server\\n\u0026#34;); clnt_sock-\u0026gt;close(); sleep(100000); } Client 调用 close 后，如果 Server 在 CLOSE_WAIT 阶段向 Client 发送数据，Client 会发送 RST 报文给 Server，直接终止 TCP 连接\n如果 Client 调用的是 shutdown，只关闭写端，那么是可以正常关闭 TCP 连接的\n// client.cpp #include \u0026#34;TCPSocket.hpp\u0026#34; #include \u0026lt;cstdio\u0026gt; int main(void) { auto clnt_sock = new TCPSocket(); clnt_sock-\u0026gt;connect(\u0026#34;127.0.0.1\u0026#34;, 1145); printf(\u0026#34;connented with server\\n\u0026#34;); int socket = clnt_sock-\u0026gt;native_sock(); shutdown(socket, SHUT_WR); std::cout \u0026lt;\u0026lt; clnt_sock-\u0026gt;receive() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; sleep(100000); clnt_sock-\u0026gt;close(); } It’s important to note that shutdown() doesn’t actually close the file descriptor—it just changes its usability. To free a socket descriptor, you need to use close().\nIn contrast, close() will not only send a FIN packet but will also release the socket descriptor and any associated resources. It\u0026rsquo;s worth noting that if close() is called while data is still in the send buffer, the system will attempt to send this data to the peer before fully closing the connection.\nKeep in mind, to ensure all resources are properly cleaned up, you should always close() a socket when you\u0026rsquo;re done with it, even if you\u0026rsquo;ve already called shutdown().\nshutdown 只是改变了文件描述符的「可用性」，要真正释放资源，必须调用 close，总结如下：\n作为被动关闭方，直接调用 close 作为主动关闭方，如果有读/写的需求（这需要双方的协商，即自定协议），可以先使用 shutdown，但 最后一定要调用 close 关闭 socket ","permalink":"https://blogs.skylee.top/posts/network/tcp/%E6%9D%82%E9%A1%B9/note/","tags":["Network","TCP"],"title":"TCP 杂项知识点"},{"categories":["Network"],"content":"TCP 三次握手优化 客户端 客户端优化的手段主要是控制 SYN 报文的重传次数\n比如，在公司内网，或者服务之间相互调用，不需要多次重传（网络环境比较好），就可以适当减少 SYN 报文的重传次数，以尽快向应用报告错误\nSYN 的重传次数可以通过修改 /proc/sys/net/ipv4/tcp_syn_retries\n服务端 服务端可以通过优化：\nSYN + ACK 报文的重传次数：tcp_synack_retries 半连接队列的大小：调整 tcp_max_syn_backlog、listen 的 backlog、somaxconn 启用 syn_cookies 防止 SYN 攻击 全连接队列的大小：调整 listen 的 backlog、somaxconn “绕过”三次握手 这里的「绕过」不是真正绕过三次握手，而是 TCP 的 快启动 机制，在 第一次握手时，携带应用层数据 + cookie，减少请求数\n例如，对于 HTTP 而言，通常的形式是这样：\n如果启用了快启动，是这样的：\n图片来自小林 coding 第一次握手，携带了 请求 Cookies，即要求 Server 启用快启动机制 Server 如果支持快启动，会在本地生成一个 cookie，在第二次握手时发送给 Client 第三次握手，带上 HTTP 请求，这一步与之前一样 重点来了，下一次客户端第一次握手时，除了 SYN 以外，还会带上 Server 之前发来的 cookie + HTTP GET\nServer 除了发送 ACK 以外，还会判断 Cookie 是否有效（过期）：\n如果有效，发送 GET 请求对应的资源给 Client 如果无效，忽略 Cookie 可以发现，启用了 快启动机制，可以减少一次 RTT 的消耗\n此外，cookie 的值是放在 TCP 首部的 Opinion 中的\n那么在 Linux 下，如何启用快启动呢？\n修改 /proc/sys/net/ipv4/tcp_fastopn，有三个值可选：\n0：关闭 1：作为 Client 启用 2：作为 Server 启用 3：都启用 注意：需要通信双方都启用才行\nTCP 四次挥手优化 主动方 FIN_WAIT_1 的优化\n可以通过 tcp_orphan_retries 调整 FIN 报文的重传次数\ntcp_orphan_retries 的默认值为 0，但实际上对应的是重传 8 次\nFIN_WAIT_2 的优化\n如果调用的是 close 来关闭的 TCP 连接，因为 close 后的套接字，收发能力都不具备（因此也叫做 孤儿连接），不应该在 FIN_WAIT_2 等待过长时间，因此可以通过调整 tcp_fin_timeout 来修改等待的时间\n默认值为 2MSL，与 TIME_WAIT 的时间一样\n但是，如果是调用的 shutdown 来关闭的 TCP 连接，就不能修改等待时间了，shutdown 关闭的套接字，FIN_WAIT_2 的时间是不受 tcp_fin_timeout 控制的\n此外，还可以调整 tcp_max_orphans 来修改 孤儿连接 的最大数，以避免太多的连接处于 FIN_WAIT_2\nTIME_WAIT\n如果有太多连接处于 TIME_WAIT，会造成资源浪费\n可以通过修改 tcp_max_tw_buckets 来修改处于 TIME_WAIT 的连接数上限，超过上限以后，如果还有连接要释放，就会直接释放而不进入 TIME_WAIT\n此外，连接发起方 还可以 复用 TIME_WAIT 的连接，需要修改两个参数：\ntcp_tw_reuse：1 tcp_timestamps：1（为报文引入时间戳） 被动方 被动方，可以优化的点就只有 FIN 报文的重传次数了，还是修改 tcp_orphan_retries\n此外，还需要注意及时调用 close 来避免大量连接处于 CLOSE_WAIT 状态\nTCP 传输数据的优化 滑动窗口 TCP 首部滑动窗口的大小默认只有 16 位，即 65535 字节（64KB）\n对于当今的网络来说，64 KB 太小了，要发送一个包，可能得分很多次，这无疑降低了效率\n在 TCP 选项字段定义了窗口扩大因子，用于扩大 TCP 通告窗口，最大可以到 1G\n因此，可以根据实际情况动态调整窗口的大小\n在 Linux 中可以通过修改 tcp_window_scaling 的值为 1 来打开窗口扩大选项（默认已经打开）\n当然，窗口也不能真的有 1G 大（除非网络带宽很大）\n窗口大小还要考虑一个重要参数：带宽时延积\n例如，带宽为 100M，往返时延 RTT 为 10ms，那么，带宽时延积就为 100 * 0.01 = 1M\n窗口的大小不应该超过带宽时延积，否则可能会造成网络负载太高，丢包率上升\n缓冲区 仅仅修改窗口的大小还不够，OS 会根据发送（接收）缓冲区的大小动态调整发送（接收）窗口的大小\n因此，即使启用了窗口扩大选项，如果缓冲区很小，窗口的大小也会很小\n如何修改缓冲区的大小呢？\n可以通过修改 tcp_wmem 和 tcp_rmem 来调整发送（接收）缓冲区的大小范围，格式如下：\nroot@SkyLee:~# cat /proc/sys/net/ipv4/tcp_wmem 4096\t16384\t4194304 4096：下限为 4096 字节，即使内存有压力的情况下也要保证的 16384：初识值为 16384 字节 4194304：上限为 4194304 字节 前面提到，窗口的大小不应该超过带宽时延积，因此，这里就要注意设置 缓冲区上限 \u0026lt;= 带宽时延积\n在 Linux 中，默认 已经启用了动态修改 发送缓冲区 的大小，但接收缓冲区的调节功能需要修改 tcp_moderate_rcvbuf 为 1 来开启\n接收缓冲区的调整是基于 tcp_mem 的：\nroot@SkyLee:~# cat /proc/sys/net/ipv4/tcp_mem 18603\t24804\t37206 上面的数字不是字节数，而是页面数（一般来说，1 页 = 4K）\n当 TCP 使用内存：\n小于 18603 * 4KB 时，不需要调整接收缓冲区 18603 * 4KB \u0026lt;= used \u0026lt;= 24804 * 4KB 时，开始调整接收缓冲区 大于 37206 * 4KB 时，不允许新的 TCP 连接（TCP 可用内存为 0） 注意： 如果要启用自动调整功能，不要在设置 socket 的 SO_SNDBUF 或者 SO_RCVBUF 选项，设置后会关闭自动调整功能\n对于 Server 来说，应该提高 TCP 内存的上限，如果内存紧张，可以降低 buffer 的下限，以允许更多的连接数，提高并发量\n","permalink":"https://blogs.skylee.top/posts/network/tcp/tcp-%E7%9A%84%E4%BC%98%E5%8C%96/note/","tags":["Network","TCP"],"title":"TCP 的优化"},{"categories":["Network"],"content":"什么是半连接队列？什么是全连接队列？ 半连接队列是指：用于存储处于 SYN_RECV 状态的连接的队列\n当内核收到一个 SYN 报文，就将该连接放到半连接队列，并发送 ACK 给对方\n全连接队列是指：用于存储已经建立好 TCP 连接，但还未被应用调用 accept 取走的连接的队列\n当内核收到 SYN 报文对应的 ACK 后，就将该连接从半连接队列中取出，并放到全连接队列，等待应用程序调用 accept 取走\n半连接队列与全连接队列的本质 虽然都有「队列」二字，但二者 实际上都不是队列\n当一个连接请求到来，内核会将连接元数据存到 半连接队列，发送 ack 收到某个连接的 ack 的 ack 后，从半连接队列中取出该连接，发送 ack，并放到 全连接队列 中 服务进程调用 accept，取出一个建立好的 TCP 连接 “收到某个连接的 ack 的 ack 后，从半连接队列中取出该连接”的「取出」过程是一个 随机 的过程，因为这个 ack 的顺序并不固定\n如果半连接队列设计成线性结构，那么取出对应连接就需要 O(n) 的时间\n因此，半连接队列实际上是一个哈希表，取出对应连接的期望时间是 O(1)\n而全连接队列实际上是一个链表，当然，也可以理解成队列（毕竟队列也有基于链表实现的）\n在 Linux 上查看「全连接队列」的大小与容量 可以使用 ss -lnt 查看某个全连接队列的大小：\nroot@SkyLee:~# ss -lnt State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 4096 *:1145 *:* Recv-Q：当前监听的 socket，全连接队列中，没有被取走的 socket 数量（理解成 size） Send-Q：当前监听的 socket，全连接队列 size 的最大值（理解成 capacity） 「全连接队列」溢出，会发生什么？ 当全连接队列满了以后，如果还有新的连接请求到来，内核会 默认直接丢弃 该连接，这个行为可以通过修改 /proc/sys/net/ipv4/tcp_abort_on_overflow 的值来改变：\nroot@SkyLee:~# cat /proc/sys/net/ipv4/tcp_abort_on_overflow 0 0：直接丢弃 1：发送 RST 报文给对方 为啥默认值为 0？直接丢弃，不会占用客户端的资源吗？\n设置为 0，有利于提高请求的成功率\n进入全连接队列，意味着客户端已经进入 ESTABLISHED 状态\n当用户给服务器发请求时，只要服务器不 Response，客户端就会不断重试发送第三次握手的 ACK，只要在重试次数到达上限前，服务器调用了 accept 接收了该连接，就可以处理用户的请求了\n因此，除非认为服务器需要很长时间才来得及 accept（超过了重传的上限时间），不要将 tcp_abort_on_overflow 修改成 1\n「全连接队列」的容量如何修改？ 经过上面的分析，可以发现，如果全连接队列的 capacity：\n过小，可能造成 QPS 上不去 过大，可能造成连接数过多，服务器承受不了 有些时候，在生产环境发现 QPS 上不去，硬件又没吃满，排查不出问题，就可以考虑是不是全连接队列的 capacity 太小了\n那么，这个容量应该怎么修改呢？\n两个配置：\nlisten 中的 backlog /proc/sys/net/core/somaxconn capacity 的值为二者的 最小值\n在 Linux 上查看半连接队列的大小和容量 可以使用 netstat -natp | grep SYN_RECV | grep 1145 | wc -l 来查看 1145 端口上的，处于 SYN_RECV 状态的连接数量，间接的查看半连接队列的大小\nnetstat -natp | grep SYN_RECV | grep 1145 | wc -l 0 目前没有一个命令可以直接查看半连接队列的容量，但是如果想查看的话：\n可以使用工具（如 hping3）对想查看的 server socket 发起 SYN 攻击，把半连接队列打满 再使用上面的命令查看，就可以间接的查看半连接队列的 capacity 「半连接队列」溢出，会发生什么？ 当半连接队列满了以后，如果还有新的连接请求到来，并且 没有启用 syn_cookies，那么，新的连接请求 将会被抛弃\n但是，可以通过启用 syn_cookies，来避免抛弃新的连接请求\n图片来自小林 coding syncookies 参数主要有以下三个值：\n0，表示关闭该功能； 1，表示仅当半连接队列放不下时，再启用它； 2，表示无条件开启功能 应对 SYN 攻击，可以将 syn_cookies 设置为 1\nroot@SkyLee:~# echo 1 \u0026gt; /proc/sys/net/ipv4/tcp_syncookies 开启 syn_cookies，可以实现有无半连接队列，都可以避免新的连接请求被抛弃（syn_cookies = 2），那为什么还需要半连接队列？全部都用 syn_cookies 不就好了？\n有以下原因：\n兼容性：不是所有的系统都支持或默认启用 SYN Cookie。保持半连接队列的机制可以确保在不支持 SYN Cookie 的系统上也能处理 SYN 连接 性能因素：syn_cookies 在正常操作中可能会引起一些额外的计算负担，尤其是在必须验证和重建初始序号时。当流量正常时，使用半连接队列可能会更有效率 资源管理：半连接队列允许服务器有更精确的控制方式来管理半开连接的资源。例如，它可以限制队列长度来防止资源耗尽 因此，半连接队列的存在是有必要的\n最佳实践：在正常操作时使用半连接队列，而当检测到可能的 SYN Flood 攻击时启用 SYN Cookie\n「半连接队列」的容量如何确定？ 首先，是网上经常谈到的 /proc/sys/net/ipv4/tcp_max_syn_backlog 参数\nroot@SkyLee:~# cat /proc/sys/net/ipv4/tcp_max_syn_backlog 1024 但是，半连接队列的容量在不同的 Linux 版本上，有所不同\n下面以内核版本为 2.6.32 的 Linux 为例，说说如何确定半连接队列的容量\n第一个因素就是上面提到的 tcp_max_syn_backlog\n第二个因素是 全连接队列的容量\n是的，全连接队列的容量也会影响半连接队列，具体算法如下：\n如果 tcp_max_syn_backlog \u0026gt; min(somaxconn, listen_backlog)，即全连接队列的大小，那么 max_qlen_log = min(somaxconn, listen_backlog) * 2 否则，max_qlen_log = tcp_max_syn_backlog * 2 简单来说，max_qlen_log 的值为 min(全连接队列容量，tcp_max_syn_backlog) * 2\nmax_qlen_log 的值就是 理论 半连接队列的容量\n但实际上，如果 size \u0026gt; tcp_max_syn_backlog - (tcp_max_syn_backlog \u0026gt;\u0026gt; 2)，连接也会被抛弃\n因此，2.6.32 版本的 Linux 实际的 capacity 的值的计算公式 为：\nmin(tcp_max_syn_backlog - (tcp_max_syn_backlog \u0026gt;\u0026gt; 2), min(tcp_max_syn_backlog, min(somaxconn, listen_backlog)) * 2) 在 2.2 以前的 Linux 中，理论 半连接队列的大小为 listen 中的 backlog 在 5.0 版本的 Linux，理论 半连接队列的大小就是全连接队列的大小\n总结一波：\n不同版本的 Linux，半连接队列的 capacity 的计算方式不同 但半连接队列的 capacity 还是要受到全连接队列大小的影响 为了防止 SYN 攻击，可以启用 syn_cookies ","permalink":"https://blogs.skylee.top/posts/network/tcp/tcp-%E7%9A%84%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97/note/","tags":["Network","TCP"],"title":"TCP 的半连接队列和全连接队列"},{"categories":["Network"],"content":" TCP 基本认识 什么是 TCP ？ TCP 是一种 面向连接、可靠、基于字节流的协议\n什么是 TCP 连接？ Socket + Seq + WindowSize\nSocket：IP + Port Seq：序列号 WindowSize：窗口大小 如何唯一确定一个 TCP 连接 源 IP + 源 Port + 目的 IP + 目的 Port\nTCP 连接数的上限？ 理论上限：32 位 IP + 16 位 Port，即 2^48\n实际远远达不到，取决于：\n最大文件描述符数量（三个等级：系统级、用户级、进程级） 系统内存大小 TCP 与 UDP 区别？使用场景有什么不同？ TCP 有连接，UDP 无连接 TCP 可靠，UDP 不保证可靠（尽最大努力交付，可以在应用层实现可靠，如 QUIC） TCP 一对一，UDP 可以一对一、一对多、多对多 TCP 有流量控制、拥塞窗口等，UDP 没有 TCP 面向字节流，UDP 面向报文 分片机制：TCP 在传输层就可以分片，UDP 仅可能在 IP（网络层）分片 首部开销：TCP 的首部较大，UDP 较小 使用场景：\nTCP 适用于需要保证绝对可靠的场景，如下载、HTTP 等\nUDP 适用于允许一定丢包的场景，如视频通话、直播等\nTCP、UDP 可以用同一个端口吗？ 可以\n\u0026gt; 图片来自小林 coding TCP 连接建立 三次握手过程？ 第三次握手可以携带应用层数据\n为什么是三次？不是两次？四次？ 常见的说法：三次握手可以保证通信双方都具有发送和接收的能力\n更详细的说，分为三点：\n避免历史连接 如果是两次握手，那么 Server 先收到历史的 SYN 报文，就会 立即分配资源，建立 TCP 连接（历史连接）\n然后，Client 收到历史报文的 ACK 后，发现 ack 与预期不符，就会发送 RST 报文给 Server\nServer 收到 RST 报文，终止建立的 TCP 连接\n可以发现，对于 Server 来说，造成了资源浪费，不应该为历史连接分配资源\n而采取三次握手，Server 在第二次握手不会立即分配资源，就避免了资源浪费\n因此，三次握手最主要的原因就是 避免历史连接\n同步双方序列号 序列号在 TCP 中非常重要：\n去除重复数据 实现按序接收 实现重传 采用两次握手，只能同步一方的序列号\n避免资源浪费 与 避免历史连接 的情景是一样的\n此外，为什么不是四次，这是因为，三次握手已经可以同步双方序列号，并且，可以确定信道是「可用」的，四次、五次\u0026hellip; 只不过是增强了可信程度\n为啥每次的初始 seq 不同？ 主要是为了避免「过期」的报文对现有连接的混淆\n图片来自小林 coding 如果每次的初始 seq 不同，就能 很大程度上 避免这个问题（不是完全避免，因为序列号是一个循环，下面会讲）\n初始 seq 的产生算法？ ISN = M + F\nM 是一个计时器，4us + 1 F 是一个哈希算法，取四元组的哈希值 为什么有了 MTU，TCP 还需要 MSS 来分片？ 主要是为了减少重传的数据\n假设 TCP 没有 MSS 来分片，如果一个 TCP 报文过长，在 IP 层也会被分片\n正常情况，没有丢包，那当然没啥问题\n但是，如果丢包，由于 TCP 的重传机制，会重传丢失的报文\n由于 TCP 对 IP 分片（网络层）是无感的，因此，TCP 会 重传整个报文\n如果 TCP 基于 MSS（MTU - IP 首部 - TCP 首部）分片，就可以保证在 IP 层不被分片，即使出现重传，也只需要重传丢失的片段，提高效率\n第一、二、三次握手丢失，分别发生什么？ 这里假设客户端是连接请求发起方\n第一次握手丢失\n客户端会重传 SYN 报文（Linux 默认重传 5 次），且每次的间隔时间指数递增（1、2、4\u0026hellip;）\n超过重传次数，客户端就会终止 TCP 连接\n第二次握手丢失\n对于客户端来说，与第一次握手丢失的情况是一样的\n对于 Server 来说，超时未收到客户端的第三次握手，也会重传 ACK 报文\n图片来自小林 coding 第三次握手丢失\n对于 Server 来说，与第二次握手丢失情况是一样的\n对于 Client 来说，第三次握手请求发出后，就进入了 ESTABLISHED 状态\nClient 若想终止 TCP 连接，就只能依靠 TCP 的保活机制了\nSYN 攻击是什么？如何减少 SYN 攻击带来的影响？ SYN 攻击是指：大量客户端向 Server 发起 TCP 连接请求，收到 Server 的 ACK 以后，始终不回答，导致 Server 的 半连接队列被打满，使 Server 无法处理正常客户端的连接请求\nSYN 攻击也是一种 Dos 攻击\n如何防御 SYN 攻击？\n思路是保证半连接队列不被打满\n防火墙：限制一个 IP 可以请求的客户端的数量 增加半连接队列的大小（调整 net.ipv4.tcp_max_syn_backlog、listen() 函数中的 backlog、net.core.somaxconn） 减少 SYN-ACK 的重传次数以及重传时间，使「半连接」快速释放 启用 SYN-Cookies 启用 SYN-Cookies 后，三次握手的过程：\n第二次握手，即使半连接队列满了，也不会丢弃连接，Server 计算出一个 cookie 值，放到 seq 中，发给 Client 如果第三次握手，Client 的 ack 值合法，就加到 accept 的全连接队列中 图片来自小林 coding TCP 连接断开 四次挥手过程 为什么是四次？可不可以是三次？ 分析上图，可以发现：被动关闭连接方通常需要处理数据，看看有没有数据还要发送，ACK 和 FIN 是分开发的\n因此，需要四次挥手\n但是，在特定情况下，四次是可以变成三次的，即将 ACK 和 FIN 一起发送，但需要一定的条件：\n被动断开方需要开启 ACK 延迟确认机制（默认开启） 被动断开方没有数据要传输 第一、二、三、四次挥手丢失，分别发生什么？ 假设客户端主动终止连接\n第一次挥手丢失\n与 SYN 报文的重传一样，超时未收到 Server 的 ACK 后，客户端会重传 FIN 报文，最终强制关闭\n第二次挥手丢失\n客户端与第一次挥手丢失一样\nServer 的 CLOSE_WAIT 结束后，发送 FIN 报文给 Client，但此时 Client 已经关闭，因此，Server 收不到 ACK，继续重传 FIN 直到重传上限\n第三次挥手丢失\nServer 会一直重传 FIN 报文直到重传上限\n但 Client 就要分情况讨论了\n如果 Client 是调用的 close 关闭的 TCP 连接：\n如果 Client 是调用的 shutdown 关闭的 TCP 连接，由于 tcp_fin_timeout 无法控制 shutdown 关闭的连接，因此，Client 会一直处于 FIN_WAIT2 状态\n第四次挥手丢失\n图片来自小林 coding 为什么要有 TIME_WAIT？ 保证被动关闭方能正确关闭\n假设 Client 为主动关闭方\n当 Client 发出对 Server 的 FIN 报文的 ACK（也就是第四次挥手）后，如果没有 TIME_WAIT，Client 就直接进入 CLOSED 状态\n如果 ACK 丢失，Server 重传 FIN 给 Client，但 Client 已经关闭，因此会返回一个 RST 报文给 Server，导致 Server 的 TCP 连接不正常关闭\n防止历史数据被下一次连接（同样四元组）接收\n个人感觉这个的可能性比较小\n图片来自小林 coding 要出现这种情况，至少满足两个条件的其中之一：\n历史数据传输时间很长很长，以至于初始 Seq 已经跑了一个循环 传输速度很快，Seq 用得很快，以至于用完整个循环的 Seq 第一个情况可以说不可能发生（MSL 的概念）\n第二个情况发生的可能性也很小\n因此，TIME_WAIT 最主要的原因还是保证被动关闭方能正确关闭\n为啥 TIME_WAIT 的时间为 2MSL？ MSL：最大报文生存时间，超过 MSL，可以认为一个彻底在网络中消失\n如果 ACK 丢失了，被动关闭连接的一方会超时重传 FIN 报文\n如果 TIME_WAIT 的时间是 2MSL，可以保证主动关闭连接的一方可以收到重传的 FIN 报文（ACK 过去消耗一个 MSL，重传的 FIN 回来，消耗一个 MSL），然后再次发送 ACK，让对方正确关闭\n也就是说，2MSL 的时间至少允许 ACK 报文丢失一次\n超过 2MSL 的时间也不是不可以，不过丢包率这么高的网络出现的概率太小了，忽略它比解决它更有性价比\nTIME_WAIT 过多会怎么样？ 无论对于客户端还是服务端来说，TIME_WAIT 过多，都会 占用资源\n对于客户端来说，如果将所有可用的端口都用完了（都处于 TIME_WAIT 状态）就无法对相同的「目的 IP + 目的 Port」发起连接了\n如何优化 TIME_WAIT？ TIME_WAIT 被设计出来，就不是用来优化掉的\n相反，应该利用 TIME_WAIT 来保护我们的系统\n服务器出现大量 TIME_WAIT 的原因？ 出现 TIME_WAIT 的本质原因，还是因为 Server 主动关闭了 TCP 连接\n未启用 HTTP 长连接（或服务器禁用 keep-alive）：服务器在发送完资源以后，会主动断开 TCP 连接 客户端禁用 keep-alive 启用了 HTTP 长连接，但是单个连接的请求数太多（nginx 超过 100 次，会断开该连接） 为啥客户端禁用 keep-alive，还是 Server 主动关闭连接呢？\n这个问题要从系统调用来谈\n如果是 Server 主动关闭，只需要一次系统调用（close）\n如果是 Client 主动关闭\nServer 在写完最后一个 response 后，还是要调用 epoll/select 来监听 socket， 当 Client 调用 close 后，Server 这边产生 read 事件，发起 read 系统调用 发现连接被对方关闭，于是调用 close 一共是 3 次系统调用\n因此，考虑到这一点，还是应该让 Server 主动关闭连接\n服务器出现大量 CLOSE_WAIT 的原因？ 这个主要是因为程序的 bug 问题（没有调用 close）\n没有注册 server socket 到 epoll，产生 close 事件，server 也不知道 没有及时 accept 客户端请求，导致大量 Client 主动关闭连接 accept 后，没有注册 clnt_sock 到 epoll 读事件 忘记调用 close 如果已经建立了连接，但是客户端突然出现故障了怎么办？ 分两种情况：\n二者之间有数据传输 二者之间没有数据传输 对于第一种情况，客户端挂了，server 由于收不到 ack，会持续重传直到最大重传次数，关闭连接\n对于第二种情况，就只能依靠 TCP 的 keep-alive 机制了\n默认保活时间是 2h\n超过 2h，双方都没有通信过，就会启动 keep-alive 机制\nkeep-alive 机制会持续向对方发起一个探测报文，直到对方响应，或者达到最大重传次数\n默认间隔时间：75s 默认最大重传次数：9 可以发现，TCP 的保活机制需要很长的时间才能断开一个「死亡」连接\n因此，一般需要应用层的协议手动实现 heart-beat 机制\nSocket 编程 针对 TCP，Server 的流程 初始化 socket bind listen accept read/write close listen 的 backlog 的意义？ 在 早期 Linux 内核 backlog 是 SYN 队列的容量，也就是「半连接队列」容量。\n但在 Linux 内核 2.2 之后，backlog 变成 accept 队列\n也就是说，对于 2.2 之后的 Linux，backlog 的大小，决定了「全连接队列」的容量\n但实际上，全连接队列的容量还取决于内核的 somaxconn 参数\n因此，全连接队列实际容量是 min(somaxconn, backlog)\naccept 发生在三次握手的哪一步？ 发生在 server 收到 client 第三次握手时，即三次握手成功后\n没有 accept，可以建立 TCP 连接吗？ 可以\naccept 发生在三次握手结束后，在 accept 前，TCP 连接就已经建立好了\n没有 listen，可以建立 TCP 连接吗？ 可以\n存在两种情况：\nTCP 自连接（自己连接自己） 双方同时向对方发起连接 两种情况的共同点：没有服务端的参与，也就是没有 listen\n","permalink":"https://blogs.skylee.top/posts/network/tcp/tcp-%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B-%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/note/","tags":["Network","TCP"],"title":"TCP 三次握手/四次挥手"},{"categories":["Network"],"content":" 重传机制 超时重传 发送完一个 TCP 报文后，发送方会启动重传计时器\n若在超时时间内，都没有收到 ack，就会触发超时重传，以保证可靠传输\n重传时间 RTO 的确定很重要：\n如果过小，会造成不必要的重传，浪费网络资源 如果过大，会造成报文整体传输时间过长，增加延迟 那该如何确定 RTO 的值？\n一般来说，RTO 设置为 RTT 再加上一小段时间比较合适\n这里的 RTT 是加权平均往返时间\n此外，如果重传后，仍未收到 ack，那么下一次重传的时间是上一次的 2 倍\n快重传 快重传是指：不用等待重传计时器超时，只要收到三次重复的 ack，就重传\n图片来自小林 coding 那么问题来了，重传哪些报文呢？\n假设发送了报文 1 ～ 6，其中，2、3 丢失了，发送方收到三个重复的 ack = 2\n如果重传所有的报文，显得没必要，浪费资源，因为对方已经收到部分报文了 如果仅重传 2，那么报文 3 又要等到收到三个 ack 才能重传，效率低 SACK SACK 就是用来解决上面的问题的\n图片来自小林 coding SACK 包含了已经收到的报文的序列号，这样，发送方就可以只重传丢失的报文了\n滑动窗口 为什么有滑动窗口？ 假设没有滑动窗口，使用 TCP 发送一条数据后，必须要等待对方发来 ack 以后，才能发送下一条数据，效率很低\n如果引入「窗口」的概念，发送方发送的字节数小于窗口大小时，无需等待对方发来 ack，而是继续发送，直到未确认的字节数大小等于发送窗口大小，大大提高效率\n此外，窗口内发送的多条数据，不一定每一个都要收到 ack（即允许部分 ack 丢失，或者压根就不确认），只要收到最后一个报文的 ack，就可以一并确认之前的报文，这种机制叫做 累计确认，进一步提高了效率\n窗口大小由谁确定？ 窗口大小通常由 接收方 确定\n这是因为，接收方需要处理发送方发来的数据，发送方不应该发送太快，以至于接收方来不及处理\n因此，接收方可以根据处理能力，动态调整窗口的大小，控制发送方的发送速率\n发送窗口 接收窗口 二者大小是否相等？ 二者的大小可以说是大约相等\n接收方动态更新自己的接收窗口大小，并通告给发送方，这个过程存在延迟\n流量控制 基于滑动窗口的流量控制过程 OS 对滑动窗口的控制 真实的流量控制过程不会像上面一样那么简单（上面的接收窗口大小始终没有变化）\n接收方可能比较忙，虽然接收了数据，但处理数据的能力比较弱\n在这种情况下，buffer 中会堆积较多的数据，为了防止将 buffer 打满，OS 会动态调整接收窗口的大小\n图片来自小林 coding 上面的情况比较极端，最后的接收窗口大小甚至减为 0！\n当应用将堆积的数据取走以后，就可以恢复窗口的大小，向发送方通告此时的接收窗口大小，让发送方可以继续发送数据\n那么，如果通告消息丢失了呢？\n接收方等待发送方发送数据，而发送方也在等待接收方通告窗口大小，造成死锁？！\n零窗口探测报文 事实上，上面的「死锁」是不会发生的\n当发送方收到的接收方的窗口大小为 0 时，会启动持续计时器\n当持续计时器超时后，发送方就会向接收方发出 零窗口探测报文\n如果窗口大小确实还为 0，重置计时器 如果窗口大小不为 0，重置发送窗口大小，并继续发送数据 如果超时未收到响应，触发重传 窗口的探测次数一般为 3，探测超过 3 次，窗口还为 0（或者没有响应），有的 TCP 实现就会发出 RST 报文，终止连接\n糊涂窗口综合征 糊涂窗口综合征是指：接收方的窗口太小（或设置不当），导致发送方每次发送的包很小，进而提高发送次数，以及 ack 次数，造成的网络拥塞现象\n分析一波，问题主要出现在：\n接收方的窗口太小 发送方每次发送的数据太少 解决第一个问题，接收方的通常处理策略为：\n当接收窗口大小小于 min(MSS, cache_size / 2) 时，发送零窗口报文给对方，让对方不再发送数据 等待应用逐渐取走数据，直到接收窗口大小 \u0026gt;= min(MSS, cache_size / 2)，发送当前的窗口大小 解决第二个问题，发送方的通常处理策略为：\n使用 Nagle 算法\n当窗口大小 \u0026gt;= MSS，并且待发送数据大小 \u0026gt;= MSS 收到一个 ack 只要满足其中一个条件，就可以发送数据\n可以发现，使用 Nagle 算法，接收方必须保证不通告小窗口给发送方，否则满足第二个条件，还是可以发送小包，造成糊涂窗口综合征\n此外，由于 Nagle 算法默认启用，对于传递的数据本身就比较小的场景（如 ssh），需要禁用 Nagle 算法，以保证低延迟\n拥塞控制 什么是拥塞窗口？ 拥塞窗口 cwnd 的大小为 拥塞窗口和接收窗口二者的最小值，由发送方维护\n前面提到的流量控制，控制的是单个 TCP 连接的流量，但对整个网络而言，是无感的\nTCP 是一个「无私」的协议，当它检测到网络出现了拥塞，就会减缓自己连接的速率，避免整个网络的拥塞程度加深\n如何判断是否出现拥塞？ 当重传计时器超时，就可以认为，网络出现了拥塞\n拥塞控制全过程 整体由四个算法构成：\n慢开始 拥塞避免 快重传 快恢复 首先执行慢开始，cwnd 指数增加，直到到达 ssthresh（门限值） 到达 ssthresh 后，开始拥塞避免，cwnd 线性增加 如果发生超时重传，我们认为发生拥塞，重传后，cwnd 设为初始值，ssthresh 设置为 ssthresh / 2，重新执行慢开始 如果收到三个重复 ack（认为丢包），快速重传，执行快恢复（ssthresh 设置为 ssthresh / 2，cwnd 设置为 当前的 ssthresh），然后执行拥塞避免 详细说说快恢复 快恢复开始于收到 3 个重复的 ack 时\n第一种实现，就像上一张图展示的一样，快重传后，ssthresh 设置为 ssthresh / 2，cwnd 设置为 当前的 ssthresh\n第二种实现有所不同：\n图片来自小林 coding sthresh 设置为 ssthresh / 2，执行快恢复，cwnd 设置为 当前的 ssthresh + 3\n如果再收到相同的 ack，cwnd + 1 否则，将 cwnd 设置为当前的 sthresh，开始拥塞避免 为什么快恢复将 cwnd 设置为 当前的 ssthresh + 3？\n加 3 代表快速重传时已经确认接收到了 3 个重复的数据包；\n为什么快恢复过程中还要增加 cwnd？\n过程 2（cwnd 逐渐加 1）的存在是为了尽快将丢失的数据包发给目标，从而解决拥塞的根本问题（三次相同的 ACK 导致的快速重传），所以这一过程中 cwnd 反而是逐渐增大的。\n最终的 cwnd 还是减小的（减为之前的 sthresh 的一半）\n为什么有了流量控制还需要拥塞控制？ 前面提到的流量控制，控制的是 单个 TCP 连接的流量，但对整个网络而言，是无感的\nTCP 是一个「无私」的协议，当它检测到网络出现了拥塞，就会减缓自己连接的速率，避免整个网络的拥塞程度加深\n如果没有拥塞控制，并且发生了拥塞，那么整个网络的可用性下降，丢包率上升，需要更多的重传，更多的重传又会进一步加深拥塞程度，是一个 恶性循环\n","permalink":"https://blogs.skylee.top/posts/network/tcp/tcp-%E9%87%8D%E4%BC%A0%E6%9C%BA%E5%88%B6%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%8B%A5%E5%A1%9E%E7%AA%97%E5%8F%A3%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/note/","tags":["Network","TCP"],"title":"TCP 重传机制、滑动窗口、流量控制、拥塞控制"},{"categories":["Network"],"content":" HTTP 是啥 HTTP 是一种在 两个设备 间，用于传输文字、图片、音频等 超文本内容 的协议\n常见状态码 200 304 403 404 500 502 常见字段 Host Content-Length Content-Type Content-Encoding GET 与 POST 都是安全和幂等的吗？ 一般来说，GET 是安全和幂等的，按照规范，GET 请求仅仅是请求 Server 的资源，不会修改 Server 的数据\n而 POST 不是安全和幂等的，按照规范每一次的 POST 请求，都有可能修改 Server 的资源\n但是，这都不是绝对的，可以不遵守规范，发送 GET 请求，也可以修改 Server 的资源\nHTTP Cache 实现方式 HTTP 的缓存有两种实现方式：\n强制缓存 协商缓存 ==强制缓存==\n强制缓存 是利用响应报文中的：\nCache-Control Expires 来启用的，且 Cache-Control 优先级更高（因为更加控制精细）\n当浏览器判断缓存的数据没有过期，就 不会 向 Server 请求这个资源，而是直接使用缓存\n因此，强制缓存的 决定权是在浏览器\n==协商缓存==\n协商缓存 是利用响应报文中的：\nETag Last-Modified 来启用的\n当浏览器判断缓存过期后，如果过期的响应报文包含 ETag 或者 Last-Modified 字段，就会向 Server 发起请求，由 Server 判断资源是否过期\n如果没有过期，Server 返回 304，客户端可以继续使用「过期」的缓存 否则，返回 200，Body 包含请求的数据 图解如下：\nHTTP/1.1 特性 优点 简单 灵活 易扩展 跨平台 缺点 明文传输，不安全 无状态（双刃剑），需要状态信息，需要带上 Cookie 或者 Session 头 性能 相较于 HTTP/1，1.1 版本支持了长连接，可以复用 TCP 连接，不用每次请求都建立一次 TCP 连接，效率更高\n但是，1.1 版本存在 队头阻塞 的问题\n队头阻塞是指：虽然 1.1 版本可以复用 TCP 连接，但是如果单个请求的响应很慢，那么后面的请求也会被阻塞（因为是 串行 请求的）\nHTTPS 与 HTTP 的区别？ HTTP 是明文传输，而 HTTPS 是密文传输\n如何实现加密？ HTTPS 采取 混合加密 的方式保证数据的安全传输\n两种加密方式：\n对称加密：双方密钥一致 非对称加密：公钥 + 私钥 建立 HTTPS 的大致步骤 在 TLS 握手时，采取的是非对称加密 在数据传输时，采取的是对称加密 由于 HTTPS 采取了两种方式，因此叫做混合加密\n非对称加密 私钥加密，公钥解密，保证数据不被冒充 公钥加密，私钥解密，保证数据不被篡改 但是，公钥也可能被伪造，因此，还需要有效 证书\n客户端可以判断证书是否可信，如果可信，可以在证书中找到 Server 的公钥\nTLS（基于 RSA）握手的详细步骤 握手后，后续通信都使用 Session Key 进行对称加密了\nECDHE 算法 前面提到的基于 RSA 算法的 TLS，仍存在安全性问题\n因为 Server 的 私钥是一直不变 的，如果泄漏，那之前捕获的所有数据都可以被破解\nECDHE 算法就是来解决这个问题的\n核心思想：双方的私钥都是随机生成的\n双方事先确定好使用哪种椭圆曲线，和曲线上的基点 G，这两个参数都是公开的； Client 生成一个私钥，记为 p0，与基点相乘，得到公钥 q0 Server 生成一个私钥，记为 p1，与基点相乘，得到公钥 q1 双方交换公钥 由于椭圆满足交换律（x1y2 = x2y1），因此，双方可以得到相同的 Session Key（p0q1、p1q0） 由于每次的私钥都不同，因此 ECDHE 安全性更高，即使私钥被破解（很困难），也只会影响单次通信的安全性\nTLS（基于 ECDHE）握手详细步骤 客户端生成一个随机数 A，发送自己支持的加密方式 服务器确认加密方式，并生成一个随机数 B，一并发送给 Client 服务器发送证书 服务器生成一个私钥 P0，并发送选择的椭圆曲线、计算出的公钥 Q0 客户端验证证书 客户端生成一个私钥 P1，并将公钥发送给 Server 客户端计算 SessionKey（A + B + P1Q0） 服务器计算 SessionKey（A + B + P0Q1） 为了更安全，双方没有使用 P1Q0（ECDHE 计算出的共享密钥）作为会话密钥，而是再加上两个随机数\nHTTPS 是否绝对安全？ 场景是这样的：用户连上了一个「假基站」，将用户的请求转发到中间人服务器上，由中间人服务器转发用户请求给服务器\n图片来自小林 coding 对于用户来说，用户对中间人服务器是无感的 对中间人服务器来说，它就相当于原来的客户端 因此中间人服务器当然可以解析出用户的请求信息，也可以解析服务器的响应信息\n但前提是：用户信任了中间人服务器的 CA 证书！\n一般来说，即使是连上了中间人的服务器，也会因为请求的 URL 与中间人服务器提供证书中的信息不一致而导致请求被 block 掉\n所以，目前来说，HTTPS 是可以保证安全的，只要用户不作死\nHTTP/1.1、HTTP/2、HTTP/3 的演变 HTTP/2 做了什么优化？ 头部压缩 二进制格式 支持并发连接 服务器推送 注意：\n虽然 HTTP/2 支持并发连接，但是，仍然存在「队头阻塞」的问题\n假设浏览器使用 HTTP/2 协议发出了若干个并发请求\n由于 TCP 协议的重传机制，当队头的响应报文的某一段丢失了，会等待服务器重传以后，才会将剩余的数据返回给应用\n图片来自小林 coding HTTP/3 做了什么优化？ 为了解决队头阻塞的问题，以及避免建立冗杂的 TCP 连接和 TLS 握手，HTTP/3 使用了基于 UDP 的 QUIC 协议\n不同于 TCP 的 IP + Port 的形式，QUIC 使用连接 ID 区分不同的连接\nQUIC 的优点：\n无队头阻塞 连接速度快 支持连接迁移（如 WLAN 切换到 5G，不需要重新建立连接） HTTP/1.1 如何优化？ 核心是减少 HTTP 请求数\n建立缓存（前面提到过） 使用代理服务器处理重定向，减少 HTTP 请求数 懒加载 将多个小的资源合并成一个大的资源 压缩 HTTPS 如何优化？ 硬件优化 软件优化：升级 Linux 版本，openssl 版本 协议优化： 使用 ECDHE 算法，而不是 RSA（ECDHE 支持「抢跑」三次握手结束后，客户端就可以发送消息 ） 升级 TLS 版本到 1.3，1 RTT 即可握手 证书优化： 使用 ECDSA 证书，因为相同安全性下，密钥更短 缓存证书响应结果，避免频繁访问 CA 会话复用：如果 HTTPS 要重连，可以复用上一次的 SessionKey 常见复用技术：SessionID、Session Ticket 但无法保证前向安全，有「重放攻击」风险，需要设置 SessionKey 的合理过期时间\n既然有 HTTP，为啥还要 RPC？ 事实上，RPC 在 80 年代就有了，而 HTTP 是在 90 年代才流行起来的\n所以，更准确的来说，应该是：“既然有了 RPC，为啥还要 HTTP？”\n面向对象不同\nHTTP 基于 C/S 架构，RPC 基于 B/S 架构\n从使用目的来说，HTTP 的目的是 提供一个通用的协议 ，大家都可以用，通用型强\n而 RPC，严格来说，不是一种协议，目的是 希望程序员可以想调用本地方法一样，去调用远端方法\n所以，一般对外，使用 HTTP 协议，而内部的集群微服务间通信，使用 RPC\n性能因素\n相较于 HTTP/1.1，RPC 的性能会更好，RPC 的实现，不需要包含 HTTP 那么长的首部信息，对象序列化的时候，也可以不使用 json，而是 protobuf，更精简\n而服务之间的相互调用，就符合 RPC 的场景，不需要这么多首部信息，因此，内部的集群微服务间通信，使用 RPC\n其他\n事实上，既然有 HTTP，为啥还要 RPC？ 这个问题本身就不准确，HTTP 与 RPC 本身就可以是同时存在的，而不是互斥的，有各自的使用场景。甚至，gRPC 底层实现就是基于 HTTP/2.0 的\nWebSocket 假设要实现一个 二维码登录 功能，如果使用 HTTP/1.1，怎么做？\n作为前端，是不知道用户是否已经扫码，也不知道扫码结果\n方法一：定时轮询\n客户端每隔 1 ～ 2s 向服务器请求数据，如果服务器响应登录成功或失败，停止轮询\n这种方式对于用户来说，可能会有一定延迟，如果客户端刚刚发出查询请求用户就扫码，会感知到明显延迟\n方法二：长轮询\n一般来说，客户端发起 HTTP 请求后，如果 Server 超过 3s 没应答，认为超时\n但长轮询不同，每次 HTTP 请求会持续较场时间（如 30s），在请求时间内，如果用户扫码成功，服务器可以立即响应给客户端，用户体验比较好\n但无论是定时轮询还是长轮询，本质都是向服务器「拉」数据，都不太优雅\n如果服务器可以在用户扫码后，立即向客户端 推送 数据，客户端就不用发出多个 HTTP 请求，用户体验也会好很多\nWebSocket 就是来解决 HTTP/1.1 的这个问题的\nWebSocket 是一种基于 TCP 的 全双工 协议，多用于 Server 主动向客户端推送数据\n如何建立？ 要想建立 WebSocket 连接，需要用到 HTTP，并带上一个特殊的 header\nConnection: Upgrade Upgrade: WebSocket Sec-WebSocket-Key: T2a6wZlAwhgQNqruZ2YUyg==\\r\\n 含义：浏览器想升级协议，协议为 WebSocket，并带上一段 base64 编码的 string\n如果服务器正好支持升级成 WebSocket 协议。就会走 WebSocket 握手流程，同时根据客户端生成的 base64 码，用某个公开的算法变成另一段字符串，放在 HTTP 响应的 Sec-WebSocket-Accept 头里，同时带上 101 状态码，发回给浏览器。HTTP 的响应如下：\nHTTP/1.1 101 Switching Protocols\\r\\n Sec-WebSocket-Accept: iBJKv/ALIW2DobfoA4dmr3JHBCY=\\r\\n Upgrade: WebSocket\\r\\n Connection: Upgrade\\r\\n 图片来自小林 coding 升级成 WebSocket 协议后，后续的通信与 HTTP 就没啥关系了\n使用场景 WebSocket 适用于 服务器主动推送 数据的 实时通讯场景，如网页游戏\n既然 http/2.0 支持服务器推送，那为啥还要用 websocket 协议？ 需要弄清楚一点：HTTP/2 的服务器推送场景是：\n当客户端请求一个页面（index.html）后，服务器可以预测接下来客户端要请求的数据（如 js、css 文件），并主动将这些数据推送给客户端\n而 WebSocket 适用于需要频繁、实时交互的场景，例如聊天室、实时股票信息显示、在线游戏等。HTTP/2 的服务器推送虽然能够预先推送资源，但并不适用于实时性强的应用。\n","permalink":"https://blogs.skylee.top/posts/network/http/note/","tags":["Network","http"],"title":"HTTP 协议基础"},{"categories":["Network"],"content":"以 “键入 URL 到网页显示，发生了什么？” 来跑一遍整个 TCP/IP 体系\nLinux 接收数据的过程 发送数据的过程类似，但是要注意发送数据的拷贝次数\n发送网络数据的时候，涉及几次内存拷贝操作？\n第一次，调用发送数据的系统调用的时候，内核会申请一个内核态的 sk_buff 内存，将用户待发送的数据拷贝到 sk_buff 内存，并将其加入到发送缓冲区。\n第二次，在使用 TCP 传输协议的情况下，从传输层进入网络层的时候，每一个 sk_buff 都会被 克隆一个新的副本出来 。副本 sk_buff 会被送往网络层，等它发送完的时候就会释放掉，然后原始的 sk_buff 还保留在传输层，目的是为了实现 TCP 的可靠传输 ，等收到这个数据包的 ACK 时，才会释放原始的 sk_buff 。\n第三次，当 IP 层发现 sk_buff 大于 MTU 时才需要进行。会再申请额外的 sk_buff，并将原来的 sk_buff 拷贝为多个小的 sk_buff。\n因此，应用在发送数据的时候，应该控制单个 packet 的长度 + TCP 首部 + IP 首部 不超过 MTU，以避免第三次拷贝（TCP 有 MSS 限制，在 TCP 这一层次就会分片，而不是在网络层分片，减少重传的数据量）\n","permalink":"https://blogs.skylee.top/posts/network/%E5%9F%BA%E7%A1%80%E7%AF%87/note/","tags":["Network"],"title":"网络基础"},{"categories":["Linux"],"content":"Linux 线程概念 在 Linux 系统中，进程实际上是由线程来实现的，没有独立的、与线程完全分离的进程实体。实际上，进程可以被视为线程的容器，线程是进程的执行单元。\n在传统的操作系统中，进程被认为是独立的实体，具有独立的地址空间、文件描述符、资源等。而在 Linux 中，进程与线程之间没有明显的差异，线程之间共享同一进程的资源，包括内存空间、文件描述符、信号处理等。每个进程至少有一个线程，即主线程，其他线程都是主线程的衍生。\nLinux 采用了 轻量级进程（LWP，Lightweight Process）模型，将线程作为进程的基本执行单元 。通过在进程内创建和管理线程，实现并发执行和资源共享。\n因此，在 Linux 中，进程只是线程的一个容器，用于提供资源隔离和管理的框架 。每个进程都有一个唯一的进程标识符（PID），用于标识该进程及其线程组。线程之间的切换和调度是通过操作系统内核来完成的 ，进程和线程之间没有明确的边界。\n内核级线程与用户级线程 内核级线程：内核级线程是由操作系统内核直接支持和管理的线程。在这种模型中，线程的创建、调度和管理都由操作系统内核 完成。每个内核级线程都有独立的内核栈和内核上下文，由内核进行调度和切换。在内核级线程模型中，多个线程可以并发执行在多个处理器核心上，充分利用多核资源。\n用户级线程：用户级线程是在用户空间实现的线程。用户级线程的 创建、调度和管理由用户态的线程库或运行时库完成，而操作系统对这些线程是无感知的 。在 单个内核级线程 上运行的若干个用户级线程，由用户态的调度算法决定线程的切换和执行，不需要操作系统内核的介入，所以线程的切换开销较小。\n接下来的内容讲的都是内核级线程\n线程的优点 线程的创建和销毁成本较低 线程的切换成本低 线程占用的资源少 对于 计算密集型 的程序而言，使用线程可以将计算部分分配到多个线程执行，提高效率 对于 IO 密集型 的程序而言，使用线程可以将 IO 操作重叠，等待多种 IO 操作 线程的缺点 潜在的性能损失：如果开辟的线程数量大于了计算机可用的处理器数量，那么会产生额外的同步和调度开销 健壮性降低：编写多线程程序时，往往会因为数据的不恰当访问而导致数据不一致问题，线程之间是缺乏保护的 缺少访问控制：进程是访问控制的基本粒度，在一个线程中调用某些OS函数会对整个进程造成影响 编写难度高：调试一个多线程程序往往比单线程程序困难 线程异常 对于一个多线程程序，如果某一个线程出现异常，崩溃，那么对应的进程也会崩溃，其它线程也会跟着退出\n例如：\nvoid *thread0(void *arg) { int loops = 3; while (loops--) { printf(\u0026#34;Thread0 is running...\\n\u0026#34;); sleep(1); } raise(SIGABRT); // 线程 1 抛出 SIGABRT return NULL; } void *thread1(void *arg) { while (1) // 死循环 { printf(\u0026#34;Thread1 is running...\\n\u0026#34;); sleep(1); } } int main(void) { pthread_t tid0, tid1; pthread_create(\u0026amp;tid1, NULL, thread1, NULL); // 创建线程 1 pthread_create(\u0026amp;tid0, NULL, thread0, NULL); // 创建线程 0 sleep(10); // 等待 thread 0 抛出 SIGABRT printf(\u0026#34;Main thread is quitting...\\n\u0026#34;); return 0; } 输出：\n可以看到，当 thread0 抛出异常后，thread1 和 主线程 都退出了\n线程与进程 在一开始介绍线程概念的时候，已经讲到了线程与进程的关系，这里再总结一下：\n进程是资源分配的基本单位，线程是调度的基本单位\n线程共享进程的资源，包括 内存空间、文件描述符、每种 信号处理的方式 等\n线程也有自己独享的资源：\n线程 ID 一组寄存器（用于保存上下文数据和调度） 栈区（线程的临时数据） errno 屏蔽信号集 调度优先级 根据「进程是线程的容器」这句话理解二者的关系可能更加容易\n进程与线程的本质区别？\n是否共享内存空间，是进程与线程的本质区别，Linux 并不区分进程与线程，线程对于内核来说，不过是一个共享特定资源的进程而已\nLinux 线程控制 创建线程 可以使用 pthread_create 来创建一个线程\n原型如下：\n#include \u0026lt;pthread.h\u0026gt; int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine)(void *), void *arg); 参数说明：\nthread：指向 pthread_t 类型的 指针，用于存储新创建线程的标识符。 attr：指向 pthread_attr_t 类型的指针，用于指定新线程的属性。可以传入 NULL 使用默认属性。 start_routine：指向线程函数的指针，该函数在新线程中运行。函数的返回值和参数类型都必须是 void * 类型。 arg：传递给线程函数的参数。 返回值：\n创建成功返回 0 创建失败返回一个错误码 pthread_create 函数调用成功后，将创建一个新的线程，并将其标识符存储在 thread 参数指向的位置。新线程将立即开始执行 start_routine 指向的函数，并将 arg 作为参数传递给该函数。\n新线程在创建后独立运行，与创建它的线程（通常是主线程）并发执行 。\n注意： pthread_create 函数仅仅是创建了一个新线程，并 不会等待新线程的结束 。\n如何等待？下文有介绍\n线程 ID 前面提到，可以通过 pthread_create 的第一个参数获取创建的子线程的 ID\n此外，在一个线程内，也可以通过 pthread_self 来获取自己的线程 ID\n例如：\nvoid *thread0(void *arg) { printf(\u0026#34;[%d]Thread0: Thread0\u0026#39;s tid: %ld\\n\u0026#34;, getpid(), pthread_self()); sleep(30); return NULL; } int main(void) { pthread_t tid0; if(pthread_create(\u0026amp;tid0, NULL, thread0, NULL) != 0) { perror(\u0026#34;Create thread error!\\n\u0026#34;); return -1; } sleep(30); printf(\u0026#34;[%d]Main thread: Main thread\u0026#39;s tid: %ld\\n\u0026#34;, getpid(), pthread_self()); printf(\u0026#34;[%d]Main thread: Thread0\u0026#39;s tid: %ld\\n\u0026#34;, getpid(), tid0); return 0; } 输出：\n可以看到：\n两种方式获取的线程 ID 相同 主线程与 thread0 的线程 ID 不同 但这种方式获取的是「用户级」的线程 ID，而不是「内核级」的线程 ID\n如何获取内核级线程 ID 呢？使用 ps -aL\n为了观察内核级线程 ID，我们让程序在后台运行，并输入终端指令：\nps -aL|head -1 \u0026amp;\u0026amp; ps -aL|grep test|grep -v grep 观察到如下结果：\n其中，LWP 就是所谓的「内核级」线程 ID\n实际上，「内核级」线程 ID 就是「轻量级线程号」，OS 调度时，是根据 LWP 来调度，分配资源是根据 PID 来分配\n线程终止 在 Linux 中，一共有三种方式使一个线程终止：\n从线程函数（就是传给 pthread_create 的函数）中 return 调用 pthread_exit 一个线程可以调用 pthread_cancel 来终止另一个线程 pthread_exit pthread_exit 是一个线程库函数，用于在线程中显式地退出线程。它允许线程提前结束执行，并返回一个指定的退出状态值。\n函数原型如下：\nvoid pthread_exit(void *retval); 参数 retval 是一个指向线程退出状态的指针。线程退出状态可以是任何指针类型的值，用于传递线程结束时的信息。\n当调用 pthread_exit 函数时，当前线程会立即终止，并将 retval 的值作为线程的退出状态。这个退出状态可以由其他线程使用 pthread_join 函数获取（后文会讲）。\npthread_cancel pthread_cancel 是一个线程库函数，用于向目标线程发送取消请求，以「请求」目标线程终止执行。\n函数原型如下：\nint pthread_cancel(pthread_t thread); 参数 thread 是目标线程的线程标识符\n当调用 pthread_cancel 函数时，它会发送一个取消请求给目标线程。目标线程可以通过以下几种方式响应取消请求：\n线程忽略取消请求：线程可以选择忽略取消请求，不做任何响应。 线程立即取消：线程可以立即取消，即终止当前的线程执行。 线程延迟取消：线程可以在取消点（cancellation point）处取消，即等待到达一个预定的取消点后再终止执行。取消点是指线程执行的某个特定位置，例如线程调用了阻塞的 I/O 操作或者阻塞的系统调用时。 注意： 取消请求是异步的， 目标线程不一定会立即响应取消请求 。另外，要使 pthread_cancel 函数生效，目标线程必须设置为可取消状态，可以通过 pthread_setcancelstate 函数进行设置。\n线程等待 为什么要线程等待？\n如果一个线程退出，它的空间不会释放，而是继续留在进程空间内 线程退出后，如果再创建一个新的线程，不会复用之前退出的线程的地址空间 pthread_join pthread_join 是一个线程库函数，用于等待指定的线程终止，并获取其返回值。\n函数原型如下：\nint pthread_join(pthread_t thread, void **retval); 参数 thread 是要等待的目标线程的线程标识符\n参数 retval 是一个指向指针的指针，用于接收目标线程的返回值。目标线程的返回值通过 pthread_exit 函数传递。\n当调用 pthread_join 函数时， 它会阻塞当前线程 ，直到目标线程终止。如果目标线程已经终止，那么 pthread_join 函数会立即返回。如果目标线程尚未终止，当前线程将被阻塞，直到目标线程终止为止。\n一旦目标线程终止，它的返回值将被存储在 retval 指向的内存位置中。如果不关心目标线程的返回值，可以将 retval 参数设置为 NULL。\n分离线程 在默认情况下，一个新创建的线程是可以 join 的，创建该线程的线程需要进行 join 操作，否则无法回收相关资源\n然而，根据前文提到的，当调用 pthread_join 函数时， 它会阻塞当前线程 ，直到目标线程终止。因此，join 操作是有代价的\n如果不关心线程的返回值，join 就成了一种负担，有没有什么方法避免 join 带来的负担，同时可以回收线程的资源呢？\npthread_detach pthread_detach 是一个线程库函数，用于将一个线程标记为“分离状态”，以指示 线程终止后自动释放其资源。\n函数原型如下：\nint pthread_detach(pthread_t thread); 参数 thread 是要分离的目标线程的线程标识符\n调用 pthread_detach 函数将目标线程标记为「分离状态」，这意味着一旦目标线程终止，它的资源将被自动释放，而不需要通过调用 pthread_join 来等待目标线程的终止和获取返回值。\n注意： 分离状态的线程无法被等待。也就是说，pthread_join 和 pthread_detach 是互斥的。因此，一旦线程被标记为「分离状态」，就不能再调用 pthread_join 函数等待它的终止。\n例如，如果我们同时对一个线程进行等待和分离操作：\nvoid* thread_run(void *arg) { printf(\u0026#34;%s\\n\u0026#34;, (char*)(arg)); pthread_detach(pthread_self()); // 线程自己分离 return NULL; } int main(void) { pthread_t tid; if((pthread_create(\u0026amp;tid, NULL, thread_run, \u0026#34;Thread is running...\u0026#34;)) != 0) { perror(\u0026#34;Create thread error!\\n\u0026#34;); return 1; } sleep(1); // 等待线程分离 if(pthread_join(tid, NULL) == 0) // 一个线程不能既是 joinable 又是分离的，所以 join 会失败 printf(\u0026#34;Join success!\\n\u0026#34;); else printf(\u0026#34;Join failed!\\n\u0026#34;); return 0; } 输出：\n可以看到，加入是失败了的\n实例 // return void *thread1(void *arg) { printf(\u0026#34;Thread1 is about returning...\\n\u0026#34;); int *state = (int *)malloc(sizeof(int)); *state = 1; return (void *)state; } // exit void *thread2(void *arg) { printf(\u0026#34;Thread2 is about exitting...\\n\u0026#34;); int *state = (int *)malloc(sizeof(int)); *state = 2; pthread_exit((void *)state); // 传递的必须是堆区或者全局开辟的 } // cancel void *thread3(void *arg) { while (1) { printf(\u0026#34;Thread3 is running...\\n\u0026#34;); sleep(1); } return NULL; } // detach int flag = 0; // 标记 thread 4 的退出状态 void* thread4(void *arg) { sleep(3); // 给 main thread 时间来分离 int loops = 3; while (loops--) { printf(\u0026#34;Thread4 is running...\\n\u0026#34;); sleep(1); } printf(\u0026#34;Thread4 is about returning...\\n\u0026#34;); flag = 1; return NULL; } int main(void) { pthread_t tid; void *state; // 线程返回的状态 // 创建 Thread 1 pthread_create(\u0026amp;tid, NULL, thread1, NULL); pthread_join(tid, \u0026amp;state); printf(\u0026#34;Thread1[%X] return value: %d\\n\u0026#34;, tid, *(int *)state); free(state); // 创建 Thread 2 pthread_create(\u0026amp;tid, NULL, thread2, NULL); pthread_join(tid, \u0026amp;state); printf(\u0026#34;Thread2[%X] exit code: %d\\n\u0026#34;, tid, *(int *)state); free(state); // 创建 Thread 3 pthread_create(\u0026amp;tid, NULL, thread3, NULL); sleep(3); printf(\u0026#34;Main thread is canceling thread3[%X]...\\n\u0026#34;, tid); pthread_cancel(tid); pthread_join(tid, \u0026amp;state); if (state == PTHREAD_CANCELED) printf(\u0026#34;Thread3[%X] was calceled.\\n\u0026#34;, tid); else printf(\u0026#34;Thread3[%X] return NULL\\n\u0026#34;, tid); // 创建 Thread 4 pthread_create(\u0026amp;tid, NULL, thread4, NULL); pthread_detach(\u0026amp;tid); // 快速分离线程 4，防止线程 4 已经退出，而主线程还没分离线程 4，导致资源无法回收 printf(\u0026#34;Main thread is detaching thread4[%X] ...\\n\u0026#34;, tid); while (!flag) { printf(\u0026#34;Main thread is running...\\n\u0026#34;); sleep(1); } printf(\u0026#34;Thread 4 has returned, main thread is about exitting...\\n\u0026#34;); return 0; } 输出：\nLinux 线程互斥 相关概念 临界资源：多个线程共享的资源 临界区：访问临界资源的代码 互斥：保证任意时刻有且仅有一个线程访问临界区 原子性：不会被任何调度机制打断的操作 有问题的售票机 先来看一个例子：\nint remainTickets = 66; void* sell(void *arg) { const char* curThread = (const char*)(arg); while (1) { if(remainTickets \u0026gt; 0) // 可以卖一张票 { usleep(1000); // 阻塞 1000 us，模拟较长的计算过程 --remainTickets; printf(\u0026#34;%s: remainTickets: %d\\n\u0026#34;, curThread, remainTickets); } else break; } return NULL; } int main(void) { pthread_t tid0, tid1, tid2, tid3; pthread_create(\u0026amp;tid0, NULL, sell, \u0026#34;Thread 0\u0026#34;); pthread_create(\u0026amp;tid1, NULL, sell, \u0026#34;Thread 1\u0026#34;); pthread_create(\u0026amp;tid2, NULL, sell, \u0026#34;Thread 2\u0026#34;); pthread_create(\u0026amp;tid3, NULL, sell, \u0026#34;Thread 3\u0026#34;); pthread_join(tid0, NULL); pthread_join(tid1, NULL); pthread_join(tid2, NULL); pthread_join(tid3, NULL); return 0; } 某一次运行时的输出：\nSky_Lee@SkyLeeMBP Test % ./cfile Thread 1: remainTickets: 65 ... Thread 1: remainTickets: 1 Thread 0: remainTickets: -1 Thread 3: remainTickets: -2 Thread 2: remainTickets: 0 Thread 1: remainTickets: -3 神奇的是： remainTickets \u0026lt;= 0 时，线程似乎仍然对 remainTickets 进行了减 1 操作\n为什么？\n原因就在于没有对临界资源 remainTickets 进行保护\n例如，此时 remainTickets == 1 对于某个线程（假设为线程1）：\n判断 if 条件为 true，向下一步执行 此时发生了调度，切换到另一个线程（假设为线程2） 由于线程 1 没有来得及对 remainTickets 进行减 1 操作，因此，if 条件仍然成立 线程 2 对 remainTickets 进行了减 1 操作，此时，remainTickets == 0 调度，切换到线程 1，继续对 remainTickets 进行减 1 操作，此时，remainTickets == -1 因此，要想避免这种情况，就需要对临界资源保护\n那么该如何保护呢？\n互斥量（mutex） 在Linux中，互斥量（mutex）是一种用于实现线程同步的机制，用于保护共享资源的访问。它可以确保在任意给定时间只有一个线程可以访问受互斥量保护的代码段或共享资源，以避免并发访问导致的数据竞争和不一致性。\n互斥量的主要特性如下：\n锁定和解锁：互斥量提供了两个主要操作： 锁定（Lock）和解锁（Unlock） 。线程在访问受互斥量保护的代码段或共享资源之前需要获取锁，以确保独占访问。当访问完成后，线程需要释放锁，以允许其他线程获取锁。 互斥性：互斥量保证在任意给定时间只有一个线程可以获取到锁。如果一个线程已经持有了互斥锁，其他线程尝试获取锁时将被阻塞，直到该线程释放锁。 有关互斥量的接口 在Linux中，互斥量的实现可以使用pthread_mutex_t数据类型和相关的函数。常用的互斥量操作函数包括：\npthread_mutex_init：初始化互斥量。 pthread_mutex_destroy：销毁互斥量。 pthread_mutex_lock：获取互斥量的锁定。 pthread_mutex_trylock：尝试获取互斥量的锁定，如果获取失败则立即返回。 pthread_mutex_unlock：释放互斥量的锁定。 pthread_mutex_init 函数原型：\nint pthread_mutex_init(pthread_mutex_t* mutex, const pthread_mutexattr_t* attr); 参数：\nmutex：一个指向 pthread_mutex_t 类型的指针，用于指定要初始化的互斥量。 attr：一个指向 pthread_mutexattr_t 类型的指针，用于指定互斥量的属性。可以将其设置为 NULL，表示使用默认属性。 pthread_mutex_init 函数在调用时会为互斥量分配内存，并将其初始化为可用状态。\n返回值：\n如果函数调用成功，返回值为 0，表示初始化互斥锁成功。 如果函数调用失败，返回值为非零的错误码，表示初始化互斥锁失败。 注意： 创建互斥量时，一定要用 pthread_mutex_init 初始化\npthread_mutex_lock 函数原型：\nint pthread_mutex_lock(pthread_mutex_t *mutex); 参数：\nmutex：指向互斥量对象的指针。互斥量用于提供线程之间的互斥访问。 返回值：\n成功：返回0。 失败：返回错误代码。 注意： 可以在不同线程中多次调用pthread_mutex_lock，但要记得每次调用都要对应一个pthread_mutex_unlock。\npthread_mutex_unlock 函数原型：\nint pthread_mutex_unlock(pthread_mutex_t *mutex); 参数：\nmutex：指向互斥量对象的指针。互斥量用于提供线程之间的互斥访问。 返回值：\n成功：返回0。 失败：返回错误代码。 功能：pthread_mutex_unlock函数用于释放互斥量的锁。它允许其他线程获取该互斥量的锁。\n调用pthread_mutex_unlock函数会将互斥量标记为可用状态，允许其他线程获取它。如果当前没有其他线程正在等待获取该互斥量的锁，则互斥量将变为可用状态，否则将允许一个等待的线程获取锁。\n注意：\npthread_mutex_unlock函数 应该在持有互斥量锁的线程中调用 ，否则行为是未定义的。 解锁一个未加锁的互斥量，或 解锁其他线程拥有的互斥量是不正确的，并且可能导致未定义的行为。 pthread_mutex_destroy 函数原型：\nint pthread_mutex_destroy(pthread_mutex_t *mutex); 参数：\nmutex：指向互斥量对象的指针。互斥量是需要销毁的对象。 返回值：\n成功：返回0。 失败：返回错误代码。 功能：pthread_mutex_destroy函数用于销毁互斥量对象。在不再需要使用互斥量时，应该调用该函数来释放相关资源。\n调用pthread_mutex_destroy函数会销毁指定的互斥量对象，并释放相关的资源。在调用该函数之后，对互斥量对象的操作是未定义的。\n注意：\n在调用pthread_mutex_destroy之前，确保没有任何线程正在使用互斥量，否则会导致未定义的行为。 销毁一个已经被其他线程锁定的互斥量是不正确的，并且可能导致未定义的行为。 不能对销毁后的互斥量进行上锁，解锁等操作 改进后的售票机 int remainTickets = 100; pthread_mutex_t mutex; // 互斥量 void* sell(void *arg) { const char* curThread = (const char*)(arg); while (1) { pthread_mutex_lock(\u0026amp;mutex); if(remainTickets \u0026gt; 0) // 可以卖一张票 { // pthread_mutex_lock(\u0026amp;mutex); // 不能放在这，有可能 if 判断为 True 时发生调度 usleep(1000); // 阻塞 1000 us，模拟较长的计算操作 --remainTickets; printf(\u0026#34;%s: remainTickets: %d\\n\u0026#34;, curThread, remainTickets); pthread_mutex_unlock(\u0026amp;mutex); // 确保一个 lock 对应一个 unlock } else { pthread_mutex_unlock(\u0026amp;mutex); // 确保一个 lock 对应一个 unlock break; } } return NULL; } int main(void) { pthread_mutex_init(\u0026amp;mutex, NULL); pthread_t tid0, tid1, tid2, tid3; pthread_create(\u0026amp;tid0, NULL, sell, \u0026#34;Thread 0\u0026#34;); pthread_create(\u0026amp;tid1, NULL, sell, \u0026#34;Thread 1\u0026#34;); pthread_create(\u0026amp;tid2, NULL, sell, \u0026#34;Thread 2\u0026#34;); pthread_create(\u0026amp;tid3, NULL, sell, \u0026#34;Thread 3\u0026#34;); pthread_join(tid0, NULL); pthread_join(tid1, NULL); pthread_join(tid2, NULL); pthread_join(tid3, NULL); pthread_mutex_destroy(\u0026amp;mutex); return 0; } 互斥锁实现原理 pthread_mutex_lock 和 pthread_mutex_unlock 对应了下面的伪代码 lock 和 unlock\nbool locked = false; // 初始化为未锁定 lock() { while(1) // 防止伪唤醒 { if(!locked) { locked = true; return ; } else 挂起等待; } } unlock() { locked = false; 唤醒等待锁的进程; return ; } 可以发现，上锁时，先判断锁是否可用，如果不可用，会将线程 挂起等待 ，让出时间片，直到锁可用\nlock 和 unlock 操作必须是原子操作，不允许中断，因为如果不是原子操作，允许发生调度，可能会导致数据不一致问题，使锁的状态混乱\n由于互斥锁没有忙等待，因此适用于 持锁时间长 的操作\n自旋锁 自旋锁（spin lock）与互斥锁一样，用于保护共享资源的并发访问。但与互斥锁不同，自旋锁 不会使线程进入睡眠状态，而是 通过不断忙等（自旋）的方式来尝试获取锁 。当一个线程发现自旋锁已经被其他线程占用时，它会一直在一个循环中自旋，直到锁被释放。\n特点：\n自旋锁的加锁和解锁过程是原子的，不会被其他线程打断。 自旋锁适用于多核处理器，因为在自旋等待期间，线程会占用一个 CPU 核心进行自旋操作。 自旋锁的使用步骤与互斥锁大致相同：\n初始化自旋锁。 在需要保护的共享资源访问之前，使用自旋锁的加锁操作尝试获取锁。 如果获取锁成功，则进入临界区，访问共享资源。 访问完成后，使用自旋锁的解锁操作释放锁。 实现原理 每把自旋锁都有一个 bool 类型的变量 available，用于标记这把锁是否可用\npthread_spin_lock 和 pthread_spin_unlock 对应了下面的 acquire 和 release：\nbool available; acquire() { while(!available); // 循环检查锁的状态，忙等 available = false; } release() { available = true; } acquire 和 release 操作必须是原子操作，不允许中断，因为如果不是原子操作，允许发生调度，可能会导致 available 的数据不一致问题，使锁的状态混乱\n自旋锁的最大缺点就是忙等待，当有一个线程在临界区中，任何其他线程在进入临界区时必须调用 acquire，自旋等待锁，直到可用。\n但如果线程对临界区的访问很快，那么它将很快释放锁，忙等待的代价就很小，甚至 小于线程切换的代价\n因此，自旋锁适用于：处理器资源不紧张以及持锁时间非常短的情况\n可重入与线程安全 概念 重入：同一个函数被不同的执行流调用，当前一个流程还没有执行完，就有其他的执行流再次进入\n可重入（Reentrancy），也称为可重入性或递归性，是指一个函数或代码段在被多个线程或进程同时调用时能够正确地执行而不会出现不一致或意外的结果。\n线程安全（Thread Safety）是指在多线程环境下，能够保证共享资源或数据结构在并发访问时的正确性和一致性。具体来说，线程安全的代码能够在 多个线程同时访问 共享资源时，正确地完成所需的操作，而不会产生不可预期的结果或导致数据损坏\n常见线程不安全情况 不保护共享变量的函数 返回静态变量指针的函数 调用线程不安全的函数 常见线程安全情况 对共享变量只读，不修改的函数 类的接口是原子操作 多个线程之间任意切换，不会产生二义性的函数 常见不可重入情况 使用了 malloc/free 的函数，因为 malloc/free 使用全局链表管理堆 使用了标准库 IO 操作的函数 常见可重入情况 使用局部变量，不使用全局变量、静态变量 不使用 malloc/free 不使用标准 IO 操作 不调用 不可重入函数 线程安全与可重入的联系与区别 可重入函数一定线程安全 线程安全的函数不一定可重入 怎么理解？\n对于可重入函数，其保证了多个线程同时调用时，不会出现二义性，这肯定是线程安全的\n可重入函数在同一时间只会影响局部变量或者线程私有数据，不会修改全局变量或者共享数据。\n而线程安全的函数是可以修改全局变量或者共享数据的，只要对临界资源进行保护即可（例如使用互斥锁），因此，线程安全的函数不一定可重入\n死锁 死锁是指在一组进程中的各个进程均占有不会释放的资源，但因 互相申请 被其他进程所占用不会释放的资源而处于的一种 永久等待 状态。\n例如：\npthread_mutex_t mutex0, mutex1; void* thread0(void *arg) { printf(\u0026#34;Now thread0 is running...\\n\u0026#34;); pthread_mutex_lock(\u0026amp;mutex0); // 获取互斥锁 0 printf(\u0026#34;Thread0 has got mutex0.\\n\u0026#34;); sleep(3); // 模拟耗时操作 pthread_mutex_lock(\u0026amp;mutex1); // 获取互斥锁 1 printf(\u0026#34;Thread0 has got mutex1.\\n\u0026#34;); pthread_mutex_unlock(\u0026amp;mutex1); pthread_mutex_unlock(\u0026amp;mutex0); return NULL; } void* thread1(void *arg) { printf(\u0026#34;Now thread1 is running...\\n\u0026#34;); pthread_mutex_lock(\u0026amp;mutex1); // 获取互斥锁 1 printf(\u0026#34;Thread1 has got mutex1.\\n\u0026#34;); sleep(3); // 模拟耗时操作 pthread_mutex_lock(\u0026amp;mutex0); // 获取互斥锁 0 printf(\u0026#34;Thread1 has got mutex0.\\n\u0026#34;); pthread_mutex_unlock(\u0026amp;mutex0); pthread_mutex_unlock(\u0026amp;mutex1); return NULL; } int main(void) { pthread_t tid0, tid1; pthread_mutex_init(\u0026amp;mutex0, NULL); pthread_mutex_init(\u0026amp;mutex1, NULL); pthread_create(\u0026amp;tid0, NULL, thread0, NULL); usleep(1000); pthread_create(\u0026amp;tid1, NULL, thread1, NULL); pthread_join(tid0, NULL); pthread_join(tid1, NULL); pthread_mutex_destroy(\u0026amp;mutex0); pthread_mutex_destroy(\u0026amp;mutex1); printf(\u0026#34;Main thread is about quitting...\\n\u0026#34;); } 为了更好的观察死锁现象，我们再新建一个终端用于计时\n可以发现，预估运行时间为 6 s，但即使程序运行了 36 s，它也没有退出\n这就是死锁现象\n来分析为什么会发生死锁\n假设线程调度如下：\n线程 1 尝试获取互斥锁 1，成功（因为一开始没有线程对 互斥锁 1 上锁） 此时发生调度，切换到线程 2 线程 2 尝试获取互斥锁 2，成功（因为此时没有线程对 互斥锁 2 上锁） 此时，死锁已经产生\n如果此时线程 2 继续运行，尝试获取锁 1，失败（因为线程 1 已经对锁 1 上锁），线程阻塞，切换到线程 1 线程 1 运行，尝试获取锁 2，失败（因为线程 2 已经对锁 2 上锁），线程阻塞，切换到线程 2 线程 2 再次尝试获取锁 1，失败 \u0026hellip;\u0026hellip; 引用一张图帮助理解： 死锁产生的四个必要条件 经过上面的分析，我们可以总结死锁产生的四个必要条件：\n互斥条件：争抢必须互斥使用的资源才会导致死锁 不剥夺条件：资源只能由线程释放，不能强行剥夺 请求和保持条件：在保持某种互斥资源不放的同时，申请另一个互斥资源 循环等待条件：存在一个互斥资源的循环等待链 避免死锁的方法 要想避免死锁的产生，就必须对资源进行合理地分配\n破坏死锁的四个必要条件 加锁顺序一致 资源一次性分配（会导致资源利用率降低，以及饥饿问题） 避免锁未释放 此外，还可以通过使用避免死锁的算法，如死锁检测算法，银行家算法等来避免死锁带来的问题\nLinux 线程同步 概念 使用同步机制来协调线程之间的执行顺序和访问共享资源的方式，以避免竞态条件和数据不一致性。常用的线程同步机制包括互斥锁、条件变量、信号量和屏障等。\n为什么要有线程同步 线程同步保证了各个线程对临界资源的有序访问，如果没有线程同步，可能产生以下问题：\n竞态条件 数据不一致 死锁 线程饥饿 竞态条件（Race Condition）是指在多线程或多进程环境下，多个线程或进程对共享资源的访问产生的不确定性和不可预测性结果。竞态条件会导致程序的执行结果与预期不符，可能引发各种问题，包括数据损坏、死锁、无限循环等。\n条件变量机制 条件变量（Condition Variable）用于线程之间的等待和通知机制。它允许一个或多个线程在满足特定条件之前等待，而不是忙等待。当条件满足时，其他线程可以通过发送信号来通知等待的线程继续执行。\n在 C 语言中，pthread_cond_t 是用于线程间条件变量的类型，它是 POSIX 线程库中的一部分。条件变量用于线程间的等待和通知机制，允许线程在特定条件下等待，并在条件满足时被通知继续执行。\n相关接口 pthread_cond_init 一个条件变量在使用前必须初始化\npthread_cond_init 函数用于初始化条件变量，创建一个可用的条件变量对象。初始化后的条件变量可用于线程同步和线程间的等待/通知操作。\n函数原型：\nint pthread_cond_init(pthread_cond_t *cond, const pthread_condattr_t *attr); 参数：\ncond：指向条件变量的指针，用于初始化。 attr：指向条件变量属性的指针，通常设为NULL以使用默认属性。 返回值：\n成功：0 失败：错误码 pthread_cond_wait pthread_cond_wait 函数用于在条件变量上等待。\n函数原型：\nint pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex); 参数：\ncond：指向条件变量的指针，用于等待和唤醒线程。 mutex：指向互斥锁的指针，用于保护共享资源的访问。 返回值：\n成功时，返回0。 失败时，返回一个非零的错误码，表示出现了错误。 注意：\n在调用该函数之前，线程必须先获取互斥锁，然后将互斥锁传递给 pthread_cond_wait 在等待条件变量时，线程会自动释放互斥锁，并在 被唤醒后重新获取互斥锁。 在等待条件变量时，线程可能会出现 虚假唤醒 的情况，因此需要 在循环中检查条件是否满足 ，以防止出现竞态条件。 线程在等待条件变量时可能会被信号中断，需要根据需要处理信号中断的情况。 pthread_cond_signal pthread_cond_signal 函数用于向等待在条件变量上的一个线程发送信号，唤醒其中一个线程继续执行。它通常与 pthread_cond_wait 配合使用，用于实现线程间的同步。\n函数原型：\nint pthread_cond_signal(pthread_cond_t *cond); 参数、返回值与 pthread_cond_wait 一致，这里不再赘述\npthread_cond_boardcast 与 pthread_cond_signal 不同的是，发送广播通知，pthread_cond_boardcast 会唤醒所有等待在条件变量上的线程。\n函数原型：\nint pthread_cond_broadcast(pthread_cond_t *cond); 参数、返回值与 pthread_cond_wait 一致，这里不再赘述\npthread_cond_destroy pthread_cond_destroy 函数用于销毁条件变量。在 使用完条件变量后，应该调用该函数进行清理工作，释放相关的资源。\n函数原型：\nint pthread_cond_destroy(pthread_cond_t *cond); 参数、返回值与 pthread_cond_wait 一致，这里不再赘述\n注意：\n调用 pthread_cond_destroy 之前，应该 确保没有线程在等待条件变量上。否则，销毁条件变量将导致未定义的行为。 销毁条件变量后，不能再使用已销毁的条件变量。 实例 pthread_mutex_t mutex; pthread_cond_t cond; int flag = 0; char *shareResorces = NULL; // 共享资源 void *thread0(void *arg) { printf(\u0026#34;Now thread0 is running...\\n\u0026#34;); pthread_mutex_lock(\u0026amp;mutex); // 先上锁 while (!flag) // 当条件不满足 { // 循环等待其它线程唤醒（循环目的？防止假唤醒，因为 wait 可能失败提前返回） // 会释放锁 pthread_cond_wait(\u0026amp;cond, \u0026amp;mutex); } printf(\u0026#34;Thread0: Access shareResorces: %s\\n\u0026#34;, shareResorces); free(shareResorces); pthread_mutex_unlock(\u0026amp;mutex); printf(\u0026#34;Now thread0 is about quitting...\\n\u0026#34;); return NULL; } void *thread1(void *arg) { printf(\u0026#34;Now thread1 is running...\\n\u0026#34;); pthread_mutex_lock(\u0026amp;mutex); shareResorces = (char *)malloc(sizeof(char) * 100); strcpy(shareResorces, \u0026#34;Hello, pthread_cond_t!\u0026#34;); flag = 1; pthread_cond_signal(\u0026amp;cond); // 唤醒线程 0 pthread_mutex_unlock(\u0026amp;mutex); // 解锁 printf(\u0026#34;Now thread1 is about quitting...\\n\u0026#34;); return NULL; } // 同步：先执行线程 1，再执行线程 0 int main(void) { pthread_mutex_init(\u0026amp;mutex, NULL); pthread_cond_init(\u0026amp;cond, NULL); pthread_t tid0, tid1; pthread_create(\u0026amp;tid0, NULL, thread0, NULL); sleep(1); pthread_create(\u0026amp;tid1, NULL, thread1, NULL); pthread_join(tid0, NULL); pthread_join(tid1, NULL); pthread_cond_destroy(\u0026amp;cond); pthread_mutex_destroy(\u0026amp;mutex); printf(\u0026#34;Now main thread is about quitting...\\n\u0026#34;); } 输出：\n可以发现，即使让线程 0 先执行 1s，线程 0 也必须等到线程 1 写入了资源后才能访问到资源，避免了空指针异常，这体现了线程同步的必要性\n为什么 wait 操作需要用到互斥量 mutex 在前面的介绍中，可以发现 pthread_cond_wait 函数参数含有互斥量 mutex，这是为什么？\n第一个原因：\n在等待的过程中，共享数据可能发生变化，这就需要互斥锁来保护临界资源，有了互斥锁，就能保证在等待唤醒的过程中，对临界资源的访问是互斥的\n第二个原因：\n确保原子性。\n例如，如果 pthread_cond_wait 函数参数 不 含有互斥量 mutex：\n// 错误的设计 pthread_mutex_lock(\u0026amp;mutex); while (condition_is_false) { pthread_mutex_unlock(\u0026amp;mutex); // 此处如果发生调度，想想会发生什么？ pthread_cond_wait(\u0026amp;cond); pthread_mutex_lock(\u0026amp;mutex); } pthread_mutex_unlock(\u0026amp;mutex); 如果这样写，在 解锁之后，等待之前 ，条件可能已经满足，信号已经发出，但是该信号可能就被 pthread_cond_wait(\u0026amp;cond) 错过了，导致该进程永远等待唤醒\n生产者消费者问题 生产者消费者问题是一种经典的互斥、同步问题，涉及到多个生产者和消费者线程对共享资源的访问和操作。\n生产者线程负责生产产品，并将其放入一个 有限大小的缓冲区 中，而消费者线程负责从缓冲区中取出产品进行消费。生产者和消费者之间通过共享缓冲区进行通信。\n生产者和消费者之间需要协调合作，避免出现以下问题：\n缓冲区溢出：当缓冲区已满时，生产者必须等待，直到有空闲位置。 缓冲区为空：当缓冲区为空时，消费者必须等待，直到有可用产品。 为了解决生产者消费者问题，需要用到同步机制，下面是以条件变量机制解决的生产者消费者问题的代码：\n// BlockQueue.h #pragma once #include \u0026lt;iostream\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; class BlockQueue { std::queue\u0026lt;int\u0026gt; data; size_t bufferSize; // 阻塞队列缓冲区大小 pthread_mutex_t mutex; // 互斥锁 pthread_cond_t _full; // 队列满 pthread_cond_t _empty; // 队列空 bool full(void) const; bool empty(void) const; public: BlockQueue(size_t bufferSize = 6); ~BlockQueue(); void push(int val); // 和 JAVA 的 pop 类似，返回队列首元素 int pop(void); }; // BlockQueue.cpp #include \u0026#34;BlockQueue.h\u0026#34; inline bool BlockQueue::full(void) const { return data.size() == bufferSize; } inline bool BlockQueue::empty(void) const { return data.empty(); } BlockQueue::BlockQueue(size_t bufferSize) : bufferSize(bufferSize) { pthread_mutex_init(\u0026amp;mutex, NULL); pthread_cond_init(\u0026amp;_full, NULL); pthread_cond_init(\u0026amp;_empty, NULL); } BlockQueue::~BlockQueue() { pthread_mutex_destroy(\u0026amp;mutex); pthread_cond_destroy(\u0026amp;_full); pthread_cond_destroy(\u0026amp;_empty); } void BlockQueue::push(int val) { pthread_mutex_lock(\u0026amp;mutex); std::cout \u0026lt;\u0026lt; \u0026#34;Check if the BlockQueue is full\u0026#34; \u0026lt;\u0026lt; std::endl; while (full()) { std::cout \u0026lt;\u0026lt; \u0026#34;BlockQueue is full! Waiting for consumer...\u0026#34; \u0026lt;\u0026lt; std::endl; pthread_cond_wait(\u0026amp;_full, \u0026amp;mutex); // 阻塞，等待消费者消费产品 } data.push(val); std::cout \u0026lt;\u0026lt; \u0026#34;Push success! The size of BlockQueue is \u0026#34; \u0026lt;\u0026lt; data.size() \u0026lt;\u0026lt; std::endl; pthread_mutex_unlock(\u0026amp;mutex); pthread_cond_signal(\u0026amp;_empty); // 通知可能正在等待 _empty 的消费者 } int BlockQueue::pop(void) { pthread_mutex_lock(\u0026amp;mutex); std::cout \u0026lt;\u0026lt; \u0026#34;Check if the BlockQueue is empty\u0026#34; \u0026lt;\u0026lt; std::endl; while (empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;BlockQueue is empty! Waiting for producer...\u0026#34; \u0026lt;\u0026lt; std::endl; pthread_cond_wait(\u0026amp;_empty, \u0026amp;mutex); // 阻塞，等待生产者生产产品 } int ret = data.front(); data.pop(); std::cout \u0026lt;\u0026lt; \u0026#34;Pop success! The size of BlockQueue is \u0026#34; \u0026lt;\u0026lt; data.size() \u0026lt;\u0026lt; std::endl; pthread_mutex_unlock(\u0026amp;mutex); pthread_cond_signal(\u0026amp;_full); // 通知可能正在等待 _full 的生产者 return ret; } // main.cpp #include \u0026#34;BlockQueue.h\u0026#34; #include \u0026lt;random\u0026gt; void *thread0(void *arg); void *thread1(void *arg); int main(void) { BlockQueue blockQueue; pthread_t tid0, tid1; pthread_create(\u0026amp;tid0, NULL, thread0, \u0026amp;blockQueue); sleep(10); // 让队列满了再开始线程 1 pthread_create(\u0026amp;tid1, NULL, thread1, \u0026amp;blockQueue); pthread_join(tid0, NULL); pthread_join(tid1, NULL); } // 生产者线程 void *thread0(void *arg) { std::uniform_int_distribution\u0026lt;int\u0026gt; u(0, 9); std::default_random_engine e(time(NULL)); BlockQueue *blockQueue = static_cast\u0026lt;BlockQueue*\u0026gt;(arg); for (int loop = 0; loop \u0026lt; 60; ++loop) { blockQueue-\u0026gt;push(u(e)); sleep(1); } return NULL; } // 消费者线程 void *thread1(void *arg) { BlockQueue *blockQueue = static_cast\u0026lt;BlockQueue*\u0026gt;(arg); // 记得用指针，否则线程 0、线程 1 的阻塞队列都不同！ for (int loop = 0; loop \u0026lt; 60; ++loop) { int val = blockQueue-\u0026gt;pop(); std::cout \u0026lt;\u0026lt; \u0026#34;Thread1: received data from BlockQueue: \u0026#34; \u0026lt;\u0026lt; val \u0026lt;\u0026lt; std::endl; sleep(1); } return NULL; } 某一次运行时输出：\n注意：\n在使用条件变量时需要初始化，销毁 循环等待其它线程唤醒，防止假唤醒 在线程内操作队列时，为了保证两个线程操作相同的阻塞队列，需要使用指针，避免拷贝 POSIX 信号量 POSIX 信号量是一种用于进程间同步和互斥的机制。它是 POSIX 标准中定义的一组函数和数据类型，用于实现并发编程中的线程同步操作。\nPOSIX 信号量的特点和使用方法包括：\n信号量用于实现资源的同步访问和互斥操作，通过控制对共享资源的访问来避免竞态条件和数据不一致性问题。 信号量可以通过 P（wait）和 V（post）操作来进行加锁和解锁操作，以控制对资源的访问。 等待 P 操作的线程或进程会被阻塞，直到信号量的计数器满足条件，才能继续执行。 执行 V 操作会增加信号量的计数器，并唤醒等待的线程或进程继续执行。 信号量的计数器可以表示资源的数量，也可以表示互斥锁的状态。 POSIX 信号量提供了一组函数，如 sem_init、sem_wait、sem_post 和 sem_destroy 等，用于初始化、操作和销毁信号量。 常用接口 要想使用 POSIX 信号量的接口，需要包含头文件 semaphore.h\nsem_init sem_init函数用于初始化一个POSIX信号量。它创建一个新的信号量，并设置初始值。\n原型：\nint sem_init(sem_t *sem, int pshared, unsigned int value) 参数:\nsem: 指向要初始化的信号量的指针 pshared: 指定信号量的共享方式，取值为0或非零。 如果pshared为0，则信号量被 线程内部 使用，只能在同一进程内的线程间共享。 如果pshared非零，则信号量可以在多个 进程间 共享 value: 指定信号量的初始值，必须是非负数。 返回值:\n成功时返回0，表示信号量初始化成功。 失败时返回-1，表示初始化失败，具体错误信息保存在errno中。 注意：\n在调用sem_init之前，应确保sem指针是有效的，且未被初始化。 对于线程内部使用的信号量，通常将pshared参数设置为0。 信号量的初始值通过value参数指定，它 表示资源的初始数量 。 sem_wait（P 操作） sem_wait函数用于获取（等待）一个信号量。如果信号量的值大于0，表示有可用资源，sem_wait会将信号量的值减1，并立即返回。如果信号量的值为0，表示没有可用资源，sem_wait将阻塞线程，直到有资源可用时才返回。\n原型：\nint sem_wait(sem_t *sem) 参数:\nsem: 指向要操作的信号量的指针 返回值:\n成功时返回0，表示成功获取到信号量。 失败时返回-1，表示出现错误，具体错误信息保存在errno中。 注意： sem_wait是一个阻塞操作，如果没有可用资源，它会一直等待，直到有资源可用或出现错误。\nsem_post（V 操作） sem_post函数用于释放一个信号量。它会将信号量的值加1，表示释放了一个资源。如果有其他线程正在等待该信号量，则其中一个线程将 被唤醒 ，可以获取到该信号量，并继续执行。\nsem_post 与 sem_wait 的用法类似，这里不再赘述\nsem_destroy sem_destroy函数用于销毁一个信号量。它会释放信号量相关的资源，并将信号量重置为未初始化的状态。\n原型：\nint sem_destroy(sem_t *sem) 参数:\nsem: 指向要销毁的信号量的指针 返回值:\n成功时返回0，表示成功销毁信号量。 失败时返回-1，表示出现错误，具体错误信息保存在errno中。 生产者消费者模型再探 再理一下生产者与消费者关系：\n有空闲缓冲区 -\u0026gt; 生产者可以生产产品 有非空闲缓冲区 -\u0026gt; 消费者可以消费产品 // CircularQueue.h #pragma once #include \u0026lt;vector\u0026gt; #include \u0026lt;semaphore.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; class CircularQueue { std::vector\u0026lt;int\u0026gt; data; size_t bufferSize; size_t size; sem_t _full; // 记录当前缓冲区存放的数据个数 sem_t _empty; // 记录当前缓冲区空闲个数 pthread_mutex_t mutex; int front = 0; int rear = 0; public: CircularQueue(int bufferSize = 6); ~CircularQueue(); void push(int val); int pop(void); }; // CircularQueue.cpp #include \u0026lt;cstdio\u0026gt; #include \u0026#34;CircularQueue.h\u0026#34; CircularQueue::CircularQueue(int bufferSize) :bufferSize(bufferSize) { pthread_mutex_init(\u0026amp;mutex, NULL); sem_init(\u0026amp;_empty, 0, bufferSize); // 将 _empty 信号量的数量设置为 bufferSize sem_init(\u0026amp;_full, 0, 0); data.resize(bufferSize); } CircularQueue::~CircularQueue() { sem_destroy(\u0026amp;_empty); sem_destroy(\u0026amp;_full); pthread_mutex_destroy(\u0026amp;mutex); } void CircularQueue::push(int val) { sem_wait(\u0026amp;_empty); // P(_empty) printf(\u0026#34;CircularQueue: pushed data %d.\\n\u0026#34;, val); fflush(stdout); data[rear] = val; rear = (rear + 1) % bufferSize; sem_post(\u0026amp;_full); // V(_full) } int CircularQueue::pop(void) { sem_wait(\u0026amp;_full); // P(_full) int ret = data[front]; front = (front + 1) % bufferSize; sem_post(\u0026amp;_empty); // V(_empty) printf(\u0026#34;CircularQueue: poped data %d.\\n\u0026#34;, ret); return ret; } // main.cpp #include \u0026#34;CircularQueue.h\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;random\u0026gt; #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; // 生产者线程 void *thread0(void *arg) { std::uniform_int_distribution\u0026lt;int\u0026gt; u(0, 9); std::default_random_engine e(time(NULL)); CircularQueue *circularQueue = static_cast\u0026lt;CircularQueue*\u0026gt;(arg); for (int loop = 0; loop \u0026lt; 60; ++loop) { int val = u(e); circularQueue-\u0026gt;push(val); sleep(1); } return NULL; } // 消费者线程 void *thread1(void *arg) { CircularQueue *circularQueue = static_cast\u0026lt;CircularQueue*\u0026gt;(arg); for (int loop = 0; loop \u0026lt; 60; ++loop) { int val = circularQueue-\u0026gt;pop(); sleep(1); } return NULL; } int main(void) { CircularQueue circularQueue; pthread_t tid0, tid1; pthread_create(\u0026amp;tid0, NULL, thread0, \u0026amp;circularQueue); sleep(10); // 让队列满了再开始线程 1 pthread_create(\u0026amp;tid1, NULL, thread1, \u0026amp;circularQueue); pthread_join(tid0, NULL); pthread_join(tid1, NULL); } 输出：\n读者写者问题 有读者和写者两组并发进程，共享一个文件，当两个或两个以上的读进程同时访问共享数据时不会产生副作用，但若某个写进程和其他进程（读进程或写进程）同时访问共享数据时则可能导致数据不一致的错误。因此要求：①允许多个读者可以同时对文件执行读操作；②只允许一个写者往文件中写信息;③任一写者在完成写操作之前不允许其他读者或写者工作;④写者执行写操作前，应让己有的读者和写者全部退出。\n概括一下：\n允许多个读进程同时读数据 只允许一个写进程写数据 在写数据的过程中，不允许其它任何进程访问数据 semaphore rw = 1; // 信号量，用于实现文件的互斥访问 int count = 0; // 记录读文件进程的数量，用于实现多个进程“同时”读数据 writer() { while(1) { P(rw); // 上锁 写文件; V(rw); // 解锁 } } reader() { while(1) { if(count == 0) // 如果自己是第一个读文件的进程 1 P(rw); // 上锁 2 ++count; 读文件; --count; if(count == 0) // 如果自己是最后一个退出的进程 V(rw); // 解锁 } } 上面的代码看起来没什么毛病嗷，但是，如果在 if(count == 0) 这句话发生了进程调度，切换到另一个读进程，即：\n第一个读进程执行到 1，进程调度，切换到第二个读进程 第二个读进程执行到 1，进程调度，切换到第一个读进程 第一个读进程上锁 第二个读进程无法上锁，因为已经上锁，被阻塞 也就是说，此时并没有实现两个读进程同时访问文件的要求\n如何解决？\n很容易想到，出现上面的问题，根本原因是：\nif(count == 0) // 如果自己是第一个读文件的进程 1 P(rw); // 上锁 2 ++count; 这三句话不是原子操作，导致第二个进程以为 count == 0，进而发生阻塞\n因此，再添加一个互斥信号量，以实现 “类原子操作”，即：\nsemaphore mutex = 1; reader() { ... P(mutex); if(count == 0) // 如果自己是第一个读文件的进程 1 P(rw); // 上锁 2 ++count; V(mutex); 读文件; P(mutex); --count; if(count == 0) // 如果自己是最后一个退出的进程 V(rw); // 解锁 V(mutex); ... } 无论是否修改，这个代码存在潜在的写进程饥饿问题，如果有源源不断的读进程，那么写进程将持续被阻塞（只有最后一个退出的读进程才能解锁 rw）\n因此这种方式又叫读优先\n要想实现写优先，需要再添加一个信号量：\nsemaphore w = 1; writer() { while(1) { P(w); P(rw); // 上锁 写文件; V(rw); // 解锁 V(w); } } reader() { ... P(w); P(mutex); if(count == 0) // 如果自己是第一个读文件的进程 1 P(rw); // 上锁 2 ++count; V(mutex); V(w); ... } 假设按照 读者 1 -\u0026gt; 写者 1 -\u0026gt; 读者 2 这种模式并发执行 P(w)\n读者 1 执行 P(w) 写者 1 执行 P(w)，失败，被阻塞，挂在阻塞队列的队头 读者 2 执行 P(w)，失败，被阻塞，挂在阻塞队列的队头后 读者 1 执行 V(w)，解锁，唤醒 写者 1 进程 \u0026hellip; 可以发现，添加一个信号量 w，解决了写进程饥饿的问题，实现了 “写优先”\n当然，这个 “写优先” 不是真正意义上的写优先，而是一种「先来先服务」的原则，体现在阻塞队列中\n线程池 线程池是一种并发编程的技术，可以管理和 重用 多个线程来执行任务。\n主要目的 提高线程的利用率和性能 控制线程的数量 避免创建和销毁线程的开销。 组成 任务队列（Task Queue）：用于存储待执行的任务。线程池中的线程从任务队列中获取任务并执行。\n线程池管理器（Thread Pool Manager）：负责创建、初始化和管理线程池。它维护线程池的状态、线程数量、任务队列等信息。\n工作线程（Worker Threads）：线程池中的线程，负责执行任务。它们从任务队列中获取任务，并执行任务的代码逻辑。\n工作流程 初始化线程池：创建指定数量的线程，并初始化任务队列等。\n提交任务：外部程序将任务提交给线程池管理器，任务会被添加到任务队列中。\n空闲线程获取任务：当线程池中的线程空闲时，它们会从任务队列中获取任务并执行。\n执行任务：线程执行任务的代码逻辑，处理任务的操作。\n返回结果：任务执行完成后，可以返回结果给调用方。\n终止线程池：当不再需要线程池时，可以通过线程池管理器来终止线程池，释放资源。\n优点 提高性能：线程池可以重用线程，避免了频繁创建和销毁线程的开销，提高了性能。\n控制并发度：通过控制线程的数量，可以限制并发执行的任务数量，避免系统资源过度占用。\n提供任务排队机制：任务可以在任务队列中排队等待执行，避免任务丢失和阻塞调用方。\n提供任务调度和管理：线程池管理器可以灵活地调度任务，并提供监控和管理线程池的功能。\n实例 ThreadTask.h（任务） #pragma once #include \u0026lt;functional\u0026gt; #include \u0026lt;random\u0026gt; #include \u0026lt;cstdio\u0026gt; class ThreadTask { using task_t = std::function\u0026lt;void(void)\u0026gt;; task_t task; // 简单四则运算作为任务函数 public: ThreadTask() :task(nullptr) {} ThreadTask(const task_t \u0026amp;task) :task(task) {} void run(void) { task(); } }; void task(void) { std::random_device rd; std::mt19937 e(rd()); std::uniform_int_distribution\u0026lt;int\u0026gt; u(1, 100); char ops[] = {\u0026#39;+\u0026#39;, \u0026#39;-\u0026#39;, \u0026#39;*\u0026#39;, \u0026#39;/\u0026#39;}; int a = u(e); int b = u(e); char op = ops[u(e) % 4]; int res = 0; switch (op) { case \u0026#39;+\u0026#39;: res = a + b; break; case \u0026#39;-\u0026#39;: res = a - b; break; case \u0026#39;*\u0026#39;: res = a * b; break; case \u0026#39;/\u0026#39;: res = a / b; break; default: break; } // printf(\u0026#34;[%X]%d %c %d = %d\\n\u0026#34;, pthread_self(), a, op, b, res); // 如果要看执行结果，取消注释，这里为了观察运行时间，才注释掉 // sleep(1); } 其中，\nstd::random_device rd; std::mt19937 e(rd()); std::uniform_int_distribution\u0026lt;int\u0026gt; u(1, 100); 是 C++ 11 引入的随机数库，通过使用随机设备作为种子来产生随机数\nThreadPool.hpp（线程池） #pragma once #include \u0026#34;ThreadTask.h\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; /** * @brief 工具函数，每个线程的入口函数，运行任务 * * @param arg 线程池的 this 指针 * @return void* */ void *runTask(void *arg); class ThreadPool { static constexpr int MAX_THREAD_NUM = 6; // 默认允许的线程数量的最大值 friend void *runTask(void *arg); private: size_t curThreadNum; // 线程池当前含有的线程数量 size_t maxThreadNum; // 线程池最大允许的线程数量 pthread_mutex_t lock; // 互斥锁 pthread_cond_t cond; // 条件变量 bool _quit = false; // 标记线程池是否退出 std::queue\u0026lt;ThreadTask\u0026gt; taskQueue; // 线程调度队列 int popCnt = 0; private: // 锁定调度队列 void lockQueue(void); // 解锁调度队列 void unlockQueue(void); // 唤醒一个正在等待的线程 void wakeupOne(void); void wakeupAll(void); // 让线程阻塞 void threadWait(void); // 让线程退出 void threadQuit(void); public: ThreadPool(size_t maxThreadNum = MAX_THREAD_NUM); ~ThreadPool(); void pushTask(const ThreadTask \u0026amp;task); void popTask(ThreadTask \u0026amp;task); // 使线程池退出所有线程 void quit(void); }; inline void ThreadPool::lockQueue(void) { pthread_mutex_lock(\u0026amp;lock); } inline void ThreadPool::unlockQueue(void) { pthread_mutex_unlock(\u0026amp;lock); } inline void ThreadPool::wakeupOne(void) { pthread_cond_signal(\u0026amp;cond); } inline void ThreadPool::wakeupAll(void) { pthread_cond_broadcast(\u0026amp;cond); } inline void ThreadPool::threadWait(void) { if (_quit) threadQuit(); pthread_cond_wait(\u0026amp;cond, \u0026amp;lock); } inline void ThreadPool::threadQuit(void) { --curThreadNum; unlockQueue(); pthread_exit(NULL); } void ThreadPool::pushTask(const ThreadTask \u0026amp;task) { lockQueue(); if (_quit) { unlockQueue(); return; } taskQueue.push(task); unlockQueue(); wakeupOne(); // 唤醒可能正在等待的线程 } void ThreadPool::popTask(ThreadTask \u0026amp;task) { // lockQueue(); // 不能上锁，因为在 runTask 函数已经上锁，重复上锁会阻塞！ task = taskQueue.front(); taskQueue.pop(); // unlockQueue(); } void ThreadPool::quit(void) { lockQueue(); _quit = true; unlockQueue(); while (curThreadNum \u0026gt; 0) { wakeupAll(); usleep(1000); } } ThreadPool::ThreadPool(size_t maxThreadNum) : curThreadNum(0), maxThreadNum(maxThreadNum) { pthread_mutex_init(\u0026amp;lock, NULL); pthread_cond_init(\u0026amp;cond, NULL); pthread_t tid; for (size_t i = 0; i \u0026lt; maxThreadNum; ++i) { if (pthread_create(\u0026amp;tid, NULL, runTask, this) != 0) { throw std::runtime_error(\u0026#34;Create Thread Error!\\n\u0026#34;); } ++curThreadNum; } } ThreadPool::~ThreadPool() { pthread_mutex_destroy(\u0026amp;lock); pthread_cond_destroy(\u0026amp;cond); } void *runTask(void *arg) { pthread_detach(pthread_self()); ThreadPool *threadPool = static_cast\u0026lt;ThreadPool *\u0026gt;(arg); while (1) // 让线程一直做任务，直到调用了 threadpool.quit()「会通知线程退出」 { threadPool-\u0026gt;lockQueue(); // 注意，在循环内上锁，否则就不能互斥的访问临界资源！！ while (threadPool-\u0026gt;taskQueue.empty()) { threadPool-\u0026gt;threadWait(); } ThreadTask task; threadPool-\u0026gt;popTask(task); threadPool-\u0026gt;unlockQueue(); task.run(); fflush(stdout); // sleep(3); } return NULL; } 重点和易错点已经标注在代码中\nmain.cpp（请求方） #include \u0026#34;ThreadPool.hpp\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;ctime\u0026gt; #include \u0026lt;cstdio\u0026gt; int main(void) { printf(\u0026#34;-------------------ThreadPool[%d]-------------------\\n\u0026#34;, getpid()); time_t startTime = time(NULL); ThreadPool threadPool(500); // 开 500 个线程 for(int i = 0; i \u0026lt; 100000; ++i) // 来 100000 个任务，分配给 500 个线程执行 { ThreadTask threadTask(task); threadPool.pushTask(threadTask); } threadPool.quit(); time_t endTime = time(NULL); printf(\u0026#34;Process runtime: %lf\\n\u0026#34;, difftime(endTime, startTime)); } 完成相同数量的任务，使用线程池：\n不使用线程池：\n可以看到，使用线程池，执行时间不到 1 s，而不使用线程池，执行相同任务，需要 26 s！\n虽然使用线程池产生了额外的调度开销与性能损失，但只要开的线程数量合适，执行效率会大大提高！\n单例设计模式 饿汉实现 template \u0026lt;typename T\u0026gt; // 饿汉实现单例模式 class Singleton { private: static T val; // 私有构造函数，防止实例化对象 Singleton() {} public: // 公共静态成员函数，用于获取单例实例 static T \u0026amp;GetInstance() { return val; } }; template \u0026lt;typename T\u0026gt; T Singleton\u0026lt;T\u0026gt;::val = 114; // 在类的外部进行初始化 int main() { int \u0026amp;val = Singleton\u0026lt;int\u0026gt;::GetInstance(); // 使用单例对象进行操作 val = 2; std::cout \u0026lt;\u0026lt; Singleton\u0026lt;int\u0026gt;::GetInstance() \u0026lt;\u0026lt; std::endl; // 2 std::cout \u0026lt;\u0026lt; Singleton\u0026lt;double\u0026gt;::GetInstance() \u0026lt;\u0026lt; std::endl; // 114 // 因为 Singleton\u0026lt;double\u0026gt; 与 Singleton\u0026lt;int\u0026gt; 是不同的实例 return 0; } 懒汉实现 int cnt = 0; // 记录实例化的次数 template \u0026lt;typename T\u0026gt; // 懒汉实现单例模式 class Singleton { private: static T* val; // 私有构造函数，防止实例化对象 Singleton() {} public: // 公共静态成员函数，用于获取单例实例 static T* GetInstance() { if(val == nullptr) { val = new T; ++cnt; } return val; } }; template\u0026lt;typename T\u0026gt; T* Singleton\u0026lt;T\u0026gt;::val = nullptr; 测试代码：\nvoid* run(void *arg) { pthread_detach(pthread_self()); int *pi = Singleton\u0026lt;int\u0026gt;::GetInstance(); return NULL; } int main() { pthread_t tid; for(int i = 0; i \u0026lt; 100; ++i) // 开 100 个线程 { pthread_create(\u0026amp;tid, NULL, run, NULL); } std::cout \u0026lt;\u0026lt; cnt \u0026lt;\u0026lt; std::endl; // 输出实例化次数 return 0; } 输出：\n这段代码存在一个严重问题：线程不安全\n在单线程环境下，如果直接使用单次判断 val == nullptr，则在第一次调用 GetInstance() 时会创建单例对象，这是正确的。\n但在多线程环境下，可能会导致多个线程 「同时」通过 第一次判断，进入临界区创建实例，从而导致 创建多个实例 ，违背了单例模式的要求。（这里的同时是宏观上的）\n如何解决？使用互斥锁\ntemplate \u0026lt;typename T\u0026gt; // 懒汉实现单例模式 class Singleton { private: static T* val; static pthread_mutex_t lock; // 私有构造函数，防止实例化对象 Singleton() {} public: // 公共静态成员函数，用于获取单例实例 static T* GetInstance() { pthread_mutex_lock(\u0026amp;lock); if(val == nullptr) { val = new T; ++cnt; } pthread_mutex_unlock(\u0026amp;lock); return val; } }; template\u0026lt;typename T\u0026gt; T* Singleton\u0026lt;T\u0026gt;::val = nullptr; template \u0026lt;typename T\u0026gt; pthread_mutex_t Singleton\u0026lt;T\u0026gt;::lock = PTHREAD_MUTEX_INITIALIZER; 再次运行：\n经过多次运行，输出均为 1，可以证明，val 只被实例化 1 次\n但实际上，上面的代码还有一个问题，性能损失\n因为不管有没有实例化 val， 每次 有一个线程尝试获取实例对象，我们 都先上锁 ，这样无疑增加了许多锁冲突，造成 性能严重损失！\n如何解决？\n双重检查锁定（Double-Checked Locking）\nstatic T* GetInstance() { if(val == nullptr) // 解决大部分锁冲突，因为大部分线程运行时，val 已被实例化 { pthread_mutex_lock(\u0026amp;lock); if(val == nullptr) // 确实需要实例化 { val = new T; ++cnt; } } pthread_mutex_unlock(\u0026amp;lock); return val; } 双重检查锁定避免了潜在的性能损失\n注意： 在使用双重检查锁定时，需要将指针 val 声明为 volatile 类型，以避免编译器进行优化。这是因为编译器在进行优化时可能会对指令进行重排，导致双重检查锁定失效。使用 volatile 关键字可以告诉编译器不要对该指针进行优化，保证代码的正确性。\nstatic volatile T* val; // ...... template\u0026lt;typename T\u0026gt; volatile T* Singleton\u0026lt;T\u0026gt;::val = nullptr; 线程安全相关 STL 的容器是否线程安全 STL 的容器 不是 线程安全的\n因为在设计 STL 时，追求的是极致的效率，如果要实现线程安全，势必会带来许多性能开销\n因此，STL 的容器不是线程安全的，需要用户保证线程安全\n智能指针是否线程安全 智能指针，即 unique_ptr 和 shared_ptr 是线程安全的\n对于 unique_ptr，它只在当前代码范围内生效，离开当前范围，自动析构，不存在线程安全问题\n对于 shared_ptr，与 unique_ptr 不同的是，离开当前范围是否析构，取决于「引用计数值」是否为 0\n因此，shared_ptr 的线程安全主要就取决于「引用计数值」的管理上\nshared_ptr 的实现使用了原子操作（例如CAS，Compare-and-Swap）来确保 对引用计数的操作是线程安全的 。shared_ptr 的引用计数是 以原子方式 进行递增和递减的，这意味着多个线程可以同时对引用计数进行操作，而不会导致竞争条件或数据不一致的问题。\n同一个shared_ptr对象可以被多线程同时读取。\n不同的shared_ptr对象（即使管理的对象指针相同）可以被多线程同时修改。\n同一个shared_ptr对象不能被多线程直接修改\n几种锁 悲观锁 悲观锁的核心思想：总是 悲观地 认为其它线程 会 修改临界资源，因此，每次获取临界数据前，总会加锁\n特点：\n阻塞等待：当获取锁的线程无法立即获得资源访问权限时，会被阻塞等待，直到获取到锁为止。 资源独占 ：悲观锁一次只允许一个线程访问共享资源，其他线程需要等待锁的释放。 效率较低 ：由于需要频繁地加锁和解锁操作，以及可能的线程切换和阻塞唤醒操作，悲观锁在高并发场景下可能导致性能下降。 因此，悲观锁适用于 读写频繁 的场景\n乐观锁 乐观锁的核心思想：总是 乐观地 认为其它线程 不会 修改临界资源，因此，每次获取临界数据前，不会加锁，而是在更新资源时 检查是否发生冲突\n这种也叫做 无锁编程\n检查方式：\n版本号（Versioning）：为共享资源引入一个版本号或时间戳，每次更新时都对版本号进行比较，以检测是否发生冲突。 CAS（Compare and Swap）：使用原子操作，比较共享资源的当前值与预期值，若相等则进行更新，否则 重试。 CAS（Compare and Swap）是一种乐观锁的实现方式，用于解决并发编程中的原子操作问题。CAS操作通过比较内存中的值与期望值，如果相等则进行交换，否则不执行交换\n特点：\n非阻塞：乐观锁 不会阻塞 其他线程的访问，线程可以自由地进行操作。 冲突检测 重试机制 例如：在线表格编辑这个场景就是使用的乐观锁，这个场景下，用户修改到同一个单元格，即发生冲突的概率较小\n如果使用悲观锁，那每个用户提交自己的修改这个过程是单线程的，很慢，用户体验不好 如果使用乐观锁，由于乐观锁实际上就是无锁编程，并发高，并且发生冲突的概率较低（重试的概率小），用户体验会好很多 在这个场景，假设有两个用户：\n用户 A 从 Server 获取文档以及版本号V0 用户 B 从 Server 获取文档以及版本号V0 用户 A 修改了单元格 A，提交，服务器校验本地版本号与用户提交携带的版本号一致，修改成功，更新版本号为 V1 用户 B 也修改了单元格 A，提交，服务器校验本地版本号与用户提交携带的版本号 不一致，修改失败，通常客户端会重新从 Server 获取最新的文档，然后让用户重试 因此，只有 加锁成本高，并且发生 冲突概率比较小 的情况下，可以考虑使用乐观锁\n读写锁 在之前我们使用伪代码以及信号量机制介绍了读者写者问题\n实际上，POSIX 还提供了一些接口实现读写锁\npthread_rwlock_init pthread_rwlock_init 函数用于初始化读写锁对象。\n函数原型：\nint pthread_rwlock_init(pthread_rwlock_t *rwlock, const pthread_rwlockattr_t *attr); 参数：\nrwlock：读写锁对象的指针 attr：读写锁属性对象的指针，用于设置读写锁的属性。可以传递 NULL，表示使用默认属性 返回值：\n成功时，返回 0。 失败时，返回错误代码。 示例用法：\npthread_rwlock_t rwlock; pthread_rwlock_init(\u0026amp;rwlock, NULL); pthread_rwlockattr_setkind_np（设置锁的偏好） pthread_rwlockattr_setkind_np 函数是一个 非标准的 POSIX 函数，用于设置读写锁属性中的锁类型。\n函数原型：\nint pthread_rwlockattr_setkind_np(pthread_rwlockattr_t *attr, int pref); 参数：\nattr：读写锁属性对象的指针 pref：锁类型的偏好值。可以是以下常量之一： PTHREAD_RWLOCK_PREFER_READER_NP：偏好于读者 PTHREAD_RWLOCK_PREFER_WRITER_NP：偏好于写者 PTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP：偏好于非递归写者 返回值：\n成功时，返回 0。 失败时，返回错误代码。 注意： pthread_rwlockattr_setkind_np 是一个非标准的函数，其可移植性可能有所限制。\npthread_rwlock_rdlock pthread_rwlock_rdlock 函数用于获取读锁，允许多个线程同时读取共享资源，但不允许写入操作。\n函数原型：\nint pthread_rwlock_rdlock(pthread_rwlock_t *rwlock); 参数 rwlock 是一个指向读写锁对象的指针，指向要获取读锁的读写锁对象。\n函数返回值为 0 表示成功获取读锁，非零值表示失败。\n调用 pthread_rwlock_rdlock 函数会尝试获取读锁，\n如果当前 没有线程持有写锁，则获取成功 如果有其他线程持有写锁，则获取读锁的线程会被阻塞，直到写锁被释放 读锁是 共享的 ， 多个线程可以同时持有读锁 ，并行地读取共享资源。在 读锁被持有期间，其他线程可以继续获取读锁，但无法获取写锁，保证了读锁的共享性和读操作的并发性。\npthread_rwlock_wrlock pthread_rwlock_wrlock 函数用于获取写锁，允许一个线程独占地写入共享资源，而其他线程无法同时读取或写入。\n函数原型：\nint pthread_rwlock_wrlock(pthread_rwlock_t *rwlock); 如果当前没有线程持有读锁或写锁，则获取成功，允许当前线程 独占地写入 共享资源。 如果有其他线程持有读锁或写锁，则获取写锁的线程会被阻塞，直到读锁和写锁都被释放。 写锁是独占的，一旦某个线程获取写锁，其他线程无法同时获取读锁或写锁，保证了写操作的互斥性。\npthread_rwlock_unlock pthread_rwlock_unlock 函数用于释放读写锁（读锁或写锁）。\n函数原型：\nint pthread_rwlock_unlock(pthread_rwlock_t *rwlock); 对于读锁，如果当前线程持有读锁的计数大于 1，则只是将读锁计数减 1 对于写锁，会将写锁标记为未持有状态。 pthread_rwlock_destroy pthread_rwlock_destroy 函数用于销毁一个读写锁对象，并释放与之相关的资源。\n函数原型：\nint pthread_rwlock_destroy(pthread_rwlock_t *rwlock); 实例 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;random\u0026gt; #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; std::string sharedResorces = \u0026#34;default\u0026#34;; std::vector\u0026lt;std::string\u0026gt; dic = { \u0026#34;niuma\u0026#34;, \u0026#34;mabaoguo\u0026#34;, \u0026#34;dingzhen\u0026#34;, \u0026#34;wangyuan\u0026#34;, \u0026#34;dianlao\u0026#34; }; pthread_rwlock_t rwlock; pthread_mutex_t mutex; // 用于标准输出的互斥访问 // 读线程 void* thread0(void *arg) { pthread_detach(pthread_self()); for(int i = 0; i \u0026lt; 100; ++i) { pthread_rwlock_rdlock(\u0026amp;rwlock); // std::string buff = \u0026#34;ReadThread[\u0026#34;; // buff += std::to_string(pthread_self()-\u0026gt;__sig); // buff += \u0026#34;]: \u0026#34;; // buff += sharedResorces; // buff.push_back(\u0026#39;\\n\u0026#39;); // std::cout \u0026lt;\u0026lt; buff; // std::cout.flush(); pthread_mutex_lock(\u0026amp;mutex); printf(\u0026#34;ReadThread[%X]: \u0026#34;, pthread_self()); std::cout \u0026lt;\u0026lt; sharedResorces \u0026lt;\u0026lt; std::endl; pthread_mutex_unlock(\u0026amp;mutex); pthread_rwlock_unlock(\u0026amp;rwlock); sleep(1); } return NULL; } // 写线程 void* thread1(void *arg) { pthread_detach(pthread_self()); std::mt19937 e; std::uniform_int_distribution\u0026lt;int\u0026gt; u(0, 4); for(int i = 0; i \u0026lt; 100; ++i) { pthread_rwlock_wrlock(\u0026amp;rwlock); sharedResorces = dic[u(e)]; pthread_rwlock_unlock(\u0026amp;rwlock); sleep(1); } return NULL; } int main(void) { pthread_t tid; pthread_rwlock_init(\u0026amp;rwlock, NULL); pthread_mutex_init(\u0026amp;mutex, NULL); for(int i = 0; i \u0026lt; 10; ++i) { pthread_create(\u0026amp;tid, NULL, thread0, NULL); } pthread_create(\u0026amp;tid, NULL, thread1, NULL); sleep(10); pthread_rwlock_destroy(\u0026amp;rwlock); pthread_mutex_destroy(\u0026amp;mutex); return 0; } ","permalink":"https://blogs.skylee.top/posts/linux/linux-%E5%A4%9A%E7%BA%BF%E7%A8%8B/note/","tags":["Linux","OS","并发编程"],"title":"Linux 多线程"},{"categories":["Linux"],"content":"守护进程（Daemon Process）是在 后台运行 的一种特殊类型的进程。它独立于终端会话，并且在系统启动时启动，并持续运行以提供特定的服务或执行特定的任务。守护进程通常以系统级别的服务方式运行，提供后台任务的管理和执行。\n特点 后台运行：守护进程在后台运行，没有与用户终端相关联，不会向终端输出信息，也不会从终端接收输入。它们通常在系统启动时自动启动，并持续运行，直到系统关闭或手动停止。\n无终端关联：守护进程与终端无关，它们不会受到终端关闭或用户退出的影响。它们通常在系统级别启动，不依赖于用户登录或终端会话。\n独立于终端会话：守护进程不会与任何特定的终端会话相关联。它们不会收到终端信号（如Ctrl+C）并且不会受到用户登录或注销的影响。\n任务服务：守护进程通常用于提供特定的服务或执行特定的任务。它们可以是网络服务、系统监控、定时任务、日志记录等。守护进程可以在后台运行，持续监控和处理任务，而不会影响其他用户进程或终端会话。\n无交互性：守护进程通常是无交互性的，它们不会与用户进行直接的交互。它们的配置和控制通常通过配置文件、命令行选项或特定的管理工具进行。\n在实现守护进程时，通常需要注意以下几个方面：\n重定向标准输入、输出和错误流：守护进程通常需要将标准输入、输出和错误流重定向到文件或设备，以避免与终端相关联。 脱离终端会话：守护进程需要通过调用fork()函数创建子进程，并使子进程脱离终端会话，通过setsid()函数创建一个新的会话。 重设文件权限掩码：守护进程需要调用umask()函数重设文件权限掩码，以确保创建的文件具有适当的权限。 处理信号：守护进程通常需要处理一些重要的信号，如终止信号(SIGTERM)、重新加载配置信号(SIGHUP)等。 setsid 在Linux系统中，setsid()函数是一个系统调用，用于创建一个新的会话并设置新的进程组ID。它是守护进程创建过程中常用的一步。\n具体来说，setsid()函数的作用是：\n创建新的会话：调用setsid()函数会创建一个新的会话，使得当前进程成为该会话的首领进程（session leader）。该会话成为一个独立的会话，与原有的终端会话完全分离。\n设置新的进程组ID：在新的会话中，setsid()函数将创建一个新的进程组，并将当前进程设置为该进程组的组长进程（group leader）。新的进程组ID与会话ID相同。\nsetsid()函数通常用于将守护进程从终端会话中脱离出来，使其成为一个独立的进程实体，独立于任何终端或终端会话。这样可以确保守护进程不受终端会话的影响，并能够在后台持续运行。\nchdir chdir()是一个系统调用，用于改变当前进程的工作目录。它是\u0026quot;change directory\u0026quot;的缩写。\n函数原型如下：\nint chdir(const char *path); 参数path是一个字符串，表示要改变到的目标目录的路径。\nchdir()函数的作用是将当前进程的工作目录更改为指定的目录。它的使用方式 类似于在命令行中使用cd命令 改变目录。\n使用chdir()函数时需要注意以下几点：\n目录路径必须是有效的：传递给chdir()函数的目录路径必须是一个有效的目录路径，否则函数调用将失败并返回-1。\n目录权限：在尝试改变工作目录之前，应该 确保当前进程对目标目录具有足够的权限 ，以便能够读取和写入该目录。\n目录路径的格式：目录路径可以是相对路径或绝对路径。相对路径是相对于当前工作目录的路径，而绝对路径是从根目录开始的完整路径。\n错误处理：chdir()函数在失败时会返回-1，并设置errno变量以指示具体的错误类型。可以使用perror()或strerror()函数来输出错误信息。\numask umask是一个系统调用，用于设置文件创建时的默认权限掩码。它决定了文件和目录的默认权限。\n函数原型如下：\nmode_t umask(mode_t mask); 参数mask是一个权限掩码，用于指定要屏蔽的权限位。它是一个八进制数，表示要屏蔽的权限组合。通常使用八进制表示法来指定umask值，如0777表示屏蔽所有权限。\numask()函数的作用是设置当前进程的文件创建掩码，并返回之前的文件创建掩码。\n文件创建掩码是一个权限位掩码，用于确定在创建新文件或目录时应该屏蔽哪些权限位。umask函数通过设置掩码来影响新文件和目录的默认权限。具体来说，umask值与新文件或目录的权限进行按位与操作，将屏蔽的权限位从新文件或目录的权限中去除。\n例如，如果umask值设置为0022，表示屏蔽其他用户的写权限和组用户的写权限，那么创建新文件时，它的默认权限将是0644（所有者可读写，其他用户可读）。\n需要注意以下几点：\numask值的默认值通常是022，它使得新文件的默认权限为0644，新目录的默认权限为0755。这样设置的目的是为了确保文件和目录对所有者可读写，对其他用户只有读的权限。\numask值可以在程序中通过调用umask函数来进行更改。通常在程序开始的时候可以设置合适的umask值，以确保文件和目录的默认权限符合需求。\numask值是进程级别的，它 会影响到该进程及其所有子进程创建的文件和目录。它 不会影响已经存在的文件和目录的权限。\numask值是进程的属性，不会被继承或传递给其他进程。\nSIGHUP 信号 SIGHUP是一个UNIX和类UNIX系统中的信号，它代表着\u0026quot;挂起\u0026quot;（Hangup）信号。它通常由终端控制进程（如登录会话）发送给与终端设备连接的进程，用于通知该进程终端连接的状态发生了变化。\n默认情况下，当进程接收到SIGHUP信号时，它的默认操作是终止（Terminate）。这意味着进程会立即退出并终止其执行。\n对于守护进程来说，收到 SIGHUP 信号，通常意味着终端链接断开，我们当然不希望守护进程因此而退出，所以需要指定 SIGHUP 信号的处理方式\n实例 #include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;signal.h\u0026gt; class Daemonize { public: // 执行守护进程的初始化 Daemonize() { pid_t pid = fork(); if(pid \u0026lt; 0) { throw std::runtime_error(\u0026#34;Fork error!\\n\u0026#34;); exit(1); } else if(pid \u0026gt; 0) // 第一步：退出父进程 exit(0); // 第二步，在子进程中创建新会话，成为首领进程 if(setsid() \u0026lt; 0) { throw std::runtime_error(\u0026#34;Setsid error!\\n\u0026#34;); exit(1); } // 第三步，屏蔽 SIGHUP 信号，也可以自定义处理方式 signal(SIGHUP, SIG_IGN); // 第四步，关闭标准输入、输出、错误，脱去与原终端的所有关联 for(int fd = STDIN_FILENO; fd \u0026lt;= STDERR_FILENO; ++fd) close(fd); // 第五步，修改工作目录为工作目录，可以根据需求调整 chdir(\u0026#34;/Users/Sky_Lee/Documents/Linux/studyNotes/Linux operating system/Linux 守护进程\u0026#34;); // 第六步，修改文件掩码，可以根据需求调整 umask(0000); // 均不屏蔽 } void run(void) { // 执行核心逻辑 std::fstream fs; fs.open(\u0026#34;log.txt\u0026#34;); if(!fs.is_open()) exit(1); fs \u0026lt;\u0026lt; \u0026#34;------------------Daemonize------------------\u0026#34; \u0026lt;\u0026lt; std::endl; fs \u0026lt;\u0026lt; \u0026#34;[note]: pid: \u0026#34; \u0026lt;\u0026lt; getpid() \u0026lt;\u0026lt; std::endl; fs.close(); while(1) // 模拟其它过程 { sleep(1); } } }; int main(void) { Daemonize daemonize; daemonize.run(); } ","permalink":"https://blogs.skylee.top/posts/linux/linux-%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/note/","tags":["Linux","OS"],"title":"Linux 守护进程"},{"categories":["Linux"],"content":"查看信号列表的方式 可以使用 kill -l指令查看信号列表：\n信号的产生 通过终端按键产生信号 常见的按键组合和相应的信号：\nCtrl+C (SIGINT): 发送SIGINT信号 Ctrl+(SIGQUIT): 发送SIGQUIT信号，通常用于终止程序，并 生成core dump文件。 Ctrl+Z (SIGTSTP): 发送SIGTSTP信号，用于挂起程序的执行，将其放入后台运行。 Ctrl+D (EOF): 发送EOF（End of File）信号，表示输入流已经结束。 CoreDump Core Dump（核心转储）是指在程序发生严重错误时，操作系统将程序的内存状态保存到一个特殊的文件中，以便进行后续的调试和分析。核心转储文件通常称为core文件。\n当程序发生严重错误，比如段错误（Segmentation Fault）或其他致命错误时，操作系统会捕获这些错误，并生成一个core文件。该文件包含了导致程序崩溃的内存状态，包括程序代码、堆栈信息、寄存器状态以及其他相关的调试信息。\n由于 coredump 中可能会包含敏感信息，不安全，并且生成 coredump 文件还会占据很大的磁盘空间，因此，生成 coredump 文件默认是禁用的\n在开发时，可以通过设置ulimit来启用。通常情况下，core文件会被保存在当前工作目录下，并以core或者core.\u0026lt;进程ID\u0026gt;的文件名格式命名。\n可以看到，此时的 core file size = 0，可以使用 ulimit -c 指令来设置文件的大小：\n实例 编写一个访问空指针的程序：\nint main(void) { int *p = NULL; *p = 1; } 运行后，查看 coredump 文件：\n由软件条件产生的信号 在 Linux 进程通信 中，介绍过 SIGPIPE 信号，它是一种软件条件产生的信号\n接下来主要介绍 SIGALRM 信号\nalarm 系统调用 alarm 函数是一个用于设置定时器的系统调用。它允许我们在指定的时间间隔后接收一个SIGALRM信号，从而可以在程序中实现定时操作。\n使用alarm函数需要包含头文件\u0026lt;unistd.h\u0026gt;。函数原型如下：\nunsigned int alarm(unsigned int seconds); seconds参数指定定时器的时间间隔，单位为秒。当定时器到期时，将发送一个SIGALRM信号给调用进程。\nalarm函数的返回值为之前设置的定时器剩余的时间。\nvoid sigHandler(int signum) { printf(\u0026#34;Received SIG signal\\n\u0026#34;); } int main(void) { signal(SIGALRM, sigHandler); unsigned s0 = alarm(10); // 0 printf(\u0026#34;%d\\n\u0026#34;, s0); unsigned s1 = alarm(3); // s1 = 10，重设闹钟 printf(\u0026#34;%d\\n\u0026#34;, s1); sleep(1); unsigned s2 = alarm(1); // 3 - 1 = 2 printf(\u0026#34;%d\\n\u0026#34;, s2); time_t startTime = time(NULL); sleep(10); // 在睡了 1s 后，内核发出 SIGALRM 信号给进程，进程被唤醒 time_t endTime = time(NULL); printf(\u0026#34;Sleeped for %lf seconds.\\n\u0026#34;, difftime(endTime, startTime)); return 0; } 输出：\n信号捕捉 在刚才的例子中，有一个 void sigHandler(int signum) 函数，它是一个用于捕捉信号的函数\n但是，光有这个函数是不能捕捉到信号的，还需要「注册」\n怎么注册捏？\nsignal 系统调用 signal函数是一个用于处理信号的系统调用。它允许我们为特定的信号指定信号处理函数，以定义在接收到信号时需要执行的操作。\n使用signal函数需要包含头文件\u0026lt;signal.h\u0026gt;。函数原型如下：\nvoid (*signal(int signum, void (*handler)(int)))(int); signum参数指定要处理的信号的编号，可以是预定义的信号宏（如SIGINT、SIGALRM等）或自定义的信号。\nhandler参数是一个函数指针，指向用户定义的信号处理函数。信号处理函数的原型为void handler(int signum)，其中signum是接收到的信号编号。\nsignal函数返回一个函数指针，指向之前注册的信号处理函数。如果之前没有注册过该信号的处理函数，则返回SIG_DFL（默认的信号处理行为）。\n使用signal函数的一般流程如下：\n调用signal函数，并传递要处理的信号编号和相应的信号处理函数，这一步就是前面提到的「注册」\n当程序接收到指定的信号时，操作系统会调用相应的信号处理函数。\n信号处理函数执行特定的操作，可以是自定义的任何有效函数。\n信号处理函数执行完毕后，程序 将继续执行原来的流程。\n由硬件异常产生信号 下面是一个空指针异常（也就是硬件异常）产生 SIGSEGV 信号的例子：\nvoid sigHandler(int signum) { sleep(1); printf(\u0026#34;Received SIG signal\\n\u0026#34;); } int main(void) { printf(\u0026#34;[%d]\\n\u0026#34;, getpid()); signal(SIGSEGV, sigHandler); sleep(1); int *p = NULL; *p = 1; while(1); // 会一直输出 \u0026#34;Received SIG signal\u0026#34;，因为 SIGSEGV 是一种严重错误 // 操作系统会反复发送SIGSEGV信号，以确保程序得到处理。 return 0; } 输出：\n信号集 信号的状态 一般来说，信号有两种状态：\n未决态（Pending），即信号从产生到递达之间的状态 递达态（Delivery） 其中：\n进程可以阻塞（Block）一个信号 一个信号若被阻塞，则其在产生时就处于未决状态 阻塞与忽略不同，只要信号被阻塞，就不会递达，而忽略是信号 递达以后 ，进程可以选择的操作 信号在内核的表示 在这张图中：\n每个信号均有一个 block 位，pending 位，还有一个 hander 位，这个 hander 是一个指向处理当前信号的函数指针 一个信号产生时，OS 会将 pending 位「置位」，直到该信号递达时，将 pending 位「复位」 对于图中的 SIGHUP，它没有阻塞，也没有产生过，处理函数为默认函数 SIG_DFL 对于图中的 SIGINT，它被阻塞，且产生过，因此处于未决态，处理函数为 SIG_IGN，即忽略该信号 对于图中的 SIGQUIT，它被阻塞，但没有产生过，一旦产生，将处于未决态，直到进程主动取消对它的阻塞，处理函数为用户自定义的函数，即 sighandler sigset_t 类型 从上面的分析，我们发现：无论是 未决 还是 阻塞，对于一个信号，只需要一个 bit 位就能反应该信号是否处于 未决（或阻塞）态，因此，一个进程的 阻塞信号集 和 未决信号集 都可以用 sigset_t 这种数据类型（其实就是无符号整型）来存储\n注意： 不要试图直接打印一个 sigset_t 类型的数据，这通常是没有任何意义的\n信号集操作函数 常见的信号集操作函数如下：\n#include \u0026lt;signal.h\u0026gt; int sigemptyset(sigset_t *set); int sigfillset(sigset_t *set); int sigaddset(sigset_t *set, int signo); int sigdelset(sigset_t *set, int signo); int sigismember(const sigset_t *set, int signo); sigemptyset 函数用于将信号集 set 复位，表示不含任何信号 sigfillset 函数用于将信号集 set 置位，表示包含所有系统支持的信号 sigaddset 函数用于添加一个信号到 set 中，signo 是要添加的信号的编号 sigdelset 函数用于在 set 中删除一个信号，signo 是要删除的信号的编号 sigismember 函数用于判断一个信号是否在信号集 set 中，signo 是要查询的信号的编号 sigprocmask sigprocmask 函数用于设置或修改进程的屏蔽信号集。\n函数原型：\nint sigprocmask(int how, const sigset_t *set, sigset_t *oldset); 参数说明：\nhow：用于指定信号屏蔽字的设置方式，可取三个值： SIG_BLOCK：将 set 中的信号添加到当前信号屏蔽字中，相当于 |= 操作。 SIG_UNBLOCK：从当前信号屏蔽字中移除 set 中的信号，相当于 mask = mask \u0026amp; ~set。 SIG_SETMASK：将当前信号屏蔽字替换为 set，相当于 mask = set。 set：指向一个 sigset_t 类型的数据结构，用于设置新的信号屏蔽字。 oldset：可选参数，用于获取调用该函数前的旧信号屏蔽字。 函数返回值：\n成功：返回 0。 失败：返回 -1，并设置相应的错误码。 使用 sigprocmask 函数可以实现以下操作：\n阻塞或解除阻塞特定信号：通过设置信号屏蔽字，可以选择阻塞或解除阻塞某些信号的传递。 保护临界区：在进入临界区前，通过设置信号屏蔽字来阻塞某些信号，以避免在关键时刻被中断。 防止竞态条件：通过阻塞某些信号来避免并发执行引起的竞态条件。 注意： sigprocmask 函数只会影响当前进程的信号屏蔽字，对其他进程无影响。\nsigpending sigpending 函数用于读取进程的未决信号集\n#include \u0026lt;signal.h\u0026gt; int sigpending(sigset_t *) 实例 void printSigset(sigset_t sigSet) { char buff[33]; // 1 ~ 31 // for (int curSig = SIGHUP; curSig \u0026lt;= SIGUSR2; ++curSig) // 不同的机器，SIGUSER2 的编号不同，在 macOS 上，该信号的编号为 31 for (int curSig = 1; curSig \u0026lt;= 31; ++curSig) { // if ((sig \u0026gt;\u0026gt; curSig) \u0026amp; 1) if(sigismember(\u0026amp;sigSet, curSig)) buff[curSig] = \u0026#39;1\u0026#39;; else buff[curSig] = \u0026#39;0\u0026#39;; } buff[0] = \u0026#39;0\u0026#39;; buff[32] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;%s\\n\u0026#34;, buff); } int main(void) { sigset_t blockSigSet, oldSigSet; sigemptyset(\u0026amp;blockSigSet); // 初始化信号，置 0 sigaddset(\u0026amp;blockSigSet, SIGINT); // 将 SIGINT（中断信号）加入至阻塞信号集 // 将当前进程的阻塞信号集设置为 blockSigSet ，并将进程原来的阻塞信号集存放在 oldSigSet 中 sigprocmask(SIG_BLOCK, \u0026amp;blockSigSet, \u0026amp;oldSigSet); printf(\u0026#34;Original block signal set: \u0026#34;); printSigset(oldSigSet); // 0000 ... printf(\u0026#34;Current block signal set: \u0026#34;); printSigset(blockSigSet); // 0010 ... sigset_t pendingSigSet; while (1) { sigpending(\u0026amp;pendingSigSet); // 获取未决信号集 printSigset(pendingSigSet); sleep(1); } } 输出：\n捕捉信号再探 内核捕捉信号的过程 引用自 博主Mindtechnist 例如，用户程序注册了 SIGQUIT 信号的处理函数 sigaction_user。\n当前正在执行 main函数,这时发生中断或异常 切换到内核态 。\n在 中断处理完毕 后要返回用户态的main函数之前 检查到有信号 SIGQUIT递达 。\n内核决定 返回用户态 后不是恢复main函数的上下文继续执行,而是 执行 sigaction_user 函数 。\nsigaction_user 和 main 函数使用不同的堆栈空间，它们之间不存在调用和被调用的关系，是两个独立的控制流程。\nsigaction_user 函数返回后自动 执行特殊的系统调用 sigreturn 再次进入内核态 。\n如果没有新的信号要递达,这次再 返回用户态就是恢复 main函数的上下文继续执行 了。\n可以发现，如果使用默认的信号处理函数，状态切换两次\n如果使用自定义的信号处理函数，状态切换四次\n引入信号集 void printSigset(sigset_t sigSet); void sigHandler(int signum) { printf(\u0026#34;---In function sigHander---\\n\u0026#34;); if(signum == SIGINT) printf(\u0026#34;Received SIGINT signal\\n\u0026#34;); else if(signum == SIGSEGV) printf(\u0026#34;Received SIGSEGV signal\\n\u0026#34;); sigset_t curBlockSet; sigset_t curPendingSet; sigemptyset(\u0026amp;curBlockSet); sigemptyset(\u0026amp;curPendingSet); sigprocmask(SIG_BLOCK, NULL, \u0026amp;curBlockSet); // 由于不修改阻塞信号集，因此获取的是当前阻塞信号集 sigpending(\u0026amp;curPendingSet); printf(\u0026#34;Cur Block Signal Set:\\n\u0026#34;); printSigset(curBlockSet); printf(\u0026#34;Cur Pending Signal Set:\\n\u0026#34;); printSigset(curPendingSet); sleep(3); // exit(0); } void printSigset(sigset_t sigSet) { char buff[33]; // 1～31 // for (int curSig = SIGHUP; curSig \u0026lt;= SIGUSR2; ++curSig) // 不同的机器，SIGUSER2 的编号不同，在 macOS 上，该信号的编号为 31 for (int curSig = 1; curSig \u0026lt;= 31; ++curSig) { // if ((sig \u0026gt;\u0026gt; curSig) \u0026amp; 1) if(sigismember(\u0026amp;sigSet, curSig)) buff[curSig] = \u0026#39;1\u0026#39;; else buff[curSig] = \u0026#39;0\u0026#39;; } buff[0] = \u0026#39;0\u0026#39;; buff[32] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;%s\\n\u0026#34;, buff); } int main(void) { printf(\u0026#34;---In function main---\\n\u0026#34;); signal(SIGSEGV, sigHandler); // 注册 SIGSEGV 信号 signal(SIGINT, sigHandler); // 注册 SIGINT 信号 sigset_t curBlockSet; sigemptyset(\u0026amp;curBlockSet); sigprocmask(SIG_BLOCK, NULL, \u0026amp;curBlockSet); printf(\u0026#34;Cur Block Signal Set:\\n\u0026#34;); printSigset(curBlockSet); sleep(1); int *p = NULL; *p = 1; while(1); return 0; } 输出：\n为什么发出 SIGINT 信号后，在 sigHandler 函数中，阻塞信号集包含 SIGINT 信号，即使我们之前并没有将其添加到阻塞信号集中？\n原因是，当信号处理函数执行时，内核会自动将相应的信号添加到阻塞信号集中，以防止同一信号的重复递送。这样做是 为了保护信号处理函数不会被同一信号中断 。因此，当 SIGINT 信号处理函数 sigHandler 执行时，阻塞信号集中可能包含 SIGINT 信号\n现在请你结合之前讲的内容，分析一下，在 sigHandler 函数中，为什么 SIGSEGV 一直出现在阻塞信号集中？\n此外，为什么未决信号集中，始终不包含 SIGINT 和 SIGSEGV 信号？\n你想想，为什么会从 main 函数切换到 sigHandler 函数？肯定是因为程序收到了 SIGINT 或者 SIGSEGV 信号啊，而未决信号集是指：从信号产生到信号递达到过程\n在打印未决信号集的时候，信号已经递达了，内核会自动将该信号从未决信号集中移除。这样做是为了确保每个信号只被处理一次，防止信号的重复触发。\n所以，打印的未决信号集不包含这两个信号\nVolatile 关键字 volatile 是一个关键字，用于 告诉编译器不要对被修饰的变量进行优化。它通常 用于修饰那些可能被程序之外的因素修改的变量，以确保每次访问该变量都从内存中读取最新的值，而不是使用缓存的值。\n来看一个与信号相关的例子：\nint flag = 0; void sigHandler(int signum) { printf(\u0026#34;Changing flag from 0 to 1\\n\u0026#34;); flag = 1; } int main(void) { time_t startTime = time(NULL); signal(SIGINT, sigHandler); while(!flag); time_t endTime = time(NULL); printf(\u0026#34;Process quit normaly with running time %lf s.\\n\u0026#34;, difftime(endTime, startTime)); return 0; } 可以看出，只要我们不向该进程发出 SIGINT 信号，它就会一直执行，直到我们发出 SIGINT 信号，被 sigHander 捕获\n如果我们在编译时，加上 -O2 选项（优化）呢？\n可以发现，无论怎么发出 SIGINT 信号，进程都不会退出！\n为啥？\n编译器在进行优化时，会尽力利用各种优化策略来提高程序的执行效率。其中一项优化策略是基于所谓的局部性原理，即认为在某个时间段内访问的数据很可能在不久的将来再次被访问。\n在循环中使用的变量，如果编译器能够确定它们在循环体内部不会被修改，就可以将其缓存到寄存器或者CPU的高速缓存中，以减少对内存的读取操作。这样可以提高程序的执行效率，因为访问寄存器或高速缓存比访问内存要快得多。\n在这里，编译器判断 flag 并不会在循环中改变，因此，会把 flag 放在寄存器中缓存，以换取更快的执行速度\n这种优化行为导致了信号处理函数中对全局变量的修改无法被循环条件及时检测到，因为 循环条件使用了被缓存的旧值 ，而 不是从内存中读取最新的值 。\n即使信号处理函数修改了全局变量的值，由于是在内存修改，寄存器的 flag 仍然是一开始缓存的 0，因此，循环仍然会继续执行。\n通过使用 volatile 关键字修饰变量，可以告诉编译器不要对这个变量进行优化，每次访问都从内存中读取最新的值。这样可以确保循环条件能够及时检测到变量的变化，从而正确退出循环。\n加上 volatile 关键字，即 volatile int flag = 0;：\n","permalink":"https://blogs.skylee.top/posts/linux/linux-%E8%BF%9B%E7%A8%8B%E4%BF%A1%E5%8F%B7/note/","tags":["Linux","OS"],"title":"Linux 进程信号"},{"categories":["Linux"],"content":"管道 管道是一种特殊的进程间通信机制，它可以用于将一个进程的输出直接传递给另一个进程的输入，从而实现它们之间的数据传输。\n匿名管道 pipe 系统调用 pipe是一个系统调用，用于创建一个管道(pipe)。它的原型如下：\nint pipe(int pipefd[2]); pipe系统调用用于创建一个匿名管道(pipe)，它是一种特殊的通信机制，用于在两个相关的进程之间进行单向的数据传输。管道具有固定的读端和写端，数据从写端流入管道，从读端流出。\n以下是一些关于pipe系统调用的要点：\npipefd 是一个长度为2的整型数组，用于存储管道的读端和写端的文件描述符。pipefd[0]表示管道的读端，pipefd[1]表示管道的写端。\n调用 pipe 系统调用将创建一个新的管道，并将其读端和写端的文件描述符分别存储在 pipefd[0]和pipefd[1]中。\n管道是一种 半双工 的通信机制，数据只能在一个方向上流动。一端用于写入数据，另一端用于读取数据。\n管道中的数据是以字节流的形式传输的，没有固定的消息边界。数据写入管道后，可以从另一端读取出来，按照写入的顺序进行读取。\n管道的大小是有限的，通常是几千字节。当管道已满时，继续写入数据会导致写操作阻塞，直到有足够的空间可用为止。类似地，如果 管道为空时进行读取操作，读操作也会阻塞。\n管道是由内核维护的，可以在父子进程之间传递数据。父进程创建管道后，可以通过fork系统调用创建子进程，子进程继承了父进程的管道文件描述符，从而实现进程间的通信。\n下面是使用 pipe 创建一个匿名管道的例子：\nint main(void) { int fds[2]; if(pipe(fds) != 0) { perror(\u0026#34;pipe error!\\n\u0026#34;); return 1; } char buff[1024]; while (fgets(buff, sizeof(buff), stdin) != NULL) // 从标准输入读数据 { int len = strlen(buff); // 写数据到管道 if(write(fds[1], buff, len) != len) { perror(\u0026#34;Buffer overflow\\n\u0026#34;); return 1; } memset(buff, 0x00, sizeof(buff)); // 从管道读数据 if(read(fds[0], buff, len) != len) { perror(\u0026#34;read error\\n\u0026#34;); return 1; } // 打印到标准输出 if(fputs(buff, stdout) != len) { perror(\u0026#34;fputs error\\n\u0026#34;); return 1; } } } 以文件描述符（fd）的角度理解管道 使用匿名管道进行进程间通信 匿名管道通常用于父子进程间的通信\nint main(void) { char buffer[1024]; int fds[2]; if (pipe(fds) != 0) { perror(\u0026#34;pipe error!\\n\u0026#34;); return -1; } pid_t pid = fork(); if (pid \u0026lt; 0) { perror(\u0026#34;fork error!\\n\u0026#34;); return -1; } else if (pid == 0) // 子进程写数据 { for(int i = 0; i \u0026lt; 3; ++i) // 从标准输入读三个数据，并发送到父进程 { printf(\u0026#34;Now child process[%d] is running...\\n\u0026#34;, getpid()); printf(\u0026#34;child[%d]: enter the data you want to send to dad process\\n\u0026#34;, getpid()); fgets(buffer, sizeof(buffer), stdin); close(fds[0]); // 关闭读端 write(fds[1], buffer, strlen(buffer)); // 向管道写数据 sleep(3); } printf(\u0026#34;Now child process[%d] is quitting...\\n\u0026#34;, getpid()); exit(0); } else { printf(\u0026#34;Now dad process[%d] is running...\\n\u0026#34;, getpid()); while (1) { printf(\u0026#34;Waitting child[%d] process...\\n\u0026#34;, pid); memset(buffer, 0x00, sizeof(buffer)); close(fds[1]); // 关闭写端 if(read(fds[0], buffer, sizeof(buffer)) == 0) break; printf(\u0026#34;Dad process[%d] recived message from child process: \u0026#34;, getpid()); printf(\u0026#34;%s\u0026#34;, buffer); } wait(NULL); printf(\u0026#34;Now dad process[%d] is quitting...\\n\u0026#34;, getpid()); } return 0; } 如果没有 close(fds[1]); 这句话，会发生什么？\n在我们的代码中，是子进程先退出，父进程后退出\n当子进程写入完数据后，父进程在读取数据时可能会一直阻塞。因为管道的写端仍然保持打开，父进程会一直等待子进程写入更多数据，但实际上子进程已经退出并关闭了写端。所以，父进程无法检测到管道的结束，会一直阻塞在读取操作上。\n这体现在子进程退出后，父进程不会退出，而是阻塞等待子进程继续写入数据\n同样的，如果修改代码，父进程先退出，子进程后退出，并且没有 close(fds[0]); 这句话，子进程也会阻塞等待父进程读取数据\n管道是一种临界资源 当子进程在写数据时，如果有进程也写数据的话，就有可能造成数据不一致，管道通过 互斥 机制保证了资源的互斥访问\n当子进程还没有写数据，或者 sleep 时，父进程就像 sleep 了一样，因为管道内部实现了 同步 机制，保证只有管道有数据时，才能读取数据，否则必须阻塞等待\n操作系统通过互斥锁保护管道的读写操作，以及使用内核缓冲区来隔离读写操作，从而实现了在管道读取或写入时不被其他进程干扰的机制\nSIGPIPE 信号 当一个进程关闭了管道的读端之后，如果还有进程尝试向管道写入数据，就会产生 SIGPIPE 信号\nSIGPIPE 信号的 默认行为是终止进程\n如果进程希望忽略 SIGPIPE 信号，可以使用 signal 函数（下面有例子）\nvoid sigHandler(int signum) // 信号处理函数 { if (signum == SIGPIPE) { printf(\u0026#34;Received SIGPIPE signal\\n\u0026#34;); } } int main(void) { signal(SIGPIPE, sigHandler); int pip[2]; pipe(pip); close(pip[0]); // 关闭读端 write(pip[1], \u0026#34;111\u0026#34;, 3); // 尝试写数据 } 此外，虽然向一个关闭写端的管道写入数据，会发生写入失败，按理来说会产生 SIGPIPE 信号，但实际上并不会\n命名管道 匿名管道最大的限制就是：只能用于具有亲缘关系的进程通信\n如果要实现两个完全不相关的进程之间的通信，就必须用到命名管道了\n命名管道实际上是一个 FIFO 类型的文件\n创建命名管道 可以在终端中使用 mkfifo 指令来创建一个命名管道：\nSky_Lee@SkyLeeMBP test % mkfifo fifo Sky_Lee@SkyLeeMBP test % ls cfile client fifo server test test.txt cfile.c client.c makefile server.c test.cpp 查看 fifo 的详细信息：\n也可以在程序中创建一个命名管道：\n#include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; int mkfifo(const char *pathname, mode_t mode); 其中：\npathname 是创建的命名管道的路径和名称。 mode 是创建的命名管道的权限。 使用命名管道进行进程间通信 父子进程 // 将 temp.txt 的数据写入管道 void writeFifo(void) { int pipeFd = open(\u0026#34;fifo\u0026#34;, O_WRONLY); int fileFd = open(\u0026#34;temp.txt\u0026#34;, O_RDONLY); if (pipeFd == -1 || fileFd == -1) { perror(\u0026#34;open error\\n\u0026#34;); exit(-1); } char buff[1024]; int bytes; while ((bytes = read(fileFd, buff, sizeof(buff))) \u0026gt; 0) { write(pipeFd, buff, bytes); } close(fileFd); close(pipeFd); printf(\u0026#34;[%d]Write fifo complete\\n\u0026#34;, getpid()); } // 读取管道数据，并打印到标准输出 void readFifo(void) { int fifoFd = open(\u0026#34;fifo\u0026#34;, O_RDONLY); if(fifoFd == -1) { perror(\u0026#34;open fifo error!\\n\u0026#34;); exit(-1); } char buff[1024]; int bytes; while ((bytes = read(fifoFd, buff, sizeof(buff))) != 0) { printf(\u0026#34;%s\\n\u0026#34;, buff); } close(fifoFd); printf(\u0026#34;[%d]Read fifo complete\\n\u0026#34;, getpid()); } int main(void) { pid_t pid = fork(); if(pid \u0026lt; 0) { perror(\u0026#34;fork error\\n\u0026#34;); return -1; } else if(pid == 0) { printf(\u0026#34;[%d]About write to fifo...\\n\u0026#34;, getpid()); writeFifo(); exit(0); } else { printf(\u0026#34;[%d]About read from fifo...\\n\u0026#34;, getpid()); readFifo(); wait(NULL); // 避免僵尸进程 } } server And client（不相关进程） server.c:\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define O_FIFO_ERROR 114 #define RD_FIFO_SUCCESS 115 #define RD_FIFO_ERROR 116 int readFromFifo(void); int main(void) { int status = readFromFifo(); exit(status); } int readFromFifo(void) { int fifoFd = open(\u0026#34;fifo\u0026#34;, O_RDONLY); if(fifoFd == -1) { perror(\u0026#34;open fifoFd error!\\n\u0026#34;); return O_FIFO_ERROR; } char buff[1024]; int bytes; while (1) { printf(\u0026#34;[%d]Waiting for the client to make a request...\\n\u0026#34;, getpid()); bytes = read(fifoFd, buff, sizeof(buff)); if(bytes \u0026gt; 0) printf(\u0026#34;[%d]Server recived data from client: %s\u0026#34;, getpid(), buff); else if(bytes == 0) { printf(\u0026#34;[%d]The client has exited and the server is about shutting down...\\n\u0026#34;, getpid()); sleep(1); break; } else { perror(\u0026#34;Read fifo error!\\n\u0026#34;); return RD_FIFO_ERROR; } } close(fifoFd); return RD_FIFO_SUCCESS; } client.c:\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define O_FIFO_ERROR 114 #define WR_FIFO_SUCCESS 115 #define WR_FIFO_ERROR 116 int writeToFifo(void); int main(void) { printf(\u0026#34;[%d]Waiting for the server...\\n\u0026#34;, getpid()); int status = writeToFifo(); exit(status); } int writeToFifo(void) { int fifoFd = open(\u0026#34;fifo\u0026#34;, O_WRONLY); if(fifoFd == -1) { perror(\u0026#34;open fifoFd error!\\n\u0026#34;); return O_FIFO_ERROR; } char buff[1024]; int bytes; while (1) { printf(\u0026#34;[%d]Please enter the data you want to send to server: \u0026#34;, getpid()); fflush(stdout); memset(buff, 0x00, sizeof(buff)); int readBytes = read(1, buff, sizeof(buff)); printf(\u0026#34;[%d]Sending...\\n\u0026#34;, getpid()); sleep(1); bytes = write(fifoFd, buff, sizeof(buff)); if(bytes == -1) { perror(\u0026#34;Write fifo error!\\n\u0026#34;); return WR_FIFO_ERROR; } printf(\u0026#34;[%d]Success!\\n\u0026#34;, getpid()); } close(fifoFd); return WR_FIFO_SUCCESS; } 运行结果：\n小细节：\n在这段代码中，fflush(stdout); 语句的作用是刷新标准输出缓冲区，确保前面的输出立即显示在终端上。如果没有这句话，可能会导致输出被缓冲起来，直到遇到换行符或者缓冲区满才会显示出来。\n如果没有使用 fflush(stdout);，在执行到输入提示信息的时候，输出可能不会立即显示在终端上，而是留存在缓冲区中。这意味着用户可能无法立即看到输入提示信息，而需要等到缓冲区满或者遇到换行符时，才会将提示信息显示出来。\n这可能会导致用户的输入体验不太好，因为他们无法及时看到输入提示。而加上 fflush(stdout); 这句话可以解决这个问题，确保输出能够及时显示在终端上。\n命名管道的本质 命名管道文件（FIFO）在文件系统中以文件的形式存在，但它并不存储数据在磁盘中。相反，它用作进程间通信的通道，数据从一个进程通过管道写入，并从另一个进程通过管道读取。\n当创建一个命名管道文件时，它会在文件系统中占据一定的存储空间，通常以零长度的文件形式存在。这个文件在磁盘上占据一些磁盘空间，但它的作用是提供进程间通信的机制，而不是存储数据本身。\n当一个进程向管道写入数据时，数据会被暂存于内核内存中的管道缓冲区中。接收数据的进程从缓冲区中读取数据。\n在这个过程中，管道缓冲区起到了临时存储数据的作用，但它不是普通的文件存储。它是一种在内存中维护的缓冲区，用于暂存数据以供进程间通信使用。\n总结起来，命名管道文件在文件系统中以文件形式存在，但并不实际存储数据。数据是通过内核内存中的管道缓冲区进行传递。\nSystem V 进程间通信 共享内存 相较于管道通信，共享内存是最快的进程间通信方式，一旦内存映射到共享它的进程的内存空间，这些进程间的数据传递就 不再涉及 OS 内核，也就不需要通过内核来交换数据，提升数据交换的效率\n创建共享内存的步骤 创建共享内存对象：首先，需要调用 shmget() 函数创建一个共享内存对象。这个函数会分配一块共享内存区域，并返回一个唯一的标识符（共享内存ID）用于后续的操作。\n连接共享内存：接下来，使用 shmat() 函数将进程与共享内存区域进行连接。这个函数将共享内存区域映射到进程的虚拟地址空间，使得进程可以直接访问该内存区域。\n使用共享内存：一旦连接成功，进程就可以像访问普通内存一样使用共享内存区域。进程可以读取和写入共享内存区域中的数据，实现进程间的数据共享。\n分离共享内存：当进程不再需要访问共享内存时，需要使用 shmdt() 函数将共享内存从进程的虚拟地址空间中分离。这个操作不会删除共享内存区域，只是断开了进程与共享内存的连接。\n删除共享内存对象：最后，当不再需要共享内存区域时，可以调用 shmctl() 函数以及指定的命令来删除共享内存对象。这个操作会释放共享内存区域，并且其他进程将无法再连接到该共享内存。\n注意：共享内存并不提供同步机制 ，因此在使用共享内存时需要额外考虑进程间的同步问题。\nftok 函数 ftok 函数用于生成一个唯一的键值（key）用于 System V IPC（进程间通信）机制中的消息队列、信号量集和共享内存的创建和访问。\n函数原型如下：\nkey_t ftok(const char *pathname, int proj_id); pathname：一个指向存在的文件的路径名的字符串。该文件的存在与内容无关，只用于生成唯一的键值。 proj_id：一个用户指定的整数，通常为一个正整数。它用于 进一步确保生成的键值的唯一性 。在不同的项目或场景中，可以使用不同的 proj_id。 ftok 函数根据给定的 pathname 和 proj_id 生成一个 32 位的键值。该键值是根据 pathname 文件的 st_dev（设备 ID）和 st_ino（inode 号）字段进行计算的。因此，只要 pathname 和 proj_id 保持不变，生成的键值将是唯一的。\nshmget 函数 shmget 函数的主要功能是 创建一个新的共享内存段或获取一个已存在的共享内存段的标识符 。\n函数原型如下：\n#include \u0026lt;sys/ipc.h\u0026gt; #include \u0026lt;sys/shm.h\u0026gt; int shmget(key_t key, size_t size, int shmflg); key：使用 ftok 函数生成。不同的进程通过相同的 key 值来访问同一块共享内存区域。\nsize：指定需要创建的共享内存段的大小（字节数），由于申请时需要对齐，因此一般设置为内存分页大小的整数倍\nshmflg：共享内存的权限标志，常见的有：\nIPC_CREAT：如果指定的共享内存段不存在，则创建一个新的共享内存段。如果共享内存段已存在，则忽略此标志。 IPC_EXCL：与 IPC_CREAT 结合使用时，如果指定的共享内存段已存在，则返回错误 。如果共享内存段不存在，则创建一个新的共享内存段。 SHM_RDONLY：以只读模式打开共享内存段。进程只能读取共享内存的内容，不能修改。 SHM_RND：将共享内存大小舍入到系统页面大小的整数倍。 函数返回值：\n成功：返回共享内存段的标识符（共享内存ID）。 失败：返回 -1，并设置 errno 变量以指示错误类型。 前面讲 ftok 函数时，提到：只要 pathname 和 proj_id 保持不变，生成的键值将是唯一的。，因此，如果不同的进程使用相同的 pathname 和 proj_id，就能保证 key 标识的唯一性，进而保证通信的进程看到的是同一份资源\n例如：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/ipc.h\u0026gt; #include \u0026lt;sys/shm.h\u0026gt; void getmem(int flag) { // 生成一个唯一的键值（key），用于 System V IPC key_t memKey = ftok(\u0026#34;test.txt\u0026#34;, 114514); if(memKey \u0026lt; 0) { perror(\u0026#34;ftok error!\\n\u0026#34;); exit(-1); } // 创建一个新的独占的共享内存段，确保只有一个进程可以使用它 int memID = shmget(memKey, 4096, flag); if(memID \u0026lt; 0) { perror(\u0026#34;shmget error!\\n\u0026#34;); exit(-1); } printf(\u0026#34;[%d]: %d %d\\n\u0026#34;, getpid(), memKey, memID); // sleep(10); } int main(void) { int pid = fork(); if(pid == 0) { getmem(IPC_CREAT); } else if(pid \u0026gt; 0) { getmem(IPC_CREAT | IPC_EXCL); } return 0; } 运行结果：\n可以看到，父子进程的共享内存的 ID 是一致的\n还可以使用终端指令查看当前系统存在的共享内存：\n可以看到，与之前的运行结果一致\nshmat 函数 shmat 函数用于将共享内存区域 附加 到调用进程的地址空间，使得进程可以访问共享内存中的数据。它的函数原型如下：\nvoid *shmat(int shmid, const void *shmaddr, int shmflg); shmid：共享内存标识符，是由 shmget 函数返回的共享内存标识符。 shmaddr：指定共享内存附加到进程地址空间的地址。通常将其设置为 NULL，由系统自动选择适当的地址。 shmflg：附加标志，用于指定共享内存的附加方式。 通常设置为 0，采取默认的方式进行附加操作。 常用的标志有 SHM_RDONLY（以只读方式附加共享内存）和 SHM_RND（将 shmaddr 地址进行舍入）。 shmat 函数将共享内存区域映射到调用进程的地址空间，并 返回一个指向共享内存区域的指针，该指针可用于对共享内存进行读写操作。\nshmdt 函数 shmdt 函数用于将共享内存从调用进程的地址空间中分离，使得进程无法再访问共享内存中的数据。它的函数原型如下：\nint shmdt(const void *shmaddr); shmaddr：指向共享内存区域的指针，该指针是由 shmat 函数返回的。 调用 shmdt 函数后，共享内存区域将从进程的地址空间中分离，但 不会删除共享内存区域本身 。其他仍然附加该共享内存的进程仍然可以访问共享内存中的数据。\n注意： 即使所有进程都将共享内存分离，即没有进程再附加该共享内存，该共享内存仍会一直存在\nshmctl 函数 shmctl 函数用于 控制 共享内存的操作，包括获取共享内存信息、修改共享内存的权限和删除共享内存等。它的函数原型如下：\nint shmctl(int shmid, int cmd, struct shmid_ds *buf); shmid：共享内存标识符，由 shmget 函数返回的标识符。 cmd：控制命令，用于指定要执行的操作。常用的命令有： IPC_STAT：获取共享内存的信息，并将结果保存在 buf 指向的结构体中。 IPC_SET：设置共享内存的权限，使用 buf 指向的结构体中的信息。 IPC_RMID：删除共享内存。 buf：指向 shmid_ds 结构体的指针，用于传递或接收共享内存的信息。 使用 shmctl 函数可以实现以下操作：\n获取共享内存的信息：通过指定 cmd 为 IPC_STAT，可以获取共享内存的当前状态信息，如共享内存的大小、创建者的用户 ID 和组 ID、访问权限等。 修改共享内存的权限：通过指定 cmd 为 IPC_SET，可以修改共享内存的访问权限。需要提供一个 shmid_ds 结构体，其中包含要修改的权限信息。 删除共享内存：通过指定 cmd 为 IPC_RMID，可以删除指定的共享内存。删除后，所有附加该共享内存的进程都无法再访问共享内存中的数据。 shmctl 函数的返回值：\n成功：返回 0。表示函数执行成功。 失败：返回 -1。表示函数执行失败。 注意： 删除共享内存并 不会立即释放 该共享内存的空间，而是 在所有附加该共享内存的进程都将其分离后才会释放。\n实例 // header.h #pragma once #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/ipc.h\u0026gt; #include \u0026lt;sys/shm.h\u0026gt; #include \u0026#34;string.h\u0026#34; #define FILE_PATH \u0026#34;test.txt\u0026#34; #define PROJ_ID 114514 /** * @brief 分配一块共享内存 * * @param size 共享内存的字节数 * @param flag 共享内存的权限标志 * @return int 共享内存的 id（-1为分配错误） */ int allocMem(size_t size, int flag); int destroyMem(int memID); // function.c #include \u0026#34;header.h\u0026#34; int allocMem(size_t size, int flag) { int memkey = ftok(FILE_PATH, PROJ_ID); if(memkey \u0026lt; 0) { perror(\u0026#34;allocMem: ftok error!\\n\u0026#34;); return -1; } int memID = shmget(memkey, size, flag); if(memID \u0026lt; 0) { perror(\u0026#34;allocMem: shmget error!\\n\u0026#34;); return -1; } return memID; } int destroyMem(int memID) { if(shmctl(memID, IPC_RMID, NULL) \u0026lt; 0) { perror(\u0026#34;destroyMem: shmctl error!\\n\u0026#34;); return -1; } return 0; } // server.c #include \u0026#34;header.h\u0026#34; int main(void) { int memID = allocMem(4096, IPC_CREAT | IPC_EXCL | 0666); if(memID == -1) exit(-1); char * memAddress = shmat(memID, NULL, 0); // 采用默认附加方式 printf(\u0026#34;Starting server...\\n\u0026#34;); sleep(3); int refreshTime; printf(\u0026#34;Please enter refresh time(second): \u0026#34;); fflush(stdout); scanf(\u0026#34;%d\u0026#34;, \u0026amp;refreshTime); // 等待 client 超时的次数，超过 10 次就退出服务器 int cnt = 0; while (cnt \u0026lt; 10) { sleep(refreshTime); fflush(stdout); if(memAddress[0] == \u0026#39;\\0\u0026#39;) { ++cnt; printf(\u0026#34;Waitting...\\n\u0026#34;); } else { printf(\u0026#34;Recived data from client: %s\u0026#34;, memAddress); cnt = 0; } memAddress[0] = \u0026#39;\\0\u0026#39;; } printf(\u0026#34;About quitting server...\\n\u0026#34;); shmdt(memAddress); sleep(2); // 等待 client 分离 destroyMem(memID); return 0; } // client #include \u0026#34;header.h\u0026#34; int main(void) { int memID = allocMem(4096, IPC_CREAT); if(memID == -1) exit(-1); char * memAddress = shmat(memID, NULL, 0); // 采用默认附加方式 printf(\u0026#34;Starting client...\\n\u0026#34;); sleep(3); char buffer[4096]; int loops; printf(\u0026#34;Please enter the num of messages you want to send: \u0026#34;); fflush(stdout); scanf(\u0026#34;%d\u0026#34;, \u0026amp;loops); while (loops--) { printf(\u0026#34;Please enter the data you want to send to server: \u0026#34;); fflush(stdout); int bytes = read(0, buffer, sizeof(buffer)); buffer[bytes] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;Sending...\\n\u0026#34;); sleep(1); strcpy(memAddress, buffer); printf(\u0026#34;Success!\\n\u0026#34;); } printf(\u0026#34;About quitting client...\\n\u0026#34;); shmdt(memAddress); sleep(2); return 0; } 运行结果：\n注意，在运行时，应该先打开服务器端，再打开用户端，如果先打开用户端，服务器端会发现已经有共享内存，进而报错（原因是 IPC_CREAT | IPC_EXCL | 0666）\n","permalink":"https://blogs.skylee.top/posts/linux/linux-%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/note/","tags":["Linux","OS"],"title":"Linux 进程通信"},{"categories":["Linux"],"content":"文件描述符（fd） 文件描述符（File Descriptor）是一个用于标识和操作打开文件的整数值。在UNIX和类UNIX操作系统中，文件描述符是一种抽象概念，用于在进程中访问文件、设备、管道等输入/输出资源。\n每个进程在运行时都有一个 文件描述符表 （File Descriptor Table），其中存储了与文件相关的信息。文件描述符表是一个数组，索引从0开始，每个索引对应一个文件描述符。标准输入（stdin）、标准输出（stdout）、标准错误（stderr）通常预先分配了文件描述符0、1和2。\n当打开或创建文件时，操作系统会分配一个未使用的文件描述符并返回给进程。进程可以使用文件描述符来执行各种操作，如读取文件内容、写入数据、改变文件的属性等。文件描述符是进程访问文件的关键工具，它允许进程通过标准的I/O操作对文件进行读写。\n文件描述符的整数值通常是非负整数。常见的文件描述符值如下：\n0：标准输入（stdin） 1：标准输出（stdout） 2：标准错误（stderr） 需要注意的是， 文件描述符在进程中是独立的 ，即 每个进程都有自己的文件描述符表 。不同进程中的文件描述符可以指向相同的文件，但它们的文件描述符值是独立的。因此，同一个文件在不同进程中可以具有不同的文件描述符值。\n在使用文件描述符后，应该确保及时关闭不再使用的文件描述符，以释放系统资源。使用 close 系统调用可以关闭文件描述符。\n分配规则 在 files_struct 数组当中，找到当前 没有被使用 的 最小 的一个下标，作为新的文件描述符。\n验证如下：\nclose(0); // 关闭 fd = 0 int fd = open(\u0026#34;test.txt\u0026#34;, O_RDWR); printf(\u0026#34;fd = %d\\n\u0026#34;, fd); // 0，因为此时 fd = 0 是空闲中最小的 如果将 close(0); 改成 close(2);，将输出 fd = 2\n如果将 close(0); 改成 close(1);，终端不会有任何输出\n为什么？\n这就是后面要讲的重定向\nopen 系统调用 open系统调用是一个在UNIX和类UNIX操作系统中常见的系统调用之一。它用于打开或创建文件，并 返回一个文件描述符 （file descriptor），以便后续的读取、写入或其他文件操作。\nopen 系统调用的原型如下：\n#include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; int open(const char *path, int flags); int open(const char *path, int flags, mode_t mode); 参数说明：\npath：要打开或创建的文件的路径。 flags：打开文件的模式和选项，如只读、只写、追加等。 mode：创建文件时的权限（仅在创建新文件时使用）。 open 系统调用返回的文件描述符可以用于后续对文件的读取、写入或其他操作。文件描述符是一个非负整数，通常是最小的可用文件描述符。\nopen 系统调用的常见 flags 选项包括：\nO_RDONLY：只读模式打开文件。 O_WRONLY：只写模式打开文件。 O_RDWR：读写模式打开文件。 O_CREAT：如果文件不存在，则创建文件。 O_APPEND：在文件末尾追加写入。 O_TRUNC：将文件截断为空。 open 系统调用的返回值表示操作成功与否，若成功则返回文件描述符，若失败则返回 -1，并设置 errno 变量来指示具体的错误原因。\n需要注意的是，在使用 open 系统调用打开文件后，应该在不再需要使用文件时，使用 close 系统调用来关闭文件描述符，以释放系统资源。\nclose 系统调用 close是一个系统调用，用于关闭一个已打开的文件描述符。它的原型如下：\nint close(int fd); close系统调用会关闭文件描述符 fd 所引用的文件。关闭文件时会释放与之关联的资源，并且不能再对该文件进行读写操作。\n注意：\nclose调用成功返回0，失败返回-1，并设置相应的错误码。在错误发生时，可以使用errno全局变量获取具体的错误信息。\n关闭文件描述符后，不能再对其进行读写操作。如果尝试使用已关闭的文件描述符进行操作，将导致未定义的行为。\n关闭文件描述符时，操作系统会减少该文件描述符的引用计数。当引用计数降至零时，相关资源（如文件描述符表中的条目）将被释放。\nread 系统调用 read 系统调用用于从文件描述符中读取数据。它的原型如下：\nssize_t read(int fd, void *buffer, size_t count); read 系统调用从文件描述符 fd 引用的文件中读取最多 count 字节的数据，并将其存储到 buffer 指向的缓冲区中。\n注意：\nfd 是一个已打开文件的文件描述符。可以是标准输入（0），标准输出（1），标准错误（2），或者其他通过 open 等系统调用获得的文件描述符。\nbuffer 是一个指向存储读取数据的内存缓冲区的指针。调用 read 时，读取的数据将被存储到这个缓冲区中。\ncount 是要读取的最大字节数。read 系统调用将尽可能多地读取数据，但不超过 count。\nread 系统调用返回实际读取的字节数。如果返回值为 0，表示已到达文件末尾（EOF）。如果返回值为 -1，表示读取出错。可以通过检查 errno 全局变量获取具体的错误信息。\nread 系统调用是一个阻塞调用，即在没有可用数据时，会一直等待直到有数据可读。可以通过将文件描述符设置为非阻塞模式（使用 fcntl 系统调用）来改变这种行为。\nread 系统调用是按字节进行读取的，即使指定了较大的 count 值，也不保证一次性读取完整的数据块。因此，在循环中多次调用 read 来读取所需的数据是常见的做法。\nwrite 系统调用 write系统调用用于将数据从缓冲区写入到已打开的文件或文件描述符。\nwrite系统调用的原型与 read 系统调用类似，具体如下：\n#include \u0026lt;unistd.h\u0026gt; ssize_t write(int fd, const void *buf, size_t count); 返回值：\n如果write成功写入了指定的字节数count，它会返回一个非负整数，表示成功写入的字节数。这意味着写入操作完成且没有发生错误。\n如果write写入的字节数少于指定的count，但没有出现错误，它仍然返回一个非负整数，表示成功写入的字节数。\n如果write返回0，表示没有数据被写入。这通常发生在写入一个长度为0的空缓冲区时。\n如果write返回-1，表示发生了错误。这时需要检查errno变量来确定具体的错误原因。常见的错误可能包括文件描述符无效、文件系统已满、I/O错误等。\n重定向 文件的重定向是一种操作，通过它可以改变程序的标准输入、标准输出和标准错误的来源或目标。在Unix-like系统中，文件的重定向使用特定的符号来实现。\n下面是常见的文件重定向符号：\n\u0026gt;：将标准输出重定向到文件，覆盖文件中的内容。例如：command \u0026gt; file.txt，将命令的输出写入到file.txt中，如果文件不存在则创建，如果文件已存在则覆盖原有内容。\n\u0026gt;\u0026gt;：将标准输出重定向到文件，追加到文件末尾。例如：command \u0026gt;\u0026gt; file.txt，将命令的输出追加到file.txt的末尾，如果文件不存在则创建。\n\u0026lt;：将文件作为标准输入。例如：command \u0026lt; input.txt，将input.txt文件中的内容作为命令的输入。\n需要注意的是，文件重定向是由shell来处理的，它将执行命令并处理重定向操作。因此，文件重定向符号在不同的shell中可能会有些差异，具体的用法和行为可能会略有不同。\n对 close(1) 的解释 之前我们提到了：如果将 close(0); 改成 close(1);，终端不会有任何输出\n因为此时发生了重定向，即将 标准输出(fd = 1) 重定向到了 test.txt 中\n这样对于任何的标准输出，最终都会输出到 test.txt 中\n下面这张图很好地解释了重定向的本质\ndup2 系统调用 dup2是一个用于复制文件描述符的系统调用，其原型如下：\nint dup2(int oldfd, int newfd); dup2系统调用将一个已有的文件描述符 oldfd 复制到另一个文件描述符 newfd。如果 newfd 已经打开，则首先关闭 newfd 引用的文件，然后将 oldfd 复制到 newfd。\ndup2的常见用法是将一个文件描述符重定向到标准输入、标准输出或标准错误。通过将 oldfd 设置为已打开文件描述符，newfd 设置为标准输入（0）、标准输出（1）或标准错误（2），可以实现文件描述符的重定向。\n例如，我们可以利用 dup2 来实现上面将 标准输出重定向到 test.txt 的操作：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int test1(void) { int fd = open(\u0026#34;test.txt\u0026#34;, O_RDWR | O_APPEND); if (fd \u0026lt; 0) { perror(\u0026#34;open error\\n\u0026#34;); return 1; } dup2(fd, 1); // 将 1（标准输出）重定向到 fd char buff[1024]; for (int i = 0; i \u0026lt; 5; ++i) // 读 5 行 { int bytes = read(0, buff, sizeof(buff) - 1); if (bytes == -1) { perror(\u0026#34;read error\\n\u0026#34;); return 1; } buff[bytes] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;line %d: %s\u0026#34;, i, buff); // 重定向到 test.txt，不会有标准输出 } return 0; } 在 myShell 中添加重定向功能 这里只添加了 \u0026gt; 和 \u0026gt;\u0026gt; 两种重定向\nauto parse = [\u0026amp;](void) -\u0026gt; int { std::string command; std::getline(std::cin, command); size_t cur = 0, size = command.size(); size_t curArgPos = 0; while (true) { if(curArgPos == MAX_COMMAND_LENGTH - 1) // 还要留一个空间放 NULL return PARSE_ERROR; auto next = command.find(\u0026#39; \u0026#39;, cur); auto temp = command.substr(cur, next - cur); // 重定向 if(temp == \u0026#34;\u0026gt;\u0026#34; || temp == \u0026#34;\u0026gt;\u0026gt;\u0026#34;) { const char *fileName = command.substr(next + 1).c_str(); int fd; if(temp == \u0026#34;\u0026gt;\u0026#34;) // 覆写 fd = open(fileName, O_CREAT | O_TRUNC | O_WRONLY, 0664); else // 追加 fd = open(fileName, O_CREAT | O_APPEND | O_WRONLY, 0664); dup2(fd, 1); // 将标准输出重定向到 fd // close(fd); break; } args[curArgPos] = new char[temp.size()]; strcpy(args[curArgPos++], temp.c_str()); // std::cout \u0026lt;\u0026lt; \u0026#34;debug: \u0026#34; \u0026lt;\u0026lt; args[curArgPos - 1] \u0026lt;\u0026lt; std::endl; if(next == std::string::npos) break; cur = next + 1; } args[curArgPos] = NULL; return PARSE_SUCCESS; }; FILE 根据上面的介绍以及对 C 语言的库函数（fread、fwrite、fopen、fclose）的了解，我们可以发现：\n库函数封装系统调用 访问文件实质上都是通过 fd 访问的 因此，FILE 结构体里面必定封装了 fd\n实例 int main(void) { const char *msg0 = \u0026#34;printf\\n\u0026#34;; const char *msg1 = \u0026#34;fwrite\\n\u0026#34;; const char *msg2 = \u0026#34;write\\n\u0026#34;; printf(\u0026#34;%s\u0026#34;, msg0); fwrite(msg1, strlen(msg1), 1, stdout); write(1, msg2, strlen(msg2)); } 输出：\nprintf fwrite write 看起来很正常对吧\n那如果我们将标准输出重定向到 test.txt 呢？\nSky_Lee@SkyLeeMBP test % ./cfile \u0026gt; test.txt Sky_Lee@SkyLeeMBP test % cat test.txt write printf fwrite 可以发现，输出顺序并没有按照预期\n为啥？\n一般 C 库函数写入文件时是 全缓冲 的，而写入显示器是 行缓冲 。\nprintf fwrite 库函数会自带缓冲区，当发生重定向到普通文件时，数据的缓冲方式由行缓冲变成了全缓冲。\n而我们放在缓冲区中的数据，就不会被立即刷新，甚至 fork 之后\n但是 进程退出之后，会统一刷新，写入文件当中。\n简单来说，上面的例子就是先将待输出的内容装在缓冲区，最后一次性吐出来，顺序就有可能与预期不同\n再来一个例子，加深理解：\nint main(void) { const char *msg0 = \u0026#34;printf\\n\u0026#34;; const char *msg1 = \u0026#34;fwrite\\n\u0026#34;; const char *msg2 = \u0026#34;write\\n\u0026#34;; printf(\u0026#34;%s\u0026#34;, msg0); fwrite(msg1, strlen(msg1), 1, stdout); write(1, msg2, strlen(msg2)); fork(); } 与之前不同的是，这次我们 fork 了一个子进程，如果直接输出到终端，结果：\nprintf fwrite write 仍然按照预期\n重定向呢？\nwrite printf fwrite printf fwrite 神奇的事情发生了：write（系统调用） 输出了一次，而 printf、fwrite（库函数） 竟然输出了两次！\n原因就是在 fork 上\n当我们 fork 了一个子进程后，由于父进程 即将退出，会刷新缓冲区 ，也就意味着父进程要修改缓冲区的数据，此时，父子进程 发生写时拷贝 ，导致父子进程的缓冲区各有一份数据，最终导致 printf（库函数）、fwrite（库函数） 输出两次\n而 write（系统调用） 输出了一次，说明系统调用没有缓冲区\n2024.2.6 补充\nprintf、fwrite 的缓冲区，是在 用户层次 实现的，即在库中实现\n在用户层有一个独立的缓冲区，可以减少对 write 的调用次数（缓冲区到达一定大小，或者用户主动调用 fflush 时，才调用 write）即减少系统调用的次数（涉及变态），提高性能\n而 write 并非没有缓冲区，write 也是有一个写缓冲区的，这个缓冲区是 内核级别 的，只是这个刷新的时机由内核去控制，对用户不可见\n当然用户也可以主动要求内核刷新 write 缓冲区的内容到磁盘（使用 fsync 或 fdatasync 函数： 这两个系统调用可以让你显式地要求操作系统立即将指定文件的所有待写入数据刷新到磁盘。它们的区别在于 fsync 会同时刷新文件数据和元数据，而 fdatasync 只刷新文件数据），避免数据的丢失，但是会影响性能，通常让 OS 来决定何时刷新是一个比较明智的选择\n理解文件系统 可以使用 stat 指令查看某个文件的 Inode 信息：\nSky_Lee@SkyLeeMBP test % stat -x test.cpp File: \u0026#34;test.cpp\u0026#34; Size: 3345 FileType: Regular File Mode: (0644/-rw-r--r--) Uid: ( 501/ Sky_Lee) Gid: ( 20/ staff) Device: 1,9 Inode: 7567874 Links: 1 Access: Tue Jun 13 11:38:37 2023 Modify: Tue Jun 13 11:38:23 2023 Change: Tue Jun 13 11:38:23 2023 Birth: Tue May 30 18:56:31 2023 Inode 在Linux和类Unix系统中，每个文件和目录都有一个与之关联的inode（索引节点）。inode是文件系统中的一个数据结构，用于 存储文件的元信息 （metadata），而不是文件的实际内容。\ninode元信息包括以下内容：\n文件类型：指示文件是普通文件、目录、符号链接等类型。\n文件访问权限：指定了文件的所有者、组和其他用户的读、写和执行权限。\n文件大小：指示文件的大小，以字节为单位。\n文件所属用户和组：指示文件的所有者和所属组。\n文件时间戳：记录了文件的三个时间戳：访问时间（atime），修改时间（mtime）和状态更改时间（ctime）。访问时间表示最后一次读取或访问文件的时间，修改时间表示最后一次修改文件内容的时间，状态更改时间表示最后一次更改inode元信息的时间。\n硬链接计数：指示有多少个目录项指向同一个inode。当创建一个文件时，初始的硬链接计数为1，每创建一个硬链接都会增加这个计数。\n文件数据块的指针 ：指示文件数据在磁盘上的位置\nUnix/Linux系统内部不使用文件名，而使用inode号码来识别文件\n可以看出，文件的属性与数据是分开存储的\n创建一个文件到底做了什么 Sky_Lee@SkyLeeMBP test % touch abc Sky_Lee@SkyLeeMBP test % ls -i abc 8369476 abc 创建一个新文件主要有一下4个操作:\n存储属性：内核先找到一个空闲的 inode (这里是8369476)，内核把文件信息记录到其中。 存储数据：假设该文件需要存储在三个磁盘块，内核找到了三个空闲块：300，500，800。将内核缓冲区的第一块数据复制到 300，下一块复制到 500，以此类推。 记录分配情况：文件内容按顺序 300, 500, 800 存放。内核在 inode 上的磁盘分布区记录了上述块列表。 添加文件名到目录：内核将入口(8369476，abc)添加到目录文件。文件名和 inode 之间的对应关系将文件名和文件的内容及属性连接起来。 硬链接 之前提到：Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件\n实际上，还可以让多个文件对应同一个 inode：\nSky_Lee@SkyLeeMBP test % ln abc def Sky_Lee@SkyLeeMBP test % ls -1i 8369476 abc 8369476 def 可以看到，abc 与 def 的 inode 值是一样的，它们被称为硬链接，此时，查看 abc 的硬链接数：\nSky_Lee@SkyLeeMBP test % stat -x abc File: \u0026#34;abc\u0026#34; Size: 1 FileType: Regular File Mode: (0644/-rw-r--r--) Uid: ( 501/ Sky_Lee) Gid: ( 20/ staff) Device: 1,9 Inode: 8369476 Links: 2 Access: Tue Jun 13 15:57:21 2023 Modify: Tue Jun 13 15:57:21 2023 Change: Tue Jun 13 16:50:34 2023 Birth: Tue Jun 13 15:56:50 2023 可以看到，Links 的值为 2\n我们对 abc 做出的任何修改都会反应到 def 上，同样的，对 def 做出的任何修改都会反应到 abc 上，这不难理解，因为它们的 inode 值相同，文件的 blocks 肯定也相同，本质上就是同一个文件\n当我们删除一个文件时，实际上是让 Links 的值减 1，如果 Links = 0，那么系统将会回收这个 inode，以及对应的 blocks\n软链接 软链接类似 windows 上的快捷方式\n可以使用 ln -s 来创建软链接：\nSky_Lee@SkyLeeMBP test % ln -s abc softabc Sky_Lee@SkyLeeMBP test % ls -1i | grep abc 8369476 abc 8370725 softabc 可以看到，abc 与 softabc 的 inode 值不同，说明是不同的文件\n软链接和硬链接的区别 定义不同\n软链接又叫符号链接，这个文件包含了另一个文件的路径名。可以是任意文件或目录，可以链接不同文件系统的文件。\n硬链接就是一个文件的一个或多个文件名。把文件名和计算机文件系统使用的 inode 链接起来。因此我们可以用多个文件名与同一个文件进行链接，这些文件名可以在同一目录或不同目录。\n限制不同\n硬链接只能对已存在的文件进行创建，不能交叉文件系统进行硬链接的创建；\n软链接可对不存在的文件或目录创建软链接；可交叉文件系统；\n创建方式不同\n硬链接不能对目录进行创建，只可对文件创建；\n软链接可对文件或目录创建；\n影响不同\n删除一个硬链接文件并不影响其他有相同 inode 号的文件。\n删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接被称为死链接（即 dangling link，若被指向路径文件被重新创建，死链接可恢复为正常的软链接）。\n","permalink":"https://blogs.skylee.top/posts/linux/linux-%E5%9F%BA%E7%A1%80-io/note/","tags":["Linux","OS","IO"],"title":"Linux 基础 IO"},{"categories":["Linux"],"content":"fork 在 C 语言中，fork 函数是一个创建新进程的系统调用。它通过复制当前进程创建一个新的子进程，使得父进程和子进程在不同的执行路径上同时运行。\nfork 函数的原型如下：\n#include \u0026lt;unistd.h\u0026gt; pid_t fork(void); 调用 fork 函数会返回两次。在父进程中，fork 函数返回新创建的子进程的进程 ID（PID），而在子进程中，fork 函数返回 0。\n调用 fork，实际上做了：\n分配新的内存块和内核数据结构给子进程 将父进程部分数据结构内容拷贝至子进程 添加子进程到系统进程列表当中 fork 返回，开始调度器调度 当一个进程调用 fork 之后，就有两个二进制代码相同的进程，而且它们都运行到 相同 的地方。\n例如：\nint main(void) { printf(\u0026#34;Before fork, pid = %d\\n\u0026#34;, getpid()); pid_t forkPid = fork(); if(forkPid \u0026lt; 0) { perror(\u0026#34;fork error!\u0026#34;); return 1; } printf(\u0026#34;After fork, pid = %d, forkPid = %d\\n\u0026#34;, getpid(), forkPid); } 输出：\nBefore fork, pid = 2157 After fork, pid = 2157, forkPid = 2158 After fork, pid = 2158, forkPid = 0 可以看到，Before 只打印了一次，而 After 打印了两次，这就说明了 fork 后，父子两个执行流分别执行。\n注意： fork 后，谁先执行完全 由调度器决定 。\n写时拷贝原则 父子进程不仅代码共享，在父子进程均没有写入新数据时，数据也是共享的\n为什么不在创建子进程的时候，就为数据开辟新的空间？\n子进程不一定要使用（修改）父进程的所有数据 实现按需分配 fork 使用场景 一个父进程希望复制自己，使父子进程同时执行不同的代码段。例如，父进程等待客户端请求，生成子进程来处理请求。 一个进程要执行一个不同的程序。例如子进程从 fork 返回后，调用 exec 函数（后面会讲）。 fork 失败场景 系统进程太多 用户进程数超过限制 进程终止 一般来说，进程终止分为两种情况：\n正常退出，如：从 main 返回，_exit() 系统调用，exit() 系统调用 异常退出，如 control + c，kill -p \u0026lt;PID\u0026gt; 等 _exit 系统调用 函数声明：\n#include \u0026lt;unistd.h\u0026gt; void _exit(int status); 参数 status 定义了进程的终止状态，父进程通过 wait 来获取该值\nexit 系统调用 函数声明：\n#include \u0026lt;unistd.h\u0026gt; void exit(int status); 参数 status 定义了进程的终止状态，父进程通过 wait 来获取该值\n_exit 与 exit 的区别 _exit 与 exit 的最大区别就是：exit 会 在进程退出前执行：\n用户通过 atexit 或 on_exit 定义的清理函数。 关闭所有打开的流，所有的缓存数据均被写入 调用 _exit 例如：\nint main(void) { printf(\u0026#34;hello\u0026#34;); exit(0); } 输出：\nSky_Lee@SkyLeeMacBook-Pro test % ./cfile hello 可以看到，字符串 \u0026ldquo;hello\u0026rdquo; 是被输出了的，执行 _exit 呢？\nint main(void) { printf(\u0026#34;hello\u0026#34;); _exit(0); } 输出：\nSky_Lee@SkyLeeMacBook-Pro test % ./cfile 可以看到，\u0026ldquo;hello\u0026rdquo; 并没有输出，这是因为 _exit 是 不会 刷新输出缓冲区的，我们可以手动刷新缓冲区来解决这个问题\nint main(void) { printf(\u0026#34;hello\u0026#34;); fflush(NULL); _exit(0); } 输出：\nSky_Lee@SkyLeeMacBook-Pro test % ./cfile hello return return status 与 exit(status) 是等价的\nreturn status 与 exit(status) 在 main 函数中是等价的\n注意： 在其它函数中使用 return 不会使进程退出，而 exit 会（虽然这句话看起来很简单，但不注意的话\u0026hellip;\u0026hellip;\n进程等待 进程等待（Process waiting）是一种同步机制，用于父进程等待子进程的完成或状态改变。当父进程创建子进程后，通常需要等待子进程的完成或获取子进程的状态信息。进程等待可以通过系统调用（如 wait 或 waitpid ）来实现。\n为什么要有进程等待 进程等待的主要目的是：\n同步：父进程可以等待子进程完成某个任务后再继续执行，以确保协调和顺序执行。这对于需要在父进程中依赖子进程结果的情况很有用。\n回收子进程资源：当子进程终止时，它会进入一种称为\u0026quot;僵尸进程\u0026quot;的状态，此时它占用系统资源但不再执行任何任务。父进程通过等待子进程，可以及时回收子进程的资源，避免产生大量僵尸进程导致系统资源耗尽。\n获取子进程状态：父进程可以通过进程等待来获取子进程的退出状态、终止原因、信号信息等。这样父进程就可以根据子进程的状态做出相应的处理，例如记录日志、重新启动子进程或采取其他措施。\n进程等待的方式 有两种进程等待的方式：wait 与 waitpid 系统调用\nwait 系统调用 在操作系统中，wait 是一个系统调用，用于父进程等待其子进程的结束并回收其资源。\nwait 的原型如下：\n#include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; pid_t wait(int *status); wait 函数接受一个指向整型的指针 status 作为参数，用于存储子进程的退出状态。它会挂起当前进程的执行，直到任一子进程结束为止。当子进程结束后，wait 函数会返回子进程的进程 ID（PID），并将子进程的退出状态存储在 status 指针指向的位置上。\n如果我们不关心子进程的退出状态，可以向 wait 传入一个空指针（NULL）\nwaitpid 系统调用 waitpid 是一个用于等待子进程状态改变的系统调用，它允许父进程阻塞自己，直到指定的子进程发生状态改变，或者指定的子进程终止。\nwaitpid 的函数原型如下：\n#include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; pid_t waitpid(pid_t pid, int *status, int options); 参数说明：\npid：要等待的子进程的进程ID，可以有以下取值： \u0026gt; 0：等待指定进程ID的子进程。 -1：等待任意子进程，类似于 wait。 0：等待与当前进程的进程组ID相同的任意子进程。 \u0026lt; -1：等待进程组ID等于 pid 绝对值的任意子进程。 status：用于保存子进程的终止状态信息的指针。如果不关心子进程的终止状态，可以传入 NULL。 options：附加选项，用于控制 waitpid 的行为。常见的选项有： WCONTINUED：等待一个已经停止的子进程继续运行。 WNOHANG：如果没有子进程改变状态，则 立即返回 ，而不进入阻塞状态。 WUNTRACED：等待一个已经停止的子进程。 返回值 waitpid 函数的返回值表示等待的子进程ID：\n如果指定的子进程终止，则返回该子进程的进程ID。 如果指定的子进程没有终止，且使用了 WNOHANG 选项，则返回 0。 如果发生错误，则返回 -1，并设置 errno 来指示错误的原因。 二者区别 wait 和 waitpid 的主要区别在于参数形式和阻塞行为。wait 等待任意子进程的状态改变并进入阻塞状态，而 waitpid 则允许指定要等待的具体子进程，并可以通过选项来控制阻塞行为。\n参数形式：\nwait：没有指定具体的子进程ID，它会等待任意子进程的状态改变。 waitpid：需要指定要等待的具体子进程ID。 阻塞行为：\nwait：如果当前没有已终止的子进程，调用 wait 会使父进程进入阻塞状态，直到有子进程终止并返回。 waitpid：可以通过传递 WNOHANG 选项来控制其阻塞行为。如果指定了 WNOHANG，即使没有已终止的子进程，waitpid 也会立即返回。 等待条件：\nwait：等待任意子进程的状态改变，包括终止、停止或继续运行。 waitpid：可以通过传递不同的 options 参数来控制等待的条件，如等待终止子进程、等待停止子进程或等待继续运行的子进程。 获取子进程状态 在 wait 与 waitpid 系统调用中，均包含了一个整形参数 status，然而，这个参数并不能简单的看成一个整形变量\n这里只关心 status 的低 16 位\n示例 int main(void) { pid_t pid = fork(); if (pid \u0026lt; 0) { throw std::runtime_error(\u0026#34;fork error!\\n\u0026#34;); return 1; } if (pid == 0) { printf(\u0026#34;child[%d]\\n\u0026#34;, getpid()); sleep(30); // 留点时间杀掉子进程 exit(10); // 没被杀，返回 10 } else { int status; pid_t childPID = wait(\u0026amp;status); // childPID \u0026gt; 0 =\u0026gt; wait 没有发生错误 // (status \u0026amp; 0x7f) =\u0026gt; 如果子进程正常退出，status 低 7 位为 0 if (childPID \u0026gt; 0 \u0026amp;\u0026amp; (status \u0026amp; 0x7f) == 0) { std::cout \u0026lt;\u0026lt; \u0026#34;child exit successfully with exit code: \u0026#34; \u0026lt;\u0026lt; ((status \u0026gt;\u0026gt; 8) \u0026amp; 0xff) \u0026lt;\u0026lt; std::endl; // status 高 8 位是退出状态 } else { std::cout \u0026lt;\u0026lt; \u0026#34;child exit failed with SIG code: \u0026#34; \u0026lt;\u0026lt; (status \u0026amp; 0x7f) \u0026lt;\u0026lt; std::endl; // status 低 7 位为 SIG 码（终止信号） } } } 输出（正常退出）：\nchild[3080] child exit successfully with exit code: 10 输出（另一个终端执行 kill -9 \u0026lt;PID\u0026gt; 指令）：\nchild[2940] child exit failed with SIG code: 9 理解了 status 的细节后，我们可以使用宏定义来简化代码：\n(status \u0026amp; 0x7f) == 0 等价于 WIFEXITED(status) ((status \u0026gt;\u0026gt; 8) \u0026amp; 0xff) 等价于 WEXITSTATUS(status) (status \u0026amp; 0x7f) 等价于 _WSTATUS(status) 下面再具体演示一下两种等待方式的实现：\n阻塞式等待 int main(void) { pid_t pid = fork(); if (pid \u0026lt; 0) { throw std::runtime_error(\u0026#34;fork error!\\n\u0026#34;); return 1; } if (pid == 0) { printf(\u0026#34;child[%d]\\n\u0026#34;, getpid()); sleep(5); exit(10); } else { int status; pid_t ret = waitpid(pid, \u0026amp;status, 0); // opinion = 0，阻塞式等待，等待 5 s if (ret == pid \u0026amp;\u0026amp; WIFEXITED(status)) { std::cout \u0026lt;\u0026lt; \u0026#34;child exit successfully with exit code: \u0026#34; \u0026lt;\u0026lt; WEXITSTATUS(status) \u0026lt;\u0026lt; std::endl;; } else { std::cout \u0026lt;\u0026lt; \u0026#34;child exit failed with SIG code: \u0026#34; \u0026lt;\u0026lt; _WSTATUS(status) \u0026lt;\u0026lt; std::endl; } } } 非阻塞式等待 int main(void) { pid_t pid = fork(); if (pid \u0026lt; 0) { throw std::runtime_error(\u0026#34;fork error!\\n\u0026#34;); return 1; } if (pid == 0) { printf(\u0026#34;child[%d]\\n\u0026#34;, getpid()); sleep(10); exit(10); } else { int status; pid_t ret = 0; while (1) { ret = waitpid(pid, \u0026amp;status, WNOHANG); if(ret == 0) { std::cout \u0026lt;\u0026lt; \u0026#34;child is running...\u0026#34; \u0026lt;\u0026lt; std::endl; sleep(1); } else break; // 子进程退出（可能正常，也可能不正常） } if (ret == pid \u0026amp;\u0026amp; WIFEXITED(status)) { std::cout \u0026lt;\u0026lt; \u0026#34;child exit successfully with exit code: \u0026#34; \u0026lt;\u0026lt; WEXITSTATUS(status) \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34;child exit failed with SIG code: \u0026#34; \u0026lt;\u0026lt; _WSTATUS(status) \u0026lt;\u0026lt; std::endl; } } } 进程程序替换 用 fork 创建子进程后执行的是和父进程相同的程序(但有可能执行不同的代码分支),子进程往往要调用一种 exec 函数以执行另一个程序。\nexec 函数 exec 函数是一个系列的系统调用函数，用于在 Linux 中执行其他程序。它用于将当前进程替换为新的可执行程序，从而使新程序取代原来的程序继续执行。\n调用 exec 函数 不会 创建新进程，只是 进行了程序的替换，原来的 PCB，mm_struct 并没有改变\nexec 函数的常见形式有以下几种：\n#include \u0026lt;unistd.h\u0026gt; int execl(const char *path, const char *arg0, ...); int execlp(const char *file, const char *arg0, ...); int execle(const char *path, const char *arg0, ..., char *const envp[]); int execv(const char *path, char *const argv[]); int execvp(const char *file, char *const argv[]); int execve(const char *path, char *const argv[], char *const envp[]); 感觉很难记？\n观察上述 6 种形式，可以发现：\nl(list)，表示参数用列表 v(vector)，表示参数用一个数组 p(path)，表示会自动搜索环境变量 e(env)，表示自己维护环境变量 只要不自己维护环境变量，exec 就会使用当前的环境变量\n理解这四个字母对应的含义，就好理解上面的六种形式了\n此外，exec 如果调用成功，那就加载新的程序，原来程序 exec 后面的部分都 不执行\n如果调用失败，返回 -1\n示例 假设当前目录为 /Users/Sky_Lee/Documents/Linux/Test/，并且有以下文件：\ncfile.c test.cpp makefile 其中，test.cpp 的内容如下：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(void) { printf(\u0026#34;Now in test[PID = %d]...\\n\u0026#34;, getpid()); if(getenv(\u0026#34;PATH\u0026#34;)) std::cout \u0026lt;\u0026lt; \u0026#34;OS PATH = \u0026#34; \u0026lt;\u0026lt; getenv(\u0026#34;PATH\u0026#34;) \u0026lt;\u0026lt; std::endl; else std::cout \u0026lt;\u0026lt; \u0026#34;OS PATH = (null)\u0026#34; \u0026lt;\u0026lt; std::endl; if(getenv(\u0026#34;MYENV\u0026#34;)) std::cout \u0026lt;\u0026lt; \u0026#34;User PATH = \u0026#34; \u0026lt;\u0026lt; getenv(\u0026#34;MYENV\u0026#34;) \u0026lt;\u0026lt; std::endl; else std::cout \u0026lt;\u0026lt; \u0026#34;User PATH = (null)\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;About to exit test...\u0026#34; \u0026lt;\u0026lt; std::endl; sleep(1); } cfile.c 的内容如下：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(void) { char *const args[] = {\u0026#34;./test\u0026#34;, NULL}; char *const envp[] = {\u0026#34;PATH=/usr/local/bin\u0026#34;, \u0026#34;MYENV=hhh\u0026#34;, NULL}; const char *path = \u0026#34;/Users/Sky_Lee/Documents/Linux/Test/test\u0026#34;; printf(\u0026#34;Now in cfile[PID = %d]...\\n\u0026#34;, getpid()); // exec ... 这里分别对应了六种示例代码 printf(\u0026#34;About exit cfile...\\n\u0026#34;); sleep(1); } 并且，我们使用 export 导入了环境变量 MYENV=\u0026quot;hello exec\u0026quot;\n现在来依次看看这六种示例：\n示例 1 execl(path, \u0026quot;./test\u0026quot;, \u0026quot;./test\u0026quot;, NULL);\n运行，输出如下：\nNow in cfile[PID = 4786]... Now in test[PID = 4786]... OS PATH = /usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Applications/VMware Fusion.app/Contents/Public:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin User PATH = hello exec About to exit test... 可以验证，调用 exec 函数 不会 创建新进程，只是 进行了程序的替换，因为两个程序的 PID 是相同的\n并且，OS PATH 和 User PATH 使用的是当前的环境变量\n示例 2 execlp(\u0026quot;./test\u0026quot;, \u0026quot;./test\u0026quot;, NULL);\n输出与示例 1 一致，加上 p 只是让我们不用提供可执行程序的路径，会自动搜索环境变量，以此找到路径\n示例 3 execle(path, \u0026quot;./test\u0026quot;, \u0026quot;./test\u0026quot;, NULL, envp);\n与示例 1 不同的是：\nOS PATH = /usr/local/bin User PATH = hhh 因为加上了 e，表明自己维护环境变量，exec 就不会使用当前的环境变量了\n示例 4、5、6 execv(path, args); execvp(\u0026#34;./test\u0026#34;, args); execve(path, args, envp); 三个示例的输出与示例 1、2、3 对应，因为加上 v，只是说明提供的是数组，不是列表\n总结 exec 函数的本质 事实上，只有 execve 是真正的系统调用，其它五个函数最终也会调用 execve\n这张图很好地说明了它们之间的关系：\n简易 Shell #include \u0026lt;iostream\u0026gt; #include \u0026lt;bitset\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; class Shell { static constexpr int MAX_COMMAND_LENGTH = 1024; static constexpr int DO_EXEC_ERROR = -1; static constexpr int DO_EXEC_SUCCESS = 0; // 后面发现没啥用（想想为啥没用 static constexpr int PARSE_SUCCESS = 1; static constexpr int PARSE_ERROR = 2; public: void Interface(void) { std::cout \u0026lt;\u0026lt; \u0026#34;mysh % \u0026#34;; std::cout.flush(); pid_t pid = fork(); if(pid == 0) doExec(); else { int status; wait(\u0026amp;status); if (WIFEXITED(status)) // 正常退出 { if (WEXITSTATUS(status)) std::cout \u0026lt;\u0026lt; \u0026#34;mysh: parse error, your command might be to long!\u0026#34; \u0026lt;\u0026lt; std::endl; } else // 非正常退出 std::cout \u0026lt;\u0026lt; \u0026#34;error! SIG code: \u0026#34; \u0026lt;\u0026lt; _WSTATUS(status) \u0026lt;\u0026lt; std::endl; } } private: int doExec(void) { char* args[MAX_COMMAND_LENGTH]; // 解析命令 auto parse = [\u0026amp;](void) -\u0026gt; int { std::string command; std::getline(std::cin, command); size_t cur = 0, size = command.size(); size_t curArgPos = 0; while (true) { if(curArgPos == MAX_COMMAND_LENGTH - 1) // 还要留一个空间放 NULL return PARSE_ERROR; auto next = command.find(\u0026#39; \u0026#39;, cur); auto temp = command.substr(cur, next - cur); args[curArgPos] = new char[temp.size()]; strcpy(args[curArgPos++], temp.c_str()); // std::cout \u0026lt;\u0026lt; \u0026#34;debug: \u0026#34; \u0026lt;\u0026lt; args[curArgPos - 1] \u0026lt;\u0026lt; std::endl; if(next == std::string::npos) break; cur = next + 1; } args[curArgPos] = NULL; return PARSE_SUCCESS; }; if(parse() == PARSE_ERROR) exit(PARSE_ERROR); execvp(args[0], args); // 注意是 exit，而不是 return， // exit 会导致子进程退出 // 而 return 不会，因为 return 只是让函数 doExec 退出 // 这也说明了 exit 与 return 是有区别的 std::cout \u0026lt;\u0026lt; \u0026#34;mysh: command not found: \u0026#34; \u0026lt;\u0026lt; args[0] \u0026lt;\u0026lt; std::endl; // 如果 execvp 正常执行，这一句不会被执行 exit(DO_EXEC_ERROR); } }; int main(void) { Shell shell; while (1) { shell.Interface(); } } 注意： 简易 Shell 不支持 cd 操作（想想为什么），此外，简易 Shell 在遇到有 ' 的指令时，往往不能做出正确的动作\n","permalink":"https://blogs.skylee.top/posts/linux/linux-%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6/note/","tags":["Linux","OS"],"title":"Linux 进程控制"},{"categories":null,"content":"","permalink":"https://blogs.skylee.top/search/_index.es/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://blogs.skylee.top/search/_index.fr/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://blogs.skylee.top/search/_index.hi/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://blogs.skylee.top/search/_index.jp/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://blogs.skylee.top/search/_index.pl/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://blogs.skylee.top/search/_index.ru/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://blogs.skylee.top/search/_index.zh-cn/","tags":null,"title":""}]